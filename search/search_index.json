{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"KEROD - Object Detection for TensorFlow 2.X (FasterRCNN, DeTr) Read Latest Documentation - Browse GitHub Code Repository Kerod is pure tensorflow 2 implementation of object detection algorithms (Faster R-CNN, DeTr) aiming production. It stands for Keras Object Detection. It aims to build a clear, reusable, tested, simple and documented codebase for tensorflow 2.X. Many ideas have been based on google object detection , tensorpack and mmdetection . Features As powerful and concise as Keras Low barrier to entry for educators and practitioners Handle batch in training and inference Rich Documentation Multi-GPU Mixed_precision. You can try it with this notebook No need to struggle to download COCO or PascalVoc. We only used tensorflow_datasets Simple (again) Algorithms [x] Feature Pyramidal Network [x] DeTr from: End to end object detection with transformers [x] Single scale SMCA (WIP not 100% the exact implementation o the paper): Fast Convergence of DETR with Spatially Modulated Co-Attention [ ] Mask-RCNN (Much of the pieces are already here just need to put everything together. It will arrive soon.) Try Kerod Notebooks Training an algorithm on COCO or Pascal VOC has never been so easy. You just need to run the cells and everything will be done for you. You can find examples in the notebooks folder . There are no runners shipped with the library. Algorithm Dataset Performance MultiGPU Mixed Precision Notebook FasterRcnnFPNResnet50Pytorch PascalVoc FasterRcnnFPNResnet50Pytorch PascalVoc :heavy_check_mark: FasterRcnnFPNResnet50Pytorch COCO FasterRcnnFPNResnet50Pytorch COCO 30 mAP on old code base. A bug has been removed since. I need 4 GPUs to rerun the training. :heavy_check_mark: DetrResnet50Pytorch COCO NEED 16 GPUS for 3 days :heavy_check_mark: SMCAR50Pytorch COCO NEED 8 GPUS for 1 days :heavy_check_mark: Pytorch: means resnet implementation Pytorch style. In the residual block we have: conv (1x1) stride 1 -> conv (3x3) stride 2 instead of conv (1x1) stride 2 -> conv (3x3) stride 1 (Caffe, Keras implementations) If you want to perform an overfit you have an example with the detr architecture: Requirements If you don't run the examples on Colab please install tensorflow_datasets : pip install tensorflow_datasets No configuration file The code is (I hope) as simple as possible. You won't find any configuration file. All the parameters have already been chosen for you. If you need to change something simply code it and create a new layer. Why: In deep learning each parameter is important. You must think thoroughly before a change on how it will impact your model. Here, the code base is super simple just rewrite the blocks that you need and create new layers using the power of Keras. Also, it makes the code easier to read. Installation This repo is tested on Python 3.6, 3.7, 3.8 and TensorFlow 2.4.0 You may want to install 'kerod' in a virtual environment or with pyenv . Create a virtual environment with the version of Python you wanna use and activate it. With pip pip install git+https://github.com/EmGarr/kerod.git From source git clone https://github.com/EmGarr/kerod.git cd kerod pip install . When you update the repository, you should upgrade installation and its dependencies as follows: git pull pip install --upgrade . You can install the package in dev mode as follow and everytime you refresh the package it will be automatically updated: pip install -e . Tutorials Simple example To run a training you just need to write the following. import numpy as np from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory , KerodModel num_classes = 20 model = factory . build_model ( num_classes , name = KerodModel . faster_rcnn_resnet50_pytorch ) # Same format than COCO and Pascal VOC in tensorflow datasets inputs = { 'image' : np . zeros (( 2 , 100 , 50 , 3 )), 'objects' : { BoxField . BOXES : np . array ([[[ 0 , 0 , 1 , 1 ]], [[ 0 , 0 , 1 , 1 ]]], dtype = np . float32 ), BoxField . LABELS : np . array ([[ 1 ], [ 1 ]]) } } data = tf . data . Dataset . from_tensor_slices ( inputs ) data = data . map ( preprocess ) data = data . map ( expand_dims_for_single_batch ) base_lr = 0.02 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( data , epochs = 2 , callbacks = [ ModelCheckpoint ( 'checkpoints' )]) results = model . predict ( data , batch_size = 1 ) use_faster_rcnn = True if use_faster_rcnn : model . export_for_serving ( 'saved_model' ) else : model . save ( 'saved_model' ) reload_model = tf . keras . models . load_model ( 'saved_model' ) for x , _ in data : if use_faster_rcnn : reload_model . serving_step ( x [ DatasetField . IMAGES ], x [ DatasetField . IMAGES_INFO ]) else : reload_model . predict_step ( x ) Mixed Precision from tensorflow.keras.mixed_precision import experimental as mixed_precision from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory policy = mixed_precision . Policy ( 'mixed_float16' ) mixed_precision . set_policy ( policy ) num_classes = 20 model = factory . build_model ( num_classes ) Multi-GPU training import numpy as np from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory batch_size_per_gpu = 2 num_gpus = 8 batch_size = batch_size_per_gpu * num_gpus padded_shape = ({ DatasetField . IMAGES : [ None , None , 3 ], DatasetField . IMAGES_INFO : [ 2 ] }, { BoxField . BOXES : [ None , 4 ], BoxField . LABELS : [ None ], BoxField . NUM_BOXES : [ 1 ], BoxField . WEIGHTS : [ None ] }) data = tf . data . Dataset . from_tensor_slices ( inputs ) data = data . padded_batch ( batch_size , padded_shape ) data = data . prefetch ( tf . data . experimental . AUTOTUNE ) mirrored_strategy = tf . distribute . MirroredStrategy () with mirrored_strategy . scope (): model = factory . build_model ( num_classes ) base_lr = 0.02 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( data , epochs = 2 , callbacks = [ ModelCheckpoint ( 'checkpoints' )]) Serving You can then use it in production with tensorflow model server . import requests from kerod.core.standard_fields import DatasetField url = 'https://my_server:XXX/v1/models/serving:predict' image = resize_to_min_dim ( inputs [ 'image' ], 800.0 , 1300.0 ) image_information = tf . cast ( tf . shape ( image )[: 2 ], dtype = tf . float32 ) # Will perform a query for a single batch but you can perform query on batch inputs = [ tf . expand_dims ( images , axis = 0 ) . numpy () . tolist (), tf . expand_dims ( image_information , axis = 0 ) . numpy () . tolist () ] headers = { \"content-type\" : \"application/json\" } response = requests . post ( url , data = json . dumps ( inputs ), headers = headers ) outputs = json . loads ( response . text )[ 'outputs' ] See the outputs of the predict_step of your. For FasterRCNN they will have a similar format: bbox : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. label : A Tensor of shape [batch_size, max_detections] containing the class for boxes. num_boxes : A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. Tests In order to run the tests you should install pytest. pip install pytest Here's the easiest way to run tests for the library: make test or pytest tests/","title":"Home"},{"location":"#features","text":"As powerful and concise as Keras Low barrier to entry for educators and practitioners Handle batch in training and inference Rich Documentation Multi-GPU Mixed_precision. You can try it with this notebook No need to struggle to download COCO or PascalVoc. We only used tensorflow_datasets Simple (again)","title":"Features"},{"location":"#algorithms","text":"[x] Feature Pyramidal Network [x] DeTr from: End to end object detection with transformers [x] Single scale SMCA (WIP not 100% the exact implementation o the paper): Fast Convergence of DETR with Spatially Modulated Co-Attention [ ] Mask-RCNN (Much of the pieces are already here just need to put everything together. It will arrive soon.)","title":"Algorithms"},{"location":"#try-kerod","text":"","title":"Try Kerod"},{"location":"#notebooks","text":"Training an algorithm on COCO or Pascal VOC has never been so easy. You just need to run the cells and everything will be done for you. You can find examples in the notebooks folder . There are no runners shipped with the library. Algorithm Dataset Performance MultiGPU Mixed Precision Notebook FasterRcnnFPNResnet50Pytorch PascalVoc FasterRcnnFPNResnet50Pytorch PascalVoc :heavy_check_mark: FasterRcnnFPNResnet50Pytorch COCO FasterRcnnFPNResnet50Pytorch COCO 30 mAP on old code base. A bug has been removed since. I need 4 GPUs to rerun the training. :heavy_check_mark: DetrResnet50Pytorch COCO NEED 16 GPUS for 3 days :heavy_check_mark: SMCAR50Pytorch COCO NEED 8 GPUS for 1 days :heavy_check_mark: Pytorch: means resnet implementation Pytorch style. In the residual block we have: conv (1x1) stride 1 -> conv (3x3) stride 2 instead of conv (1x1) stride 2 -> conv (3x3) stride 1 (Caffe, Keras implementations) If you want to perform an overfit you have an example with the detr architecture:","title":"Notebooks"},{"location":"#requirements","text":"If you don't run the examples on Colab please install tensorflow_datasets : pip install tensorflow_datasets","title":"Requirements"},{"location":"#no-configuration-file","text":"The code is (I hope) as simple as possible. You won't find any configuration file. All the parameters have already been chosen for you. If you need to change something simply code it and create a new layer. Why: In deep learning each parameter is important. You must think thoroughly before a change on how it will impact your model. Here, the code base is super simple just rewrite the blocks that you need and create new layers using the power of Keras. Also, it makes the code easier to read.","title":"No configuration file"},{"location":"#installation","text":"This repo is tested on Python 3.6, 3.7, 3.8 and TensorFlow 2.4.0 You may want to install 'kerod' in a virtual environment or with pyenv . Create a virtual environment with the version of Python you wanna use and activate it.","title":"Installation"},{"location":"#with-pip","text":"pip install git+https://github.com/EmGarr/kerod.git","title":"With pip"},{"location":"#from-source","text":"git clone https://github.com/EmGarr/kerod.git cd kerod pip install . When you update the repository, you should upgrade installation and its dependencies as follows: git pull pip install --upgrade . You can install the package in dev mode as follow and everytime you refresh the package it will be automatically updated: pip install -e .","title":"From source"},{"location":"#tutorials","text":"","title":"Tutorials"},{"location":"#simple-example","text":"To run a training you just need to write the following. import numpy as np from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory , KerodModel num_classes = 20 model = factory . build_model ( num_classes , name = KerodModel . faster_rcnn_resnet50_pytorch ) # Same format than COCO and Pascal VOC in tensorflow datasets inputs = { 'image' : np . zeros (( 2 , 100 , 50 , 3 )), 'objects' : { BoxField . BOXES : np . array ([[[ 0 , 0 , 1 , 1 ]], [[ 0 , 0 , 1 , 1 ]]], dtype = np . float32 ), BoxField . LABELS : np . array ([[ 1 ], [ 1 ]]) } } data = tf . data . Dataset . from_tensor_slices ( inputs ) data = data . map ( preprocess ) data = data . map ( expand_dims_for_single_batch ) base_lr = 0.02 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( data , epochs = 2 , callbacks = [ ModelCheckpoint ( 'checkpoints' )]) results = model . predict ( data , batch_size = 1 ) use_faster_rcnn = True if use_faster_rcnn : model . export_for_serving ( 'saved_model' ) else : model . save ( 'saved_model' ) reload_model = tf . keras . models . load_model ( 'saved_model' ) for x , _ in data : if use_faster_rcnn : reload_model . serving_step ( x [ DatasetField . IMAGES ], x [ DatasetField . IMAGES_INFO ]) else : reload_model . predict_step ( x )","title":"Simple example"},{"location":"#mixed-precision","text":"from tensorflow.keras.mixed_precision import experimental as mixed_precision from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory policy = mixed_precision . Policy ( 'mixed_float16' ) mixed_precision . set_policy ( policy ) num_classes = 20 model = factory . build_model ( num_classes )","title":"Mixed Precision"},{"location":"#multi-gpu-training","text":"import numpy as np from kerod.dataset.preprocessing import expand_dims_for_single_batch , preprocess from kerod.model import factory batch_size_per_gpu = 2 num_gpus = 8 batch_size = batch_size_per_gpu * num_gpus padded_shape = ({ DatasetField . IMAGES : [ None , None , 3 ], DatasetField . IMAGES_INFO : [ 2 ] }, { BoxField . BOXES : [ None , 4 ], BoxField . LABELS : [ None ], BoxField . NUM_BOXES : [ 1 ], BoxField . WEIGHTS : [ None ] }) data = tf . data . Dataset . from_tensor_slices ( inputs ) data = data . padded_batch ( batch_size , padded_shape ) data = data . prefetch ( tf . data . experimental . AUTOTUNE ) mirrored_strategy = tf . distribute . MirroredStrategy () with mirrored_strategy . scope (): model = factory . build_model ( num_classes ) base_lr = 0.02 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( data , epochs = 2 , callbacks = [ ModelCheckpoint ( 'checkpoints' )])","title":"Multi-GPU training"},{"location":"#serving","text":"You can then use it in production with tensorflow model server . import requests from kerod.core.standard_fields import DatasetField url = 'https://my_server:XXX/v1/models/serving:predict' image = resize_to_min_dim ( inputs [ 'image' ], 800.0 , 1300.0 ) image_information = tf . cast ( tf . shape ( image )[: 2 ], dtype = tf . float32 ) # Will perform a query for a single batch but you can perform query on batch inputs = [ tf . expand_dims ( images , axis = 0 ) . numpy () . tolist (), tf . expand_dims ( image_information , axis = 0 ) . numpy () . tolist () ] headers = { \"content-type\" : \"application/json\" } response = requests . post ( url , data = json . dumps ( inputs ), headers = headers ) outputs = json . loads ( response . text )[ 'outputs' ] See the outputs of the predict_step of your. For FasterRCNN they will have a similar format: bbox : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. label : A Tensor of shape [batch_size, max_detections] containing the class for boxes. num_boxes : A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings.","title":"Serving"},{"location":"#tests","text":"In order to run the tests you should install pytest. pip install pytest Here's the easiest way to run tests for the library: make test or pytest tests/","title":"Tests"},{"location":"CONTRIBUTING/","text":"Contribution Tests make tests Developpers Code coverage Whenever you implement a new functionality you should reach a proper code coverage. Every pull requests will be rejected if the code coverage doesn't reach 90% . Docstring formatting The doc generation tool used is portray which handle markdown format def func ( a , b ): \"\"\"Describe my function Arguments: - *a*: A param a description - *b*: A param b description Returns: The sum of a + b Raises: (If exception are raised) Exceptions: 1 \"\"\" return a + b The code formatting used is yapf The config are automatically loaded from .style.yapf YAPF tries very hard to get the formatting correct. But for some code, it won't be as good as hand-formatting. In particular, large data literals may become horribly disfigured under YAPF. The reasons for this are manyfold. In short, YAPF is simply a tool to help with development. It will format things to coincide with the style guide, but that may not equate with readability. What can be done to alleviate this situation is to indicate regions YAPF should ignore when reformatting something: # yapf: disable FOO = { # ... some very large, complex data literal. } BAR = [ # ... another large data literal. ] # yapf: enable You can also disable formatting for a single literal like this: BAZ = { ( 1 , 2 , 3 , 4 ), ( 5 , 6 , 7 , 8 ), ( 9 , 10 , 11 , 12 ), } # yapf: disable In addition of this, it's recommended to have an automatic formatter of the imports. Imports of the same module should be imported together: from libs.fooo.resnet_v1_101 import ( create_resnet , support_utils_resnet_foo , upload_foo ) Imports should be structured in 3 parts, each separated by a blank line: import os # Base package import keras # External package from pip from libs import foo # import package inner modules Finally, it is prefered that the imports are sorted alphabetically.","title":"Contributing"},{"location":"CONTRIBUTING/#contribution","text":"","title":"Contribution"},{"location":"CONTRIBUTING/#tests","text":"make tests","title":"Tests"},{"location":"CONTRIBUTING/#developpers","text":"","title":"Developpers"},{"location":"CONTRIBUTING/#code-coverage","text":"Whenever you implement a new functionality you should reach a proper code coverage. Every pull requests will be rejected if the code coverage doesn't reach 90% .","title":"Code coverage"},{"location":"CONTRIBUTING/#docstring-formatting","text":"The doc generation tool used is portray which handle markdown format def func ( a , b ): \"\"\"Describe my function Arguments: - *a*: A param a description - *b*: A param b description Returns: The sum of a + b Raises: (If exception are raised) Exceptions: 1 \"\"\" return a + b","title":"Docstring formatting"},{"location":"CONTRIBUTING/#the-code-formatting-used-is-yapf","text":"The config are automatically loaded from .style.yapf YAPF tries very hard to get the formatting correct. But for some code, it won't be as good as hand-formatting. In particular, large data literals may become horribly disfigured under YAPF. The reasons for this are manyfold. In short, YAPF is simply a tool to help with development. It will format things to coincide with the style guide, but that may not equate with readability. What can be done to alleviate this situation is to indicate regions YAPF should ignore when reformatting something: # yapf: disable FOO = { # ... some very large, complex data literal. } BAR = [ # ... another large data literal. ] # yapf: enable You can also disable formatting for a single literal like this: BAZ = { ( 1 , 2 , 3 , 4 ), ( 5 , 6 , 7 , 8 ), ( 9 , 10 , 11 , 12 ), } # yapf: disable In addition of this, it's recommended to have an automatic formatter of the imports. Imports of the same module should be imported together: from libs.fooo.resnet_v1_101 import ( create_resnet , support_utils_resnet_foo , upload_foo ) Imports should be structured in 3 parts, each separated by a blank line: import os # Base package import keras # External package from pip from libs import foo # import package inner modules Finally, it is prefered that the imports are sorted alphabetically.","title":"The code formatting used is yapf"},{"location":"docs/BENCHMARKS/","text":"Benchmarks WIP Using this notebook on Colab using a P100: the code is running at 256ms/step","title":"Benchmarks"},{"location":"docs/BENCHMARKS/#benchmarks-wip","text":"Using this notebook on Colab using a P100: the code is running at 256ms/step","title":"Benchmarks WIP"},{"location":"reference/kerod/","text":"Module kerod None None Sub-modules kerod.core kerod.dataset kerod.layers kerod.model kerod.utils","title":"Index"},{"location":"reference/kerod/#module-kerod","text":"None None","title":"Module kerod"},{"location":"reference/kerod/#sub-modules","text":"kerod.core kerod.dataset kerod.layers kerod.model kerod.utils","title":"Sub-modules"},{"location":"reference/kerod/core/","text":"Module kerod.core None None Sub-modules kerod.core.box_coder kerod.core.box_ops kerod.core.constants kerod.core.learning_rate_schedule kerod.core.losses kerod.core.matcher kerod.core.sampling_ops kerod.core.similarity kerod.core.standard_fields kerod.core.target_assigner","title":"Index"},{"location":"reference/kerod/core/#module-kerodcore","text":"None None","title":"Module kerod.core"},{"location":"reference/kerod/core/#sub-modules","text":"kerod.core.box_coder kerod.core.box_ops kerod.core.constants kerod.core.learning_rate_schedule kerod.core.losses kerod.core.matcher kerod.core.sampling_ops kerod.core.similarity kerod.core.standard_fields kerod.core.target_assigner","title":"Sub-modules"},{"location":"reference/kerod/core/box_coder/","text":"Module kerod.core.box_coder None None View Source import tensorflow as tf from kerod.core import box_ops EPSILON = 1e-8 def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ): \"\"\"Encode a box collection with respect to anchor collection according to the [Faster RCNN paper](http://arxiv.org/abs/1506.01497). Faster RCNN box coder follows the coding schema described below: t_y = (y - y_a) / h_a t_x & = (x - x_a) / w_a t_h & = log(h / h_a) t_w & = log(w / w_a) where y, x h, w denote the box's center coordinates, width and height respectively. Similarly, y_a, x_a, h_a, w_a denote the anchor's center coordinates, width and height. t_y, t_x, t_h and t_w denote the anchor-encoded center, height and width respectively. Arguments: - *boxes*: BoxList holding N boxes to be encoded. - *anchors*: BoxList of anchors. - *scale_factors*: List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: A tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw]. \"\"\" # Convert anchors to the center coordinate representation. anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) boxes = box_ops . convert_to_center_coordinates ( boxes ) ycenter , xcenter , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) # Avoid NaN in division and log below. ha += EPSILON wa += EPSILON h += EPSILON w += EPSILON ty = ( ycenter - ycenter_a ) / ha tx = ( xcenter - xcenter_a ) / wa th = tf . math . log ( h / ha ) tw = tf . math . log ( w / wa ) # Scales location targets as used in paper for joint training. if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty *= scale_factors [ 0 ] tx *= scale_factors [ 1 ] th *= scale_factors [ 2 ] tw *= scale_factors [ 3 ] return tf . concat ([ ty , tx , th , tw ], axis =- 1 ) def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ): \"\"\"Decode relative codes to boxes according to the [Faster RCNN paper](http://arxiv.org/abs/1506.01497). Faster RCNN box decoder follows the coding schema described below: ycent = t_y h_a + ycent_a xcent= t_x w_a + xcent_a h = exp(t_h) h_a w = exp(t_w) w_a where t_y, t_x, t_h, t_w denote the encoded box's center coordinates, width and height respectively. Similarly, ycent_a, xcent_a, h_a and w_a denote the anchor's center coordinates, width and height. ycent, xcent, h and w denote the anchor-encoded center, height and width respectively. Arguments: - *rel_codes*: a tensor representing N anchor-encoded boxes. - *anchors*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. - *scale_factors*: List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: - *boxes*: A Tensor of shape [N, ..., (y_max,x_max,y2,x2)]. \"\"\" anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) ty , tx , th , tw = tf . split ( value = rel_codes , num_or_size_splits = 4 , axis =- 1 ) if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty /= scale_factors [ 0 ] tx /= scale_factors [ 1 ] th /= scale_factors [ 2 ] tw /= scale_factors [ 3 ] ycenter = ty * ha + ycenter_a xcenter = tx * wa + xcenter_a h = tf . exp ( th ) * ha w = tf . exp ( tw ) * wa ymin = ycenter - h / 2. xmin = xcenter - w / 2. ymax = ycenter + h / 2. xmax = xcenter + w / 2. return tf . concat ([ ymin , xmin , ymax , xmax ], axis =- 1 ) Variables EPSILON Functions decode_boxes_faster_rcnn def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ) Decode relative codes to boxes according to the Faster RCNN paper . Faster RCNN box decoder follows the coding schema described below: ycent = t_y h_a + ycent_a xcent= t_x w_a + xcent_a h = exp(t_h) h_a w = exp(t_w) w_a where t_y, t_x, t_h, t_w denote the encoded box's center coordinates, width and height respectively. Similarly, ycent_a, xcent_a, h_a and w_a denote the anchor's center coordinates, width and height. ycent, xcent, h and w denote the anchor-encoded center, height and width respectively. Arguments: rel_codes : a tensor representing N anchor-encoded boxes. anchors : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. scale_factors : List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: boxes : A Tensor of shape [N, ..., (y_max,x_max,y2,x2)]. View Source def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ) : \"\"\" Decode relative codes to boxes according to the [ Faster RCNN paper ] ( http : // arxiv . org / abs / 1506 . 01497 ) . Faster RCNN box decoder follows the coding schema described below : ycent = t_y h_a + ycent_a xcent = t_x w_a + xcent_a h = exp ( t_h ) h_a w = exp ( t_w ) w_a where t_y , t_x , t_h , t_w denote the encoded box ' s center coordinates, width and height respectively . Similarly , ycent_a , xcent_a , h_a and w_a denote the anchor ' s center coordinates , width and height . ycent , xcent , h and w denote the anchor - encoded center , height and width respectively . Arguments : - * rel_codes * : a tensor representing N anchor - encoded boxes . - * anchors * : Tensor of shape [ N , ..., ( y_min , x_min , y_max , x_max ) ]. - * scale_factors * : List of 4 positive scalars to scale ty , tx , th and tw . If set to None , does not perform scaling . For Faster RCNN , the open - source implementation recommends using [ 10 . 0 , 10 . 0 , 5 . 0 , 5 . 0 ]. Returns : - * boxes * : A Tensor of shape [ N , ..., ( y_max , x_max , y2 , x2 ) ]. \"\"\" anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) ty , tx , th , tw = tf . split ( value = rel_codes , num_or_size_splits = 4 , axis =- 1 ) if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty /= scale_factors [ 0 ] tx /= scale_factors [ 1 ] th /= scale_factors [ 2 ] tw /= scale_factors [ 3 ] ycenter = ty * ha + ycenter_a xcenter = tx * wa + xcenter_a h = tf . exp ( th ) * ha w = tf . exp ( tw ) * wa ymin = ycenter - h / 2 . xmin = xcenter - w / 2 . ymax = ycenter + h / 2 . xmax = xcenter + w / 2 . return tf . concat ( [ ymin , xmin , ymax , xmax ], axis =- 1 ) encode_boxes_faster_rcnn def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ) Encode a box collection with respect to anchor collection according to the Faster RCNN paper . Faster RCNN box coder follows the coding schema described below: t_y = (y - y_a) / h_a t_x & = (x - x_a) / w_a t_h & = log(h / h_a) t_w & = log(w / w_a) where y, x h, w denote the box's center coordinates, width and height respectively. Similarly, y_a, x_a, h_a, w_a denote the anchor's center coordinates, width and height. t_y, t_x, t_h and t_w denote the anchor-encoded center, height and width respectively. Arguments: boxes : BoxList holding N boxes to be encoded. anchors : BoxList of anchors. scale_factors : List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: A tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw]. View Source def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ) : \"\"\" Encode a box collection with respect to anchor collection according to the [ Faster RCNN paper ] ( http : // arxiv . org / abs / 1506 . 01497 ) . Faster RCNN box coder follows the coding schema described below : t_y = ( y - y_a ) / h_a t_x & = ( x - x_a ) / w_a t_h & = log ( h / h_a ) t_w & = log ( w / w_a ) where y , x h , w denote the box ' s center coordinates, width and height respectively . Similarly , y_a , x_a , h_a , w_a denote the anchor ' s center coordinates , width and height . t_y , t_x , t_h and t_w denote the anchor - encoded center , height and width respectively . Arguments : - * boxes * : BoxList holding N boxes to be encoded . - * anchors * : BoxList of anchors . - * scale_factors * : List of 4 positive scalars to scale ty , tx , th and tw . If set to None , does not perform scaling . For Faster RCNN , the open - source implementation recommends using [ 10 . 0 , 10 . 0 , 5 . 0 , 5 . 0 ]. Returns : A tensor representing N anchor - encoded boxes of the format [ ty , tx , th , tw ]. \"\"\" # Convert anchors to the center coordinate representation . anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) boxes = box_ops . convert_to_center_coordinates ( boxes ) ycenter , xcenter , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) # Avoid NaN in division and log below . ha += EPSILON wa += EPSILON h += EPSILON w += EPSILON ty = ( ycenter - ycenter_a ) / ha tx = ( xcenter - xcenter_a ) / wa th = tf . math . log ( h / ha ) tw = tf . math . log ( w / wa ) # Scales location targets as used in paper for joint training . if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty *= scale_factors [ 0 ] tx *= scale_factors [ 1 ] th *= scale_factors [ 2 ] tw *= scale_factors [ 3 ] return tf . concat ( [ ty , tx , th , tw ], axis =- 1 )","title":"Box Coder"},{"location":"reference/kerod/core/box_coder/#module-kerodcorebox_coder","text":"None None View Source import tensorflow as tf from kerod.core import box_ops EPSILON = 1e-8 def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ): \"\"\"Encode a box collection with respect to anchor collection according to the [Faster RCNN paper](http://arxiv.org/abs/1506.01497). Faster RCNN box coder follows the coding schema described below: t_y = (y - y_a) / h_a t_x & = (x - x_a) / w_a t_h & = log(h / h_a) t_w & = log(w / w_a) where y, x h, w denote the box's center coordinates, width and height respectively. Similarly, y_a, x_a, h_a, w_a denote the anchor's center coordinates, width and height. t_y, t_x, t_h and t_w denote the anchor-encoded center, height and width respectively. Arguments: - *boxes*: BoxList holding N boxes to be encoded. - *anchors*: BoxList of anchors. - *scale_factors*: List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: A tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw]. \"\"\" # Convert anchors to the center coordinate representation. anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) boxes = box_ops . convert_to_center_coordinates ( boxes ) ycenter , xcenter , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) # Avoid NaN in division and log below. ha += EPSILON wa += EPSILON h += EPSILON w += EPSILON ty = ( ycenter - ycenter_a ) / ha tx = ( xcenter - xcenter_a ) / wa th = tf . math . log ( h / ha ) tw = tf . math . log ( w / wa ) # Scales location targets as used in paper for joint training. if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty *= scale_factors [ 0 ] tx *= scale_factors [ 1 ] th *= scale_factors [ 2 ] tw *= scale_factors [ 3 ] return tf . concat ([ ty , tx , th , tw ], axis =- 1 ) def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ): \"\"\"Decode relative codes to boxes according to the [Faster RCNN paper](http://arxiv.org/abs/1506.01497). Faster RCNN box decoder follows the coding schema described below: ycent = t_y h_a + ycent_a xcent= t_x w_a + xcent_a h = exp(t_h) h_a w = exp(t_w) w_a where t_y, t_x, t_h, t_w denote the encoded box's center coordinates, width and height respectively. Similarly, ycent_a, xcent_a, h_a and w_a denote the anchor's center coordinates, width and height. ycent, xcent, h and w denote the anchor-encoded center, height and width respectively. Arguments: - *rel_codes*: a tensor representing N anchor-encoded boxes. - *anchors*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. - *scale_factors*: List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: - *boxes*: A Tensor of shape [N, ..., (y_max,x_max,y2,x2)]. \"\"\" anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) ty , tx , th , tw = tf . split ( value = rel_codes , num_or_size_splits = 4 , axis =- 1 ) if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty /= scale_factors [ 0 ] tx /= scale_factors [ 1 ] th /= scale_factors [ 2 ] tw /= scale_factors [ 3 ] ycenter = ty * ha + ycenter_a xcenter = tx * wa + xcenter_a h = tf . exp ( th ) * ha w = tf . exp ( tw ) * wa ymin = ycenter - h / 2. xmin = xcenter - w / 2. ymax = ycenter + h / 2. xmax = xcenter + w / 2. return tf . concat ([ ymin , xmin , ymax , xmax ], axis =- 1 )","title":"Module kerod.core.box_coder"},{"location":"reference/kerod/core/box_coder/#variables","text":"EPSILON","title":"Variables"},{"location":"reference/kerod/core/box_coder/#functions","text":"","title":"Functions"},{"location":"reference/kerod/core/box_coder/#decode_boxes_faster_rcnn","text":"def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ) Decode relative codes to boxes according to the Faster RCNN paper . Faster RCNN box decoder follows the coding schema described below: ycent = t_y h_a + ycent_a xcent= t_x w_a + xcent_a h = exp(t_h) h_a w = exp(t_w) w_a where t_y, t_x, t_h, t_w denote the encoded box's center coordinates, width and height respectively. Similarly, ycent_a, xcent_a, h_a and w_a denote the anchor's center coordinates, width and height. ycent, xcent, h and w denote the anchor-encoded center, height and width respectively. Arguments: rel_codes : a tensor representing N anchor-encoded boxes. anchors : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. scale_factors : List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: boxes : A Tensor of shape [N, ..., (y_max,x_max,y2,x2)]. View Source def decode_boxes_faster_rcnn ( rel_codes , anchors , scale_factors = None ) : \"\"\" Decode relative codes to boxes according to the [ Faster RCNN paper ] ( http : // arxiv . org / abs / 1506 . 01497 ) . Faster RCNN box decoder follows the coding schema described below : ycent = t_y h_a + ycent_a xcent = t_x w_a + xcent_a h = exp ( t_h ) h_a w = exp ( t_w ) w_a where t_y , t_x , t_h , t_w denote the encoded box ' s center coordinates, width and height respectively . Similarly , ycent_a , xcent_a , h_a and w_a denote the anchor ' s center coordinates , width and height . ycent , xcent , h and w denote the anchor - encoded center , height and width respectively . Arguments : - * rel_codes * : a tensor representing N anchor - encoded boxes . - * anchors * : Tensor of shape [ N , ..., ( y_min , x_min , y_max , x_max ) ]. - * scale_factors * : List of 4 positive scalars to scale ty , tx , th and tw . If set to None , does not perform scaling . For Faster RCNN , the open - source implementation recommends using [ 10 . 0 , 10 . 0 , 5 . 0 , 5 . 0 ]. Returns : - * boxes * : A Tensor of shape [ N , ..., ( y_max , x_max , y2 , x2 ) ]. \"\"\" anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) ty , tx , th , tw = tf . split ( value = rel_codes , num_or_size_splits = 4 , axis =- 1 ) if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty /= scale_factors [ 0 ] tx /= scale_factors [ 1 ] th /= scale_factors [ 2 ] tw /= scale_factors [ 3 ] ycenter = ty * ha + ycenter_a xcenter = tx * wa + xcenter_a h = tf . exp ( th ) * ha w = tf . exp ( tw ) * wa ymin = ycenter - h / 2 . xmin = xcenter - w / 2 . ymax = ycenter + h / 2 . xmax = xcenter + w / 2 . return tf . concat ( [ ymin , xmin , ymax , xmax ], axis =- 1 )","title":"decode_boxes_faster_rcnn"},{"location":"reference/kerod/core/box_coder/#encode_boxes_faster_rcnn","text":"def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ) Encode a box collection with respect to anchor collection according to the Faster RCNN paper . Faster RCNN box coder follows the coding schema described below: t_y = (y - y_a) / h_a t_x & = (x - x_a) / w_a t_h & = log(h / h_a) t_w & = log(w / w_a) where y, x h, w denote the box's center coordinates, width and height respectively. Similarly, y_a, x_a, h_a, w_a denote the anchor's center coordinates, width and height. t_y, t_x, t_h and t_w denote the anchor-encoded center, height and width respectively. Arguments: boxes : BoxList holding N boxes to be encoded. anchors : BoxList of anchors. scale_factors : List of 4 positive scalars to scale ty, tx, th and tw. If set to None, does not perform scaling. For Faster RCNN, the open-source implementation recommends using [10.0, 10.0, 5.0, 5.0]. Returns: A tensor representing N anchor-encoded boxes of the format [ty, tx, th, tw]. View Source def encode_boxes_faster_rcnn ( boxes , anchors , scale_factors = None ) : \"\"\" Encode a box collection with respect to anchor collection according to the [ Faster RCNN paper ] ( http : // arxiv . org / abs / 1506 . 01497 ) . Faster RCNN box coder follows the coding schema described below : t_y = ( y - y_a ) / h_a t_x & = ( x - x_a ) / w_a t_h & = log ( h / h_a ) t_w & = log ( w / w_a ) where y , x h , w denote the box ' s center coordinates, width and height respectively . Similarly , y_a , x_a , h_a , w_a denote the anchor ' s center coordinates , width and height . t_y , t_x , t_h and t_w denote the anchor - encoded center , height and width respectively . Arguments : - * boxes * : BoxList holding N boxes to be encoded . - * anchors * : BoxList of anchors . - * scale_factors * : List of 4 positive scalars to scale ty , tx , th and tw . If set to None , does not perform scaling . For Faster RCNN , the open - source implementation recommends using [ 10 . 0 , 10 . 0 , 5 . 0 , 5 . 0 ]. Returns : A tensor representing N anchor - encoded boxes of the format [ ty , tx , th , tw ]. \"\"\" # Convert anchors to the center coordinate representation . anchors = box_ops . convert_to_center_coordinates ( anchors ) ycenter_a , xcenter_a , ha , wa = tf . split ( value = anchors , num_or_size_splits = 4 , axis =- 1 ) boxes = box_ops . convert_to_center_coordinates ( boxes ) ycenter , xcenter , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) # Avoid NaN in division and log below . ha += EPSILON wa += EPSILON h += EPSILON w += EPSILON ty = ( ycenter - ycenter_a ) / ha tx = ( xcenter - xcenter_a ) / wa th = tf . math . log ( h / ha ) tw = tf . math . log ( w / wa ) # Scales location targets as used in paper for joint training . if scale_factors : scale_factors = tf . convert_to_tensor ( scale_factors , dtype = anchors . dtype ) ty *= scale_factors [ 0 ] tx *= scale_factors [ 1 ] th *= scale_factors [ 2 ] tw *= scale_factors [ 3 ] return tf . concat ( [ ty , tx , th , tw ], axis =- 1 )","title":"encode_boxes_faster_rcnn"},{"location":"reference/kerod/core/box_ops/","text":"Module kerod.core.box_ops None None View Source import tensorflow as tf def transform_fpcoor_for_tf ( boxes : tf . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tf . Tensor : \"\"\"The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: - *normalized_boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. - *tensor_shape*: Height and width respectively - *crop_shape*: Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( boxes , 4 , axis =- 1 ) spacing_h = ( y_max - y_min ) / tf . cast ( crop_shape [ 0 ], boxes . dtype ) spacing_w = ( x_max - x_min ) / tf . cast ( crop_shape [ 1 ], boxes . dtype ) tensor_shape = ( tf . cast ( tensor_shape [ 0 ] - 1 , boxes . dtype ), tf . cast ( tensor_shape [ 1 ] - 1 , boxes . dtype )) ny0 = ( y_min + spacing_h / 2 - 0.5 ) / tensor_shape [ 0 ] nx0 = ( x_min + spacing_w / 2 - 0.5 ) / tensor_shape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , boxes . dtype ) / tensor_shape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , boxes . dtype ) / tensor_shape [ 1 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis =- 1 ) def convert_to_center_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: - *boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) width = x_max - x_min height = y_max - y_min ycenter = y_min + height / 2. xcenter = x_min + width / 2. return tf . concat ([ ycenter , xcenter , height , width ], axis =- 1 ) def convert_to_xyxy_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: - *boxes*: A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_cent , x_cent , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_cent - 0.5 * h x_min = x_cent - 0.5 * w y_max = y_cent + 0.5 * h x_max = x_cent + 0.5 * w return tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 ) def compute_area ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Compute the area of boxes. Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] \"\"\" with tf . name_scope ( 'Area' ): y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), - 1 ) def compute_intersection ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , perm = None ) -> tf . Tensor : \"\"\"Compute pairwise intersection areas between boxes. Arguments: - *boxes1*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *boxes2*: Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections \"\"\" with tf . name_scope ( 'Intersection' ): y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( value = boxes1 , num_or_size_splits = 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( value = boxes2 , num_or_size_splits = 4 , axis =- 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) intersect_heights = tf . maximum ( zero , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) intersect_widths = tf . maximum ( zero , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths def compute_iou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor ) -> tf . Tensor : \"\"\"Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output ``` <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> ``` Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" return compute_giou ( boxes1 , boxes2 , mode = 'iou' ) def compute_giou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , mode : str = \"giou\" ) -> tf . Tensor : \"\"\"Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] - *mode*: You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" with tf . name_scope ( mode . upper ()): if len ( boxes1 . shape ) == 2 : perm = None which_dim_expands = 0 elif len ( boxes1 . shape ) == 3 : perm = ( 0 , 2 , 1 ) which_dim_expands = 1 else : raise ValueError ( 'Compute Iou is only suppoted for 2D and 3D Tensor' ) intersections = compute_intersection ( boxes1 , boxes2 , perm = perm ) areas1 = compute_area ( boxes1 ) areas2 = compute_area ( boxes2 ) unions = areas1 [ ... , None ] + tf . expand_dims ( areas2 , which_dim_expands ) - intersections iou = tf . where ( intersections == 0 , tf . zeros_like ( intersections ), tf . truediv ( intersections , unions )) if mode == \"iou\" : return iou y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( boxes1 , 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( boxes2 , 4 , axis =- 1 ) enclose_ymin = tf . minimum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) enclose_xmin = tf . minimum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) enclose_ymax = tf . maximum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) enclose_xmax = tf . maximum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) enclose_width = tf . maximum ( zero , enclose_xmax - enclose_xmin ) enclose_height = tf . maximum ( zero , enclose_ymax - enclose_ymin ) enclose_area = enclose_width * enclose_height giou = iou - tf . math . divide_no_nan (( enclose_area - unions ), enclose_area ) return giou def normalize_box_coordinates ( boxes , height : int , width : int ): \"\"\" Normalize the boxes coordinates with image shape Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *height*: An integer - *width*: An integer \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_min / height x_min = x_min / width y_max = y_max / height x_max = x_max / width # Won't be backpropagated to rois anyway, but to save time boxes = tf . stop_gradient ( tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 )) return boxes def clip_boxes ( boxes : tf . Tensor , window : tf . Tensor ) -> tf . Tensor : \"\"\"Perform a clipping according to a window on the boxes. Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] - *window*: A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" boxes = tf . maximum ( boxes , tf . cast ( 0 , boxes . dtype )) m = tf . tile ( tf . expand_dims ( window , axis = 1 ), [ 1 , 1 , 2 ]) boxes = tf . minimum ( boxes , tf . cast ( m , boxes . dtype )) return boxes def flip_left_right ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"[Taken from tensorflow models] Left-right flip the boxes. Arguments: - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. \"\"\" ymin , xmin , ymax , xmax = tf . split ( value = boxes , num_or_size_splits = 4 , axis = 1 ) flipped_xmin = tf . subtract ( 1.0 , xmax ) flipped_xmax = tf . subtract ( 1.0 , xmin ) flipped_boxes = tf . concat ([ ymin , flipped_xmin , ymax , flipped_xmax ], 1 ) return flipped_boxes Functions clip_boxes def clip_boxes ( boxes : tensorflow . python . framework . ops . Tensor , window : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Perform a clipping according to a window on the boxes. Arguments: boxes : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] window : A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] View Source def clip_boxes ( boxes : tf . Tensor , window : tf . Tensor ) -> tf . Tensor : \"\"\"Perform a clipping according to a window on the boxes. Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] - *window*: A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" boxes = tf . maximum ( boxes , tf . cast ( 0 , boxes . dtype )) m = tf . tile ( tf . expand_dims ( window , axis = 1 ), [ 1 , 1 , 2 ]) boxes = tf . minimum ( boxes , tf . cast ( m , boxes . dtype )) return boxes compute_area def compute_area ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Compute the area of boxes. Arguments: boxes : Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] View Source def compute_area ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Compute the area of boxes. Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] \"\"\" with tf . name_scope ( 'Area' ) : y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), - 1 ) compute_giou def compute_giou ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor , mode : str = 'giou' ) -> tensorflow . python . framework . ops . Tensor Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: boxes1 : A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] boxes2 : A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] mode : You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. View Source def compute_giou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , mode : str = \"giou\" ) -> tf . Tensor : \"\"\"Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] - *mode*: You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" with tf . name_scope ( mode . upper ()) : if len ( boxes1 . shape ) == 2 : perm = None which_dim_expands = 0 elif len ( boxes1 . shape ) == 3 : perm = ( 0 , 2 , 1 ) which_dim_expands = 1 else : raise ValueError ( 'Compute Iou is only suppoted for 2D and 3D Tensor' ) intersections = compute_intersection ( boxes1 , boxes2 , perm = perm ) areas1 = compute_area ( boxes1 ) areas2 = compute_area ( boxes2 ) unions = areas1 [..., None ] + tf . expand_dims ( areas2 , which_dim_expands ) - intersections iou = tf . where ( intersections == 0 , tf . zeros_like ( intersections ), tf . truediv ( intersections , unions )) if mode == \"iou\" : return iou y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( boxes1 , 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( boxes2 , 4 , axis =- 1 ) enclose_ymin = tf . minimum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) enclose_xmin = tf . minimum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) enclose_ymax = tf . maximum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) enclose_xmax = tf . maximum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) enclose_width = tf . maximum ( zero , enclose_xmax - enclose_xmin ) enclose_height = tf . maximum ( zero , enclose_ymax - enclose_ymin ) enclose_area = enclose_width * enclose_height giou = iou - tf . math . divide_no_nan (( enclose_area - unions ), enclose_area ) return giou compute_intersection def compute_intersection ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor , perm = None ) -> tensorflow . python . framework . ops . Tensor Compute pairwise intersection areas between boxes. Arguments: boxes1 : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] boxes2 : Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections View Source def compute_intersection ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , perm = None ) -> tf . Tensor : \"\"\"Compute pairwise intersection areas between boxes. Arguments: - *boxes1*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *boxes2*: Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections \"\"\" with tf . name_scope ( 'Intersection' ) : y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( value = boxes1 , num_or_size_splits = 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( value = boxes2 , num_or_size_splits = 4 , axis =- 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) intersect_heights = tf . maximum ( zero , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) intersect_widths = tf . maximum ( zero , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths compute_iou def compute_iou ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> Arguments: boxes1 : A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] boxes2 : A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. View Source def compute_iou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor ) -> tf . Tensor : \"\"\"Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output ``` <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> ``` Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" return compute_giou ( boxes1 , boxes2 , mode = 'iou' ) convert_to_center_coordinates def convert_to_center_coordinates ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: boxes : A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] View Source def convert_to_center_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: - *boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) width = x_max - x_min height = y_max - y_min ycenter = y_min + height / 2. xcenter = x_min + width / 2. return tf . concat ([ ycenter , xcenter , height , width ], axis =- 1 ) convert_to_xyxy_coordinates def convert_to_xyxy_coordinates ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: boxes : A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] View Source def convert_to_xyxy_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: - *boxes*: A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_cent , x_cent , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_cent - 0.5 * h x_min = x_cent - 0.5 * w y_max = y_cent + 0.5 * h x_max = x_cent + 0.5 * w return tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 ) flip_left_right def flip_left_right ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor [Taken from tensorflow models] Left-right flip the boxes. Arguments: boxes : rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. View Source def flip_left_right ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"[Taken from tensorflow models] Left-right flip the boxes. Arguments: - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. \"\"\" ymin , xmin , ymax , xmax = tf . split ( value = boxes , num_or_size_splits = 4 , axis = 1 ) flipped_xmin = tf . subtract ( 1.0 , xmax ) flipped_xmax = tf . subtract ( 1.0 , xmin ) flipped_boxes = tf . concat ([ ymin , flipped_xmin , ymax , flipped_xmax ], 1 ) return flipped_boxes normalize_box_coordinates def normalize_box_coordinates ( boxes , height : int , width : int ) Normalize the boxes coordinates with image shape Arguments: boxes : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] height : An integer width : An integer View Source def normalize_box_coordinates ( boxes , height : int , width : int ) : \"\"\" Normalize the boxes coordinates with image shape Arguments : - * boxes * : Tensor of shape [ N , ..., ( y_min , x_min , y_max , x_max ) ] - * height * : An integer - * width * : An integer \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_min / height x_min = x_min / width y_max = y_max / height x_max = x_max / width # Won ' t be backpropagated to rois anyway, but to save time boxes = tf . stop_gradient ( tf . concat ( [ y_min , x_min , y_max , x_max ], axis =- 1 )) return boxes transform_fpcoor_for_tf def transform_fpcoor_for_tf ( boxes : tensorflow . python . framework . ops . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tensorflow . python . framework . ops . Tensor The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: normalized_boxes : A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. tensor_shape : Height and width respectively crop_shape : Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] View Source def transform_fpcoor_for_tf ( boxes : tf . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tf . Tensor : \"\"\"The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: - *normalized_boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. - *tensor_shape*: Height and width respectively - *crop_shape*: Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( boxes , 4 , axis =- 1 ) spacing_h = ( y_max - y_min ) / tf . cast ( crop_shape [ 0 ], boxes . dtype ) spacing_w = ( x_max - x_min ) / tf . cast ( crop_shape [ 1 ], boxes . dtype ) tensor_shape = ( tf . cast ( tensor_shape [ 0 ] - 1 , boxes . dtype ), tf . cast ( tensor_shape [ 1 ] - 1 , boxes . dtype )) ny0 = ( y_min + spacing_h / 2 - 0.5 ) / tensor_shape [ 0 ] nx0 = ( x_min + spacing_w / 2 - 0.5 ) / tensor_shape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , boxes . dtype ) / tensor_shape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , boxes . dtype ) / tensor_shape [ 1 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis =- 1 )","title":"Box Ops"},{"location":"reference/kerod/core/box_ops/#module-kerodcorebox_ops","text":"None None View Source import tensorflow as tf def transform_fpcoor_for_tf ( boxes : tf . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tf . Tensor : \"\"\"The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: - *normalized_boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. - *tensor_shape*: Height and width respectively - *crop_shape*: Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( boxes , 4 , axis =- 1 ) spacing_h = ( y_max - y_min ) / tf . cast ( crop_shape [ 0 ], boxes . dtype ) spacing_w = ( x_max - x_min ) / tf . cast ( crop_shape [ 1 ], boxes . dtype ) tensor_shape = ( tf . cast ( tensor_shape [ 0 ] - 1 , boxes . dtype ), tf . cast ( tensor_shape [ 1 ] - 1 , boxes . dtype )) ny0 = ( y_min + spacing_h / 2 - 0.5 ) / tensor_shape [ 0 ] nx0 = ( x_min + spacing_w / 2 - 0.5 ) / tensor_shape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , boxes . dtype ) / tensor_shape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , boxes . dtype ) / tensor_shape [ 1 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis =- 1 ) def convert_to_center_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: - *boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) width = x_max - x_min height = y_max - y_min ycenter = y_min + height / 2. xcenter = x_min + width / 2. return tf . concat ([ ycenter , xcenter , height , width ], axis =- 1 ) def convert_to_xyxy_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: - *boxes*: A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_cent , x_cent , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_cent - 0.5 * h x_min = x_cent - 0.5 * w y_max = y_cent + 0.5 * h x_max = x_cent + 0.5 * w return tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 ) def compute_area ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Compute the area of boxes. Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] \"\"\" with tf . name_scope ( 'Area' ): y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), - 1 ) def compute_intersection ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , perm = None ) -> tf . Tensor : \"\"\"Compute pairwise intersection areas between boxes. Arguments: - *boxes1*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *boxes2*: Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections \"\"\" with tf . name_scope ( 'Intersection' ): y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( value = boxes1 , num_or_size_splits = 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( value = boxes2 , num_or_size_splits = 4 , axis =- 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) intersect_heights = tf . maximum ( zero , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) intersect_widths = tf . maximum ( zero , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths def compute_iou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor ) -> tf . Tensor : \"\"\"Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output ``` <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> ``` Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" return compute_giou ( boxes1 , boxes2 , mode = 'iou' ) def compute_giou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , mode : str = \"giou\" ) -> tf . Tensor : \"\"\"Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] - *mode*: You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" with tf . name_scope ( mode . upper ()): if len ( boxes1 . shape ) == 2 : perm = None which_dim_expands = 0 elif len ( boxes1 . shape ) == 3 : perm = ( 0 , 2 , 1 ) which_dim_expands = 1 else : raise ValueError ( 'Compute Iou is only suppoted for 2D and 3D Tensor' ) intersections = compute_intersection ( boxes1 , boxes2 , perm = perm ) areas1 = compute_area ( boxes1 ) areas2 = compute_area ( boxes2 ) unions = areas1 [ ... , None ] + tf . expand_dims ( areas2 , which_dim_expands ) - intersections iou = tf . where ( intersections == 0 , tf . zeros_like ( intersections ), tf . truediv ( intersections , unions )) if mode == \"iou\" : return iou y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( boxes1 , 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( boxes2 , 4 , axis =- 1 ) enclose_ymin = tf . minimum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) enclose_xmin = tf . minimum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) enclose_ymax = tf . maximum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) enclose_xmax = tf . maximum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) enclose_width = tf . maximum ( zero , enclose_xmax - enclose_xmin ) enclose_height = tf . maximum ( zero , enclose_ymax - enclose_ymin ) enclose_area = enclose_width * enclose_height giou = iou - tf . math . divide_no_nan (( enclose_area - unions ), enclose_area ) return giou def normalize_box_coordinates ( boxes , height : int , width : int ): \"\"\" Normalize the boxes coordinates with image shape Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *height*: An integer - *width*: An integer \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_min / height x_min = x_min / width y_max = y_max / height x_max = x_max / width # Won't be backpropagated to rois anyway, but to save time boxes = tf . stop_gradient ( tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 )) return boxes def clip_boxes ( boxes : tf . Tensor , window : tf . Tensor ) -> tf . Tensor : \"\"\"Perform a clipping according to a window on the boxes. Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] - *window*: A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" boxes = tf . maximum ( boxes , tf . cast ( 0 , boxes . dtype )) m = tf . tile ( tf . expand_dims ( window , axis = 1 ), [ 1 , 1 , 2 ]) boxes = tf . minimum ( boxes , tf . cast ( m , boxes . dtype )) return boxes def flip_left_right ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"[Taken from tensorflow models] Left-right flip the boxes. Arguments: - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. \"\"\" ymin , xmin , ymax , xmax = tf . split ( value = boxes , num_or_size_splits = 4 , axis = 1 ) flipped_xmin = tf . subtract ( 1.0 , xmax ) flipped_xmax = tf . subtract ( 1.0 , xmin ) flipped_boxes = tf . concat ([ ymin , flipped_xmin , ymax , flipped_xmax ], 1 ) return flipped_boxes","title":"Module kerod.core.box_ops"},{"location":"reference/kerod/core/box_ops/#functions","text":"","title":"Functions"},{"location":"reference/kerod/core/box_ops/#clip_boxes","text":"def clip_boxes ( boxes : tensorflow . python . framework . ops . Tensor , window : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Perform a clipping according to a window on the boxes. Arguments: boxes : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] window : A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] View Source def clip_boxes ( boxes : tf . Tensor , window : tf . Tensor ) -> tf . Tensor : \"\"\"Perform a clipping according to a window on the boxes. Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] - *window*: A tensor of shape [batch_size, (h, w)] Returns: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" boxes = tf . maximum ( boxes , tf . cast ( 0 , boxes . dtype )) m = tf . tile ( tf . expand_dims ( window , axis = 1 ), [ 1 , 1 , 2 ]) boxes = tf . minimum ( boxes , tf . cast ( m , boxes . dtype )) return boxes","title":"clip_boxes"},{"location":"reference/kerod/core/box_ops/#compute_area","text":"def compute_area ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Compute the area of boxes. Arguments: boxes : Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] View Source def compute_area ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Compute the area of boxes. Arguments: - *boxes*: Tensor of shape [N, ..., (y_min,x_min,y_max_,x_max)] Returns: A tensor of shape [N, ..., num_boxes] \"\"\" with tf . name_scope ( 'Area' ) : y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) return tf . squeeze (( y_max - y_min ) * ( x_max - x_min ), - 1 )","title":"compute_area"},{"location":"reference/kerod/core/box_ops/#compute_giou","text":"def compute_giou ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor , mode : str = 'giou' ) -> tensorflow . python . framework . ops . Tensor Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: boxes1 : A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] boxes2 : A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] mode : You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. View Source def compute_giou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , mode : str = \"giou\" ) -> tf . Tensor : \"\"\"Computes pairwise general intersection-over-union between boxes following: https://giou.stanford.edu/ Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] - *mode*: You can select iou or giou. Returns: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" with tf . name_scope ( mode . upper ()) : if len ( boxes1 . shape ) == 2 : perm = None which_dim_expands = 0 elif len ( boxes1 . shape ) == 3 : perm = ( 0 , 2 , 1 ) which_dim_expands = 1 else : raise ValueError ( 'Compute Iou is only suppoted for 2D and 3D Tensor' ) intersections = compute_intersection ( boxes1 , boxes2 , perm = perm ) areas1 = compute_area ( boxes1 ) areas2 = compute_area ( boxes2 ) unions = areas1 [..., None ] + tf . expand_dims ( areas2 , which_dim_expands ) - intersections iou = tf . where ( intersections == 0 , tf . zeros_like ( intersections ), tf . truediv ( intersections , unions )) if mode == \"iou\" : return iou y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( boxes1 , 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( boxes2 , 4 , axis =- 1 ) enclose_ymin = tf . minimum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) enclose_xmin = tf . minimum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) enclose_ymax = tf . maximum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) enclose_xmax = tf . maximum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) enclose_width = tf . maximum ( zero , enclose_xmax - enclose_xmin ) enclose_height = tf . maximum ( zero , enclose_ymax - enclose_ymin ) enclose_area = enclose_width * enclose_height giou = iou - tf . math . divide_no_nan (( enclose_area - unions ), enclose_area ) return giou","title":"compute_giou"},{"location":"reference/kerod/core/box_ops/#compute_intersection","text":"def compute_intersection ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor , perm = None ) -> tensorflow . python . framework . ops . Tensor Compute pairwise intersection areas between boxes. Arguments: boxes1 : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] boxes2 : Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections View Source def compute_intersection ( boxes1 : tf . Tensor , boxes2 : tf . Tensor , perm = None ) -> tf . Tensor : \"\"\"Compute pairwise intersection areas between boxes. Arguments: - *boxes1*: Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] - *boxes2*: Tensor of shape [N, ..., (y_max,x_max,y_max,x_max)] Returns: A tensor with shape [N, M] representing pairwise intersections \"\"\" with tf . name_scope ( 'Intersection' ) : y_min1 , x_min1 , y_max1 , x_max1 = tf . split ( value = boxes1 , num_or_size_splits = 4 , axis =- 1 ) y_min2 , x_min2 , y_max2 , x_max2 = tf . split ( value = boxes2 , num_or_size_splits = 4 , axis =- 1 ) all_pairs_min_ymax = tf . minimum ( y_max1 , tf . transpose ( y_max2 , perm = perm )) all_pairs_max_ymin = tf . maximum ( y_min1 , tf . transpose ( y_min2 , perm = perm )) zero = tf . convert_to_tensor ( 0.0 , boxes1 . dtype ) intersect_heights = tf . maximum ( zero , all_pairs_min_ymax - all_pairs_max_ymin ) all_pairs_min_xmax = tf . minimum ( x_max1 , tf . transpose ( x_max2 , perm = perm )) all_pairs_max_xmin = tf . maximum ( x_min1 , tf . transpose ( x_min2 , perm = perm )) intersect_widths = tf . maximum ( zero , all_pairs_min_xmax - all_pairs_max_xmin ) return intersect_heights * intersect_widths","title":"compute_intersection"},{"location":"reference/kerod/core/box_ops/#compute_iou","text":"def compute_iou ( boxes1 : tensorflow . python . framework . ops . Tensor , boxes2 : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> Arguments: boxes1 : A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] boxes2 : A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. View Source def compute_iou ( boxes1 : tf . Tensor , boxes2 : tf . Tensor ) -> tf . Tensor : \"\"\"Computes pairwise intersection-over-union between boxes. Example: The axis x correspond to boxes2 and y the boxes1: ```python from kerod.core.box_ops import compute_iou import numpy as np boxes1 = np.array([[548.26666, 364.57202, 706.1333 , 524.472 ], [473.6 , 547.924 , 565.3333 , 635.336 ], [477.86664, 688.63605, 580.26666, 786.70795], [497.06668, 750.464 , 576. , 857.064 ]]) boxes2 = np.array([[474.74518, 553.37256, 565.2548 , 598.62744], [448., 736., 576., 864.], [464., 672., 592., 800.], [560., 368., 688., 496.] ]) compute_iou(boxes1, boxes2) ``` output ``` <tf.Tensor: shape=(4, 4), dtype=float64, numpy= array([[0. , 0. , 0. , 0.6490545 ], [0.51081317, 0. , 0. , 0. ], [0. , 0.23198337, 0.61294949, 0. ], [0. , 0.51356762, 0.18718853, 0. ]])> ``` Arguments: - *boxes1*: A 2D Tensor of shape [N, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, N, (y_min,x_min,y_max,x_max)] - *boxes2*: A 2D Tensor of shape [M, (y_min,x_min,y_max,x_max)] or a or 3D Tensor of shape [batch_size, M, (y_min,x_min,y_max,x_max)] Returns: Return: A tensor with shape [N, M] or [batch_size, N, M] representing pairwise iou scores. Raises: ValueError: If your tensor is different than 2D or 3D. \"\"\" return compute_giou ( boxes1 , boxes2 , mode = 'iou' )","title":"compute_iou"},{"location":"reference/kerod/core/box_ops/#convert_to_center_coordinates","text":"def convert_to_center_coordinates ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: boxes : A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] View Source def convert_to_center_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_min, x_min, y_max, x_max -> y_cent, x_cent, h, w Arguments: - *boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] Returns: A tensor of shape [N, ..., num_boxes, (ycenter, xcenter, height, width)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) width = x_max - x_min height = y_max - y_min ycenter = y_min + height / 2. xcenter = x_min + width / 2. return tf . concat ([ ycenter , xcenter , height , width ], axis =- 1 )","title":"convert_to_center_coordinates"},{"location":"reference/kerod/core/box_ops/#convert_to_xyxy_coordinates","text":"def convert_to_xyxy_coordinates ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: boxes : A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] View Source def convert_to_xyxy_coordinates ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"Convert boxes to their center coordinates y_cent, x_cent, h, w -> y_min, x_min, y_max, x_max Arguments: - *boxes*: A Tensor of shape [N, ..., (y_cent, x_cent, h, w)] Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_cent , x_cent , h , w = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_cent - 0.5 * h x_min = x_cent - 0.5 * w y_max = y_cent + 0.5 * h x_max = x_cent + 0.5 * w return tf . concat ([ y_min , x_min , y_max , x_max ], axis =- 1 )","title":"convert_to_xyxy_coordinates"},{"location":"reference/kerod/core/box_ops/#flip_left_right","text":"def flip_left_right ( boxes : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor [Taken from tensorflow models] Left-right flip the boxes. Arguments: boxes : rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. View Source def flip_left_right ( boxes : tf . Tensor ) -> tf . Tensor : \"\"\"[Taken from tensorflow models] Left-right flip the boxes. Arguments: - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Return: Flipped boxes. \"\"\" ymin , xmin , ymax , xmax = tf . split ( value = boxes , num_or_size_splits = 4 , axis = 1 ) flipped_xmin = tf . subtract ( 1.0 , xmax ) flipped_xmax = tf . subtract ( 1.0 , xmin ) flipped_boxes = tf . concat ([ ymin , flipped_xmin , ymax , flipped_xmax ], 1 ) return flipped_boxes","title":"flip_left_right"},{"location":"reference/kerod/core/box_ops/#normalize_box_coordinates","text":"def normalize_box_coordinates ( boxes , height : int , width : int ) Normalize the boxes coordinates with image shape Arguments: boxes : Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)] height : An integer width : An integer View Source def normalize_box_coordinates ( boxes , height : int , width : int ) : \"\"\" Normalize the boxes coordinates with image shape Arguments : - * boxes * : Tensor of shape [ N , ..., ( y_min , x_min , y_max , x_max ) ] - * height * : An integer - * width * : An integer \"\"\" y_min , x_min , y_max , x_max = tf . split ( value = boxes , num_or_size_splits = 4 , axis =- 1 ) y_min = y_min / height x_min = x_min / width y_max = y_max / height x_max = x_max / width # Won ' t be backpropagated to rois anyway, but to save time boxes = tf . stop_gradient ( tf . concat ( [ y_min , x_min , y_max , x_max ], axis =- 1 )) return boxes","title":"normalize_box_coordinates"},{"location":"reference/kerod/core/box_ops/#transform_fpcoor_for_tf","text":"def transform_fpcoor_for_tf ( boxes : tensorflow . python . framework . ops . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tensorflow . python . framework . ops . Tensor The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: normalized_boxes : A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. tensor_shape : Height and width respectively crop_shape : Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] View Source def transform_fpcoor_for_tf ( boxes : tf . Tensor , tensor_shape : tuple , crop_shape : tuple ) -> tf . Tensor : \"\"\"The way tf.image.crop_and_resize works (with normalized box): Initial point (the value of output[0]): x0_box * (W_img - 1) Spacing: w_box * (W_img - 1) / (W_crop - 1) Use the above grid to bilinear sample. However, what we want is (with fpcoor box): Spacing: w_box / W_crop Initial point: x0_box + spacing/2 - 0.5 (-0.5 because bilinear sample (in my definition) assumes floating point coordinate (0.0, 0.0) is the same as pixel value (0, 0)) This function transform fpcoor boxes to a format to be used by tf.image.crop_and_resize This function has been taken from tensorpack: (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Arguments: - *normalized_boxes*: A Tensor of shape [N, ..., (y_min,x_min,y_max,x_max)]. These boxes have already been normalized in the feature space. The coordinates are not in the input image space. - *tensor_shape*: Height and width respectively - *crop_shape*: Returns: A tensor of shape [N, ..., num_boxes, (y_min, x_min, y_max, x_max)] \"\"\" y_min , x_min , y_max , x_max = tf . split ( boxes , 4 , axis =- 1 ) spacing_h = ( y_max - y_min ) / tf . cast ( crop_shape [ 0 ], boxes . dtype ) spacing_w = ( x_max - x_min ) / tf . cast ( crop_shape [ 1 ], boxes . dtype ) tensor_shape = ( tf . cast ( tensor_shape [ 0 ] - 1 , boxes . dtype ), tf . cast ( tensor_shape [ 1 ] - 1 , boxes . dtype )) ny0 = ( y_min + spacing_h / 2 - 0.5 ) / tensor_shape [ 0 ] nx0 = ( x_min + spacing_w / 2 - 0.5 ) / tensor_shape [ 1 ] nh = spacing_h * tf . cast ( crop_shape [ 0 ] - 1 , boxes . dtype ) / tensor_shape [ 0 ] nw = spacing_w * tf . cast ( crop_shape [ 1 ] - 1 , boxes . dtype ) / tensor_shape [ 1 ] return tf . concat ([ ny0 , nx0 , ny0 + nh , nx0 + nw ], axis =- 1 )","title":"transform_fpcoor_for_tf"},{"location":"reference/kerod/core/constants/","text":"Module kerod.core.constants None None View Source # The max image dimension refers to the maximum size of an input image MAX_IMAGE_DIMENSION = 1600 Variables MAX_IMAGE_DIMENSION","title":"Constants"},{"location":"reference/kerod/core/constants/#module-kerodcoreconstants","text":"None None View Source # The max image dimension refers to the maximum size of an input image MAX_IMAGE_DIMENSION = 1600","title":"Module kerod.core.constants"},{"location":"reference/kerod/core/constants/#variables","text":"MAX_IMAGE_DIMENSION","title":"Variables"},{"location":"reference/kerod/core/learning_rate_schedule/","text":"Module kerod.core.learning_rate_schedule Manual Stepping is designed to integrate the computation graph and compute the learning_rate at each step. However, the WarmupLearningRateScheduler is Callback handle by the fit in keras. View Source \"\"\" Manual Stepping is designed to integrate the computation graph and compute the learning_rate at each step. However, the WarmupLearningRateScheduler is Callback handle by the fit in keras. \"\"\" from typing import List import tensorflow as tf from tensorflow.keras import backend as K from tensorflow.keras.optimizers.schedules import LearningRateSchedule from tensorflow.python.keras.callbacks import Callback class LearningRateScheduler ( Callback ): \"\"\"Warmup Learning rate scheduler. It will perform at the beginning of the training a linear warmup from `init_lr` to `base_lr`. The learning rate is decreased by 10 according to the schedule provided by `epochs`. Arguments: - *base_lr*: The target learning rate value after the linear warmup - *num_gpus*: Number of gpus used during the training - *epochs*: A list of epoch on which the learning rate should be reduce. - *use_warmup*: Perform the warmup strategy. - *init_lr*: Learning rate value from which the warmup will start. - *num_warmup_steps*: Number of training step on which the warmup will be performed. \"\"\" def __init__ ( self , base_lr : float , num_gpus : int , epochs : List [ int ], use_warmup : bool = True , init_lr : float = 1e-2 / 3 , num_warmup_steps : int = 1000 ): super () . __init__ () self . _init_lr = init_lr * min ( 8 / num_gpus , 1 ) self . slope = ( base_lr - self . _init_lr ) / num_warmup_steps self . _epochs_to_lr = { epoch : base_lr * 1 / 10 ** ( i + 1 ) for i , epoch in enumerate ( epochs )} self . _epochs = epochs self . _num_gpus = num_gpus self . _use_warmup = use_warmup self . _num_warmup_steps = num_warmup_steps def on_train_batch_begin ( self , batch , logs = None ): global_step = K . get_value (( self . model . optimizer . iterations )) if global_step <= self . _num_warmup_steps and global_step != 0 and self . _use_warmup : lr = self . _init_lr + global_step * self . slope K . set_value ( self . model . optimizer . lr , lr ) def on_epoch_begin ( self , epoch , logs = None ): if not hasattr ( self . model . optimizer , 'lr' ): raise ValueError ( 'Optimizer must have a \"lr\" attribute.' ) if not hasattr ( self . model . optimizer , 'iterations' ): raise ValueError ( 'Optimizer must have an \"iterations\" attribute.' ) global_step = K . get_value ( self . model . optimizer . iterations ) target_epochs = [ e for e in self . _epochs if epoch >= e and global_step > self . _num_warmup_steps ] if target_epochs : lr = self . _epochs_to_lr [ max ( target_epochs )] K . set_value ( self . model . optimizer . lr , lr ) class ManualStepping ( LearningRateSchedule ): \"\"\"Manually stepped learning rate schedule. (Taken and modified from Google object detection) This function provides fine grained control over learning rates. One must specify a sequence of learning rates as well as a set of integer steps at which the current learning rate must transition to the next. For example, if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning rate returned by this function is .1 for step=0,...,4, .01 for step=5...9, and .001 for step=10 and onward. You can pass this schedule directly into a `tf.keras.optimizers.Optimizer` as the learning rate. ```python lr_schedule = tf.keras.optimizers.schedules.ManualStepping( boundaries=[5, 10], rates=[.1, .01, .001], warmup=True) model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule), loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(data, labels, epochs=5) ``` The learning rate schedule is also serializable and deserializable using `tf.keras.optimizers.schedules.serialize` and `tf.keras.optimizers.schedules.deserialize`. Arguments: - *boundaries*: A List of scalar `int32` or `int64` or a `Tensor`. It is a list of global steps at which to switch learning rates. This list is assumed to consist of increasing positive integers. - *rates*: a list of (float) learning rates corresponding to intervals between the boundaries. The length of this list must be exactly len(boundaries) + 1. - *warmup*: Whether to linearly interpolate learning rate for steps in [0, boundaries[0]]. - *name*: String. Optional name of the operation. Defaults to 'ExponentialDecay'. Return: A 1-arg callable learning rate schedule that takes the current optimizer step and outputs the decayed learning rate, a scalar `Tensor` of the same type as `rates`. \"\"\" def __init__ ( self , boundaries , rates , warmup = False , name = None ): super () . __init__ () self . name = name if warmup and boundaries : slope = ( rates [ 1 ] - rates [ 0 ]) * 1.0 / boundaries [ 0 ] warmup_steps = list ( range ( boundaries [ 0 ])) warmup_rates = [ rates [ 0 ] + slope * step for step in warmup_steps ] boundaries = warmup_steps + boundaries rates = warmup_rates + rates [ 1 :] else : boundaries = [ 0 ] + boundaries self . warmup = warmup self . rates = rates self . boundaries = boundaries self . num_boundaries = len ( boundaries ) self . dtype = tf . convert_to_tensor ( rates [ 0 ]) . dtype def __call__ ( self , step ): with tf . name_scope ( self . name or \"ManualStepping\" ): boundaries = tf . convert_to_tensor ( self . boundaries , self . dtype ) rates = tf . convert_to_tensor ( self . rates , self . dtype ) step = tf . convert_to_tensor ( step , self . dtype ) rate_index = tf . reduce_max ( tf . where ( tf . greater_equal ( step , boundaries ), list ( range ( self . num_boundaries )), [ 0 ] * self . num_boundaries )) return tf . reduce_sum ( rates * tf . one_hot ( rate_index , depth = self . num_boundaries )) def get_config ( self ): return { \"boundaries\" : self . boundaries , \"rates\" : self . rates , \"warmup\" : self . warmup , \"name\" : self . name } Classes LearningRateScheduler class LearningRateScheduler ( base_lr : float , num_gpus : int , epochs : List [ int ], use_warmup : bool = True , init_lr : float = 0.0033333333333333335 , num_warmup_steps : int = 1000 ) a linear warmup from init_lr to base_lr . The learning rate is decreased by 10 according to the schedule provided by epochs . Arguments: base_lr : The target learning rate value after the linear warmup num_gpus : Number of gpus used during the training epochs : A list of epoch on which the learning rate should be reduce. use_warmup : Perform the warmup strategy. init_lr : Learning rate value from which the warmup will start. num_warmup_steps : Number of training step on which the warmup will be performed. Ancestors (in MRO) tensorflow.python.keras.callbacks.Callback Methods on_batch_begin def on_batch_begin ( self , batch , logs = None ) A backwards compatibility alias for on_train_batch_begin . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_batch_begin ( self , batch , logs = None ) : \" \"\" A backwards compatibility alias for `on_train_batch_begin`. \"\" \" on_batch_end def on_batch_end ( self , batch , logs = None ) A backwards compatibility alias for on_train_batch_end . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_batch_end ( self , batch , logs = None ) : \" \"\" A backwards compatibility alias for `on_train_batch_end`. \"\" \" on_epoch_begin def on_epoch_begin ( self , epoch , logs = None ) View Source def on_epoch_begin ( self , epoch , logs = None ) : if not hasattr ( self . model . optimizer , ' lr ' ) : raise ValueError ( ' Optimizer must have a \"lr\" attribute. ' ) if not hasattr ( self . model . optimizer , ' iterations ' ) : raise ValueError ( ' Optimizer must have an \"iterations\" attribute. ' ) global_step = K . get_value ( self . model . optimizer . iterations ) target_epochs = [ e for e in self . _epochs if epoch >= e and global_step > self . _num_warmup_steps ] if target_epochs : lr = self . _epochs_to_lr [ max ( target_epochs ) ] K . set_value ( self . model . optimizer . lr , lr ) on_epoch_end def on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Description epoch Integer, index of epoch. logs Dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . For training epoch, the values of the Model 's metrics are returned. Example : {'loss': 0.2, 'acc': 0.7} . View Source @doc_controls.for_subclass_implementers def on_epoch_end ( self , epoch , logs = None ) : \" \"\" Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: Integer, index of epoch. logs: Dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. For training epoch, the values of the `Model`'s metrics are returned. Example : `{'loss': 0.2, 'acc': 0.7}`. \"\" \" on_predict_batch_begin def on_predict_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict, contains the return value of model.predict_step , it typically returns a dict with a key 'outputs' containing the model's outputs. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_predict_batch_begin ( self , batch , logs = None ) : \" \"\" Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict, contains the return value of `model.predict_step`, it typically returns a dict with a key 'outputs' containing the model's outputs. \"\" \" on_predict_batch_end def on_predict_batch_end ( self , batch , logs = None ) Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_predict_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \" on_predict_begin def on_predict_begin ( self , logs = None ) Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_predict_begin ( self , logs = None ) : \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" on_predict_end def on_predict_end ( self , logs = None ) Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_predict_end ( self , logs = None ) : \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" on_test_batch_begin def on_test_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict, contains the return value of model.test_step . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_test_batch_begin ( self , batch , logs = None ) : \" \"\" Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict, contains the return value of `model.test_step`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \" on_test_batch_end def on_test_batch_end ( self , batch , logs = None ) Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_test_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \" on_test_begin def on_test_begin ( self , logs = None ) Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_test_begin ( self , logs = None ) : \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" on_test_end def on_test_end ( self , logs = None ) Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently the output of the last call to on_test_batch_end() is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_test_end ( self , logs = None ) : \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently the output of the last call to `on_test_batch_end()` is passed to this argument for this method but that may change in the future. \"\"\" on_train_batch_begin def on_train_batch_begin ( self , batch , logs = None ) View Source def on_train_batch_begin ( self , batch , logs = None ) : global_step = K . get_value (( self . model . optimizer . iterations )) if global_step <= self . _num_warmup_steps and global_step != 0 and self . _use_warmup : lr = self . _init_lr + global_step * self . slope K . set_value ( self . model . optimizer . lr , lr ) on_train_batch_end def on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_train_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \" # For backwards compatibility. self . on_batch_end ( batch , logs = logs ) on_train_begin def on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_train_begin ( self , logs = None ) : \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\" on_train_end def on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently the output of the last call to on_epoch_end() is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_train_end ( self , logs = None ) : \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently the output of the last call to `on_epoch_end()` is passed to this argument for this method but that may change in the future. \"\"\" set_model def set_model ( self , model ) View Source def set_model(self, model): self.model = model set_params def set_params ( self , params ) View Source def set_params(self, params): self.params = params ManualStepping class ManualStepping ( boundaries , rates , warmup = False , name = None ) This function provides fine grained control over learning rates. One must specify a sequence of learning rates as well as a set of integer steps at which the current learning rate must transition to the next. For example, if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning rate returned by this function is .1 for step=0,...,4, .01 for step=5...9, and .001 for step=10 and onward. You can pass this schedule directly into a tf.keras.optimizers.Optimizer as the learning rate. lr_schedule = tf . keras . optimizers . schedules . ManualStepping ( boundaries = [ 5 , 10 ], rates = [ .1 , .01 , .001 ], warmup = True ) model . compile ( optimizer = tf . keras . optimizers . SGD ( learning_rate = lr_schedule ), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( data , labels , epochs = 5 ) The learning rate schedule is also serializable and deserializable using tf.keras.optimizers.schedules.serialize and tf.keras.optimizers.schedules.deserialize . Arguments: boundaries : A List of scalar int32 or int64 or a Tensor . It is a list of global steps at which to switch learning rates. This list is assumed to consist of increasing positive integers. rates : a list of (float) learning rates corresponding to intervals between the boundaries. The length of this list must be exactly len(boundaries) + 1. warmup : Whether to linearly interpolate learning rate for steps in [0, boundaries[0]]. name : String. Optional name of the operation. Defaults to 'ExponentialDecay'. Return: A 1-arg callable learning rate schedule that takes the current optimizer step and outputs the decayed learning rate, a scalar Tensor of the same type as rates . Ancestors (in MRO) tensorflow.python.keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule Static methods from_config def from_config ( config ) Instantiates a LearningRateSchedule from its config. Parameters: Name Description config Output of get_config() . Returns: Type Description None A LearningRateSchedule instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Instantiates a `LearningRateSchedule` from its config. Args: config: Output of `get_config()`. Returns: A `LearningRateSchedule` instance. \"\" \" return cls ( ** config ) Methods get_config def get_config ( self ) View Source def get_config ( self ) : return { \" boundaries \" : self . boundaries , \" rates \" : self . rates , \" warmup \" : self . warmup , \" name \" : self . name }","title":"Learning Rate Schedule"},{"location":"reference/kerod/core/learning_rate_schedule/#module-kerodcorelearning_rate_schedule","text":"Manual Stepping is designed to integrate the computation graph and compute the learning_rate at each step. However, the WarmupLearningRateScheduler is Callback handle by the fit in keras. View Source \"\"\" Manual Stepping is designed to integrate the computation graph and compute the learning_rate at each step. However, the WarmupLearningRateScheduler is Callback handle by the fit in keras. \"\"\" from typing import List import tensorflow as tf from tensorflow.keras import backend as K from tensorflow.keras.optimizers.schedules import LearningRateSchedule from tensorflow.python.keras.callbacks import Callback class LearningRateScheduler ( Callback ): \"\"\"Warmup Learning rate scheduler. It will perform at the beginning of the training a linear warmup from `init_lr` to `base_lr`. The learning rate is decreased by 10 according to the schedule provided by `epochs`. Arguments: - *base_lr*: The target learning rate value after the linear warmup - *num_gpus*: Number of gpus used during the training - *epochs*: A list of epoch on which the learning rate should be reduce. - *use_warmup*: Perform the warmup strategy. - *init_lr*: Learning rate value from which the warmup will start. - *num_warmup_steps*: Number of training step on which the warmup will be performed. \"\"\" def __init__ ( self , base_lr : float , num_gpus : int , epochs : List [ int ], use_warmup : bool = True , init_lr : float = 1e-2 / 3 , num_warmup_steps : int = 1000 ): super () . __init__ () self . _init_lr = init_lr * min ( 8 / num_gpus , 1 ) self . slope = ( base_lr - self . _init_lr ) / num_warmup_steps self . _epochs_to_lr = { epoch : base_lr * 1 / 10 ** ( i + 1 ) for i , epoch in enumerate ( epochs )} self . _epochs = epochs self . _num_gpus = num_gpus self . _use_warmup = use_warmup self . _num_warmup_steps = num_warmup_steps def on_train_batch_begin ( self , batch , logs = None ): global_step = K . get_value (( self . model . optimizer . iterations )) if global_step <= self . _num_warmup_steps and global_step != 0 and self . _use_warmup : lr = self . _init_lr + global_step * self . slope K . set_value ( self . model . optimizer . lr , lr ) def on_epoch_begin ( self , epoch , logs = None ): if not hasattr ( self . model . optimizer , 'lr' ): raise ValueError ( 'Optimizer must have a \"lr\" attribute.' ) if not hasattr ( self . model . optimizer , 'iterations' ): raise ValueError ( 'Optimizer must have an \"iterations\" attribute.' ) global_step = K . get_value ( self . model . optimizer . iterations ) target_epochs = [ e for e in self . _epochs if epoch >= e and global_step > self . _num_warmup_steps ] if target_epochs : lr = self . _epochs_to_lr [ max ( target_epochs )] K . set_value ( self . model . optimizer . lr , lr ) class ManualStepping ( LearningRateSchedule ): \"\"\"Manually stepped learning rate schedule. (Taken and modified from Google object detection) This function provides fine grained control over learning rates. One must specify a sequence of learning rates as well as a set of integer steps at which the current learning rate must transition to the next. For example, if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning rate returned by this function is .1 for step=0,...,4, .01 for step=5...9, and .001 for step=10 and onward. You can pass this schedule directly into a `tf.keras.optimizers.Optimizer` as the learning rate. ```python lr_schedule = tf.keras.optimizers.schedules.ManualStepping( boundaries=[5, 10], rates=[.1, .01, .001], warmup=True) model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=lr_schedule), loss='sparse_categorical_crossentropy', metrics=['accuracy']) model.fit(data, labels, epochs=5) ``` The learning rate schedule is also serializable and deserializable using `tf.keras.optimizers.schedules.serialize` and `tf.keras.optimizers.schedules.deserialize`. Arguments: - *boundaries*: A List of scalar `int32` or `int64` or a `Tensor`. It is a list of global steps at which to switch learning rates. This list is assumed to consist of increasing positive integers. - *rates*: a list of (float) learning rates corresponding to intervals between the boundaries. The length of this list must be exactly len(boundaries) + 1. - *warmup*: Whether to linearly interpolate learning rate for steps in [0, boundaries[0]]. - *name*: String. Optional name of the operation. Defaults to 'ExponentialDecay'. Return: A 1-arg callable learning rate schedule that takes the current optimizer step and outputs the decayed learning rate, a scalar `Tensor` of the same type as `rates`. \"\"\" def __init__ ( self , boundaries , rates , warmup = False , name = None ): super () . __init__ () self . name = name if warmup and boundaries : slope = ( rates [ 1 ] - rates [ 0 ]) * 1.0 / boundaries [ 0 ] warmup_steps = list ( range ( boundaries [ 0 ])) warmup_rates = [ rates [ 0 ] + slope * step for step in warmup_steps ] boundaries = warmup_steps + boundaries rates = warmup_rates + rates [ 1 :] else : boundaries = [ 0 ] + boundaries self . warmup = warmup self . rates = rates self . boundaries = boundaries self . num_boundaries = len ( boundaries ) self . dtype = tf . convert_to_tensor ( rates [ 0 ]) . dtype def __call__ ( self , step ): with tf . name_scope ( self . name or \"ManualStepping\" ): boundaries = tf . convert_to_tensor ( self . boundaries , self . dtype ) rates = tf . convert_to_tensor ( self . rates , self . dtype ) step = tf . convert_to_tensor ( step , self . dtype ) rate_index = tf . reduce_max ( tf . where ( tf . greater_equal ( step , boundaries ), list ( range ( self . num_boundaries )), [ 0 ] * self . num_boundaries )) return tf . reduce_sum ( rates * tf . one_hot ( rate_index , depth = self . num_boundaries )) def get_config ( self ): return { \"boundaries\" : self . boundaries , \"rates\" : self . rates , \"warmup\" : self . warmup , \"name\" : self . name }","title":"Module kerod.core.learning_rate_schedule"},{"location":"reference/kerod/core/learning_rate_schedule/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/learning_rate_schedule/#learningratescheduler","text":"class LearningRateScheduler ( base_lr : float , num_gpus : int , epochs : List [ int ], use_warmup : bool = True , init_lr : float = 0.0033333333333333335 , num_warmup_steps : int = 1000 ) a linear warmup from init_lr to base_lr . The learning rate is decreased by 10 according to the schedule provided by epochs . Arguments: base_lr : The target learning rate value after the linear warmup num_gpus : Number of gpus used during the training epochs : A list of epoch on which the learning rate should be reduce. use_warmup : Perform the warmup strategy. init_lr : Learning rate value from which the warmup will start. num_warmup_steps : Number of training step on which the warmup will be performed.","title":"LearningRateScheduler"},{"location":"reference/kerod/core/learning_rate_schedule/#ancestors-in-mro","text":"tensorflow.python.keras.callbacks.Callback","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/learning_rate_schedule/#methods","text":"","title":"Methods"},{"location":"reference/kerod/core/learning_rate_schedule/#on_batch_begin","text":"def on_batch_begin ( self , batch , logs = None ) A backwards compatibility alias for on_train_batch_begin . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_batch_begin ( self , batch , logs = None ) : \" \"\" A backwards compatibility alias for `on_train_batch_begin`. \"\" \"","title":"on_batch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_batch_end","text":"def on_batch_end ( self , batch , logs = None ) A backwards compatibility alias for on_train_batch_end . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_batch_end ( self , batch , logs = None ) : \" \"\" A backwards compatibility alias for `on_train_batch_end`. \"\" \"","title":"on_batch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_epoch_begin","text":"def on_epoch_begin ( self , epoch , logs = None ) View Source def on_epoch_begin ( self , epoch , logs = None ) : if not hasattr ( self . model . optimizer , ' lr ' ) : raise ValueError ( ' Optimizer must have a \"lr\" attribute. ' ) if not hasattr ( self . model . optimizer , ' iterations ' ) : raise ValueError ( ' Optimizer must have an \"iterations\" attribute. ' ) global_step = K . get_value ( self . model . optimizer . iterations ) target_epochs = [ e for e in self . _epochs if epoch >= e and global_step > self . _num_warmup_steps ] if target_epochs : lr = self . _epochs_to_lr [ max ( target_epochs ) ] K . set_value ( self . model . optimizer . lr , lr )","title":"on_epoch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_epoch_end","text":"def on_epoch_end ( self , epoch , logs = None ) Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Parameters: Name Description epoch Integer, index of epoch. logs Dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with val_ . For training epoch, the values of the Model 's metrics are returned. Example : {'loss': 0.2, 'acc': 0.7} . View Source @doc_controls.for_subclass_implementers def on_epoch_end ( self , epoch , logs = None ) : \" \"\" Called at the end of an epoch. Subclasses should override for any actions to run. This function should only be called during TRAIN mode. Arguments: epoch: Integer, index of epoch. logs: Dict, metric results for this training epoch, and for the validation epoch if validation is performed. Validation result keys are prefixed with `val_`. For training epoch, the values of the `Model`'s metrics are returned. Example : `{'loss': 0.2, 'acc': 0.7}`. \"\" \"","title":"on_epoch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_predict_batch_begin","text":"def on_predict_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in predict methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict, contains the return value of model.predict_step , it typically returns a dict with a key 'outputs' containing the model's outputs. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_predict_batch_begin ( self , batch , logs = None ) : \" \"\" Called at the beginning of a batch in `predict` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict, contains the return value of `model.predict_step`, it typically returns a dict with a key 'outputs' containing the model's outputs. \"\" \"","title":"on_predict_batch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_predict_batch_end","text":"def on_predict_batch_end ( self , batch , logs = None ) Called at the end of a batch in predict methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_predict_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a batch in `predict` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \"","title":"on_predict_batch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_predict_begin","text":"def on_predict_begin ( self , logs = None ) Called at the beginning of prediction. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_predict_begin ( self , logs = None ) : \"\"\"Called at the beginning of prediction. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_predict_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_predict_end","text":"def on_predict_end ( self , logs = None ) Called at the end of prediction. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_predict_end ( self , logs = None ) : \"\"\"Called at the end of prediction. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_predict_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_test_batch_begin","text":"def on_test_batch_begin ( self , batch , logs = None ) Called at the beginning of a batch in evaluate methods. Also called at the beginning of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict, contains the return value of model.test_step . Typically, the values of the Model 's metrics are returned. Example: {'loss': 0.2, 'accuracy': 0.7} . View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_test_batch_begin ( self , batch , logs = None ) : \" \"\" Called at the beginning of a batch in `evaluate` methods. Also called at the beginning of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict, contains the return value of `model.test_step`. Typically, the values of the `Model`'s metrics are returned. Example: `{'loss': 0.2, 'accuracy': 0.7}`. \"\" \"","title":"on_test_batch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_test_batch_end","text":"def on_test_batch_end ( self , batch , logs = None ) Called at the end of a batch in evaluate methods. Also called at the end of a validation batch in the fit methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_test_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a batch in `evaluate` methods. Also called at the end of a validation batch in the `fit` methods, if validation data is provided. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \"","title":"on_test_batch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_test_begin","text":"def on_test_begin ( self , logs = None ) Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_test_begin ( self , logs = None ) : \"\"\"Called at the beginning of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_test_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_test_end","text":"def on_test_end ( self , logs = None ) Called at the end of evaluation or validation. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently the output of the last call to on_test_batch_end() is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_test_end ( self , logs = None ) : \"\"\"Called at the end of evaluation or validation. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently the output of the last call to `on_test_batch_end()` is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_test_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_train_batch_begin","text":"def on_train_batch_begin ( self , batch , logs = None ) View Source def on_train_batch_begin ( self , batch , logs = None ) : global_step = K . get_value (( self . model . optimizer . iterations )) if global_step <= self . _num_warmup_steps and global_step != 0 and self . _use_warmup : lr = self . _init_lr + global_step * self . slope K . set_value ( self . model . optimizer . lr , lr )","title":"on_train_batch_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_train_batch_end","text":"def on_train_batch_end ( self , batch , logs = None ) Called at the end of a training batch in fit methods. Subclasses should override for any actions to run. Note that if the steps_per_execution argument to compile in tf.keras.Model is set to N , this method will only be called every N batches. Parameters: Name Description batch Integer, index of batch within the current epoch. logs Dict. Aggregated metric results up until this batch. View Source @doc_controls.for_subclass_implementers @generic_utils.default def on_train_batch_end ( self , batch , logs = None ) : \" \"\" Called at the end of a training batch in `fit` methods. Subclasses should override for any actions to run. Note that if the `steps_per_execution` argument to `compile` in `tf.keras.Model` is set to `N`, this method will only be called every `N` batches. Arguments: batch: Integer, index of batch within the current epoch. logs: Dict. Aggregated metric results up until this batch. \"\" \" # For backwards compatibility. self . on_batch_end ( batch , logs = logs )","title":"on_train_batch_end"},{"location":"reference/kerod/core/learning_rate_schedule/#on_train_begin","text":"def on_train_begin ( self , logs = None ) Called at the beginning of training. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently no data is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_train_begin ( self , logs = None ) : \"\"\"Called at the beginning of training. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently no data is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_train_begin"},{"location":"reference/kerod/core/learning_rate_schedule/#on_train_end","text":"def on_train_end ( self , logs = None ) Called at the end of training. Subclasses should override for any actions to run. Parameters: Name Description logs Dict. Currently the output of the last call to on_epoch_end() is passed to this argument for this method but that may change in the future. View Source @doc_controls . for_subclass_implementers def on_train_end ( self , logs = None ) : \"\"\"Called at the end of training. Subclasses should override for any actions to run. Arguments: logs: Dict. Currently the output of the last call to `on_epoch_end()` is passed to this argument for this method but that may change in the future. \"\"\"","title":"on_train_end"},{"location":"reference/kerod/core/learning_rate_schedule/#set_model","text":"def set_model ( self , model ) View Source def set_model(self, model): self.model = model","title":"set_model"},{"location":"reference/kerod/core/learning_rate_schedule/#set_params","text":"def set_params ( self , params ) View Source def set_params(self, params): self.params = params","title":"set_params"},{"location":"reference/kerod/core/learning_rate_schedule/#manualstepping","text":"class ManualStepping ( boundaries , rates , warmup = False , name = None ) This function provides fine grained control over learning rates. One must specify a sequence of learning rates as well as a set of integer steps at which the current learning rate must transition to the next. For example, if boundaries = [5, 10] and rates = [.1, .01, .001], then the learning rate returned by this function is .1 for step=0,...,4, .01 for step=5...9, and .001 for step=10 and onward. You can pass this schedule directly into a tf.keras.optimizers.Optimizer as the learning rate. lr_schedule = tf . keras . optimizers . schedules . ManualStepping ( boundaries = [ 5 , 10 ], rates = [ .1 , .01 , .001 ], warmup = True ) model . compile ( optimizer = tf . keras . optimizers . SGD ( learning_rate = lr_schedule ), loss = 'sparse_categorical_crossentropy' , metrics = [ 'accuracy' ]) model . fit ( data , labels , epochs = 5 ) The learning rate schedule is also serializable and deserializable using tf.keras.optimizers.schedules.serialize and tf.keras.optimizers.schedules.deserialize . Arguments: boundaries : A List of scalar int32 or int64 or a Tensor . It is a list of global steps at which to switch learning rates. This list is assumed to consist of increasing positive integers. rates : a list of (float) learning rates corresponding to intervals between the boundaries. The length of this list must be exactly len(boundaries) + 1. warmup : Whether to linearly interpolate learning rate for steps in [0, boundaries[0]]. name : String. Optional name of the operation. Defaults to 'ExponentialDecay'. Return: A 1-arg callable learning rate schedule that takes the current optimizer step and outputs the decayed learning rate, a scalar Tensor of the same type as rates .","title":"ManualStepping"},{"location":"reference/kerod/core/learning_rate_schedule/#ancestors-in-mro_1","text":"tensorflow.python.keras.optimizer_v2.learning_rate_schedule.LearningRateSchedule","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/learning_rate_schedule/#static-methods","text":"","title":"Static methods"},{"location":"reference/kerod/core/learning_rate_schedule/#from_config","text":"def from_config ( config ) Instantiates a LearningRateSchedule from its config. Parameters: Name Description config Output of get_config() . Returns: Type Description None A LearningRateSchedule instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Instantiates a `LearningRateSchedule` from its config. Args: config: Output of `get_config()`. Returns: A `LearningRateSchedule` instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/kerod/core/learning_rate_schedule/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/core/learning_rate_schedule/#get_config","text":"def get_config ( self ) View Source def get_config ( self ) : return { \" boundaries \" : self . boundaries , \" rates \" : self . rates , \" warmup \" : self . warmup , \" name \" : self . name }","title":"get_config"},{"location":"reference/kerod/core/losses/","text":"Module kerod.core.losses None None View Source import tensorflow as tf from tensorflow.keras.losses import Loss class L1Loss ( Loss ): def call ( self , y_true , y_pred ): return tf . norm ( y_true - y_pred , ord = 1 , axis =- 1 ) Classes L1Loss class L1Loss ( reduction = 'auto' , name = None ) To be implemented by subclasses: * call() : Contains the logic for loss calculation using y_true , y_pred . Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): y_pred = tf . convert_to_tensor_v2 ( y_pred ) y_true = tf . cast ( y_true , y_pred . dtype ) return tf . reduce_mean ( math_ops . square ( y_pred - y_true ), axis =- 1 ) When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training tutorial for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy . scope (): loss_obj = tf . keras . losses . CategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE ) .... loss = ( tf . reduce_sum ( loss_obj ( labels , predictions )) * ( 1. / global_batch_size )) Ancestors (in MRO) tensorflow.python.keras.losses.Loss Static methods from_config def from_config ( config ) Instantiates a Loss from its config (output of get_config() ). Parameters: Name Description config Output of get_config() . Returns: Type Description None A Loss instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Instantiates a `Loss` from its config (output of `get_config()`). Args: config: Output of `get_config()`. Returns: A `Loss` instance. \"\" \" return cls ( ** config ) Methods call def call ( self , y_true , y_pred ) View Source def call ( self , y_true , y_pred ) : return tf . norm ( y_true - y_pred , ord = 1 , axis =- 1 ) get_config def get_config ( self ) Returns the config dictionary for a Loss instance. View Source def get_config ( self ) : \" \"\" Returns the config dictionary for a `Loss` instance. \"\" \" return { 'reduction' : self . reduction , 'name' : self . name }","title":"Losses"},{"location":"reference/kerod/core/losses/#module-kerodcorelosses","text":"None None View Source import tensorflow as tf from tensorflow.keras.losses import Loss class L1Loss ( Loss ): def call ( self , y_true , y_pred ): return tf . norm ( y_true - y_pred , ord = 1 , axis =- 1 )","title":"Module kerod.core.losses"},{"location":"reference/kerod/core/losses/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/losses/#l1loss","text":"class L1Loss ( reduction = 'auto' , name = None ) To be implemented by subclasses: * call() : Contains the logic for loss calculation using y_true , y_pred . Example subclass implementation: class MeanSquaredError ( Loss ): def call ( self , y_true , y_pred ): y_pred = tf . convert_to_tensor_v2 ( y_pred ) y_true = tf . cast ( y_true , y_pred . dtype ) return tf . reduce_mean ( math_ops . square ( y_pred - y_true ), axis =- 1 ) When used with tf.distribute.Strategy , outside of built-in training loops such as tf.keras compile and fit , please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training tutorial for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: with strategy . scope (): loss_obj = tf . keras . losses . CategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE ) .... loss = ( tf . reduce_sum ( loss_obj ( labels , predictions )) * ( 1. / global_batch_size ))","title":"L1Loss"},{"location":"reference/kerod/core/losses/#ancestors-in-mro","text":"tensorflow.python.keras.losses.Loss","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/losses/#static-methods","text":"","title":"Static methods"},{"location":"reference/kerod/core/losses/#from_config","text":"def from_config ( config ) Instantiates a Loss from its config (output of get_config() ). Parameters: Name Description config Output of get_config() . Returns: Type Description None A Loss instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Instantiates a `Loss` from its config (output of `get_config()`). Args: config: Output of `get_config()`. Returns: A `Loss` instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/kerod/core/losses/#methods","text":"","title":"Methods"},{"location":"reference/kerod/core/losses/#call","text":"def call ( self , y_true , y_pred ) View Source def call ( self , y_true , y_pred ) : return tf . norm ( y_true - y_pred , ord = 1 , axis =- 1 )","title":"call"},{"location":"reference/kerod/core/losses/#get_config","text":"def get_config ( self ) Returns the config dictionary for a Loss instance. View Source def get_config ( self ) : \" \"\" Returns the config dictionary for a `Loss` instance. \"\" \" return { 'reduction' : self . reduction , 'name' : self . name }","title":"get_config"},{"location":"reference/kerod/core/matcher/","text":"Module kerod.core.matcher None None View Source from typing import List import tensorflow as tf from kerod . utils import item_assignment from scipy . optimize import linear_sum_assignment class Matcher : \"\"\"This class assigns to each predicted \" element \" (e.g., a box) a ground-truth element. Each predicted element will have exactly zero or one matches; each ground-truth element may be matched to zero or more predicted elements. The matching is determined by the MxN match_quality_matrix, that characterizes how well each (ground-truth, prediction)-pair match each other. For example, if the elements are boxes, this matrix may contain box intersection-over-union overlap values. The matcher returns (a) a vector of length N containing the index of the ground-truth element m in [0, M) that matches to prediction n in [0, N). (b) a vector of length N containing the labels for each prediction. Arguments: - *thresholds*: a list of thresholds used to stratify predictions into levels. - *labels*: a list of values to label predictions belonging at each level. A label can be one of {-1, 0, 1} signifying {ignore, negative class, positive class}, respectively. - *allow_low_quality_matches*: if True, produce additional matches for predictions with maximum match quality lower than high_threshold. See set_low_quality_matches_ for more details. Example: thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions with iou < 0.3 will be marked with 0 and thus will be considered as false positives while training. All predictions with 0.3 <= iou < 0.5 will be marked with -1 and thus will be ignored. All predictions with 0.5 <= iou will be marked with 1 and thus will be considered as true positives. \"\"\" def __ init__ ( self , thresholds : List [ float ], labels : List [ int ], allow_low_quality_matches: bool = False ) : # Add - inf and + inf to first and last position in thresholds thresholds = thresholds [ : ] assert thresholds [ 0 ] > 0 thresholds . insert ( 0 , - float ( \"inf\" )) thresholds . append ( float ( \"inf\" )) assert all ( low <= high for ( low , high ) in zip ( thresholds [:- 1 ], thresholds [ 1 : ])) assert all ( l in [ - 1 , 0 , 1 ] for l in labels ) assert len ( labels ) == len ( thresholds ) - 1 self . thresholds = thresholds self . labels = labels self . allow_low_quality_matches = allow_low_quality_matches def __ call__ ( self , match_quality_matrix: tf . Tensor , num_valid_boxes: tf . Tensor ) : \"\"\" Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" assert len ( match_quality_matrix . shape ) == 3 num_valid_boxes = tf . squeeze ( num_valid_boxes , - 1 ) # match_quality_matrix is B ( batch ) x M ( gt ) x N ( predicted ) # Max over gt elements to find best gt candidate for each prediction matches = tf . argmax ( match_quality_matrix , axis = 1 , output_type = tf . int32 ) matched_vals = tf . math . reduce_max ( match_quality_matrix , axis = 1 ) # matched_vals , matches = match_quality_matrix . max ( dim = 0 ) match_labels = matches for ( l , low , high ) in zip ( self . labels , self . thresholds [:- 1 ], self . thresholds [ 1 : ]) : low_high = ( matched_vals >= low ) & ( matched_vals < high ) match_labels = item_assignment ( match_labels , low_high , l ) if self . allow_low_quality_matches: match_labels = self . _ set_low_quality_matches ( match_labels , match_quality_matrix , num_valid_boxes ) # Remove all the padded groundtruths # e . g : matches = [ 1 , 0 , 4 , 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0 , 0 , 1 , 1 ] mask_padded_boxes = matches >= num_valid_boxes [ : , None ] # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return matches , match_labels def _ set_low_quality_matches ( self , match_labels , match_quality_matrix , num_valid_boxes ) : \"\"\" Produce additional matches for predictions that have only low-quality matches. Specifically, for each ground-truth G find the set of predictions that have maximum overlap with it (including ties); for each prediction in that set, if it is unmatched, then match it to the ground-truth G. This function implements the RPN assignment case (i) in Sec. 3.1.2 of the Faster R-CNN paper: https://arxiv.org/pdf/1506.01497v3.pdf. Arguments: - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. \"\"\" # For each gt , find the prediction with which it has highest quality : shape [ batch_size , M ] highest_quality_gt = tf . math . reduce_max ( match_quality_matrix , axis = 2 ) # Find the highest quality match available , even if it is low , including ties . hq_foreach_gt_mask = match_quality_matrix == highest_quality_gt [..., None ] # Create a mask for the valid_boxes , shape = [ batch_size , M ] mask_valid_boxes = tf . sequence_mask ( num_valid_boxes , maxlen = tf . shape ( match_quality_matrix )[ 1 ], dtype = tf . bool ) hq_foreach_gt_mask = hq_foreach_gt_mask & mask_valid_boxes [..., None ] # If an anchor was labeled positive only due to a low - quality match # with gt_A , but it has larger overlap with gt_B , it's matched index will still be gt_B. # This follows the implementation in Detectron, and is found to have no significant impact. # shape = [batch_size, N] masks_for_labels = tf.reduce_max(tf.cast(hq_foreach_gt_mask, tf.int8), 1) match_labels = item_assignment(match_labels, masks_for_labels, 1) return match_labels def hungarian_matching(match_quality_matrix: tf.Tensor, num_valid_boxes: tf.Tensor): \"\"\"Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" def hungarian_assignment(cost_matrix): return tf.py_function(lambda c: linear_sum_assignment(c), [cost_matrix], Tout=(tf.int32, tf.int32)) # We do not want to match our padded ground_truth during the hungarian # assignment so we create a mask to increase the cost of fake ground_truth # [batch_size, M] mask = ~tf.sequence_mask(tf.squeeze(num_valid_boxes, 1), tf.shape(match_quality_matrix)[1]) # [batch_size, M, N] mask = tf.tile(mask[..., None], [1, 1, tf.shape(match_quality_matrix)[-1]]) # We set to inf all cost which corresponds to padding. # They won't interfere during the linear assignment match_quality_matrix = item_assignment ( match_quality_matrix , mask , 10 e5 ) # [ batch_size , num_gt_boxes ] indices = tf . vectorized_map ( hungarian_assignment , match_quality_matrix ) matches = tf . one_hot ( indices [ 1 ], tf . shape ( match_quality_matrix )[ - 1 ], dtype = tf . int32 ) match_labels = tf . cast ( tf . reduce_max ( matches , axis = 1 ), tf . int32 ) matches = tf . reduce_max ( matches * indices [ 0 ][..., None ], axis = 1 ) # Remove all the padded groundtruths # e . g : matches = [ 1 , 0 , 4 , 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0 , 0 , 1 , 1 ] mask_padded_boxes = matches >= num_valid_boxes # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return tf . stop_gradient ( matches ), tf . stop_gradient ( match_labels ) Functions hungarian_matching def hungarian_matching ( match_quality_matrix : tensorflow . python . framework . ops . Tensor , num_valid_boxes : tensorflow . python . framework . ops . Tensor ) Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: match_quality_matrix : A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. num_valid_boxes : A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and num_valid_boxes is equal to [3, 4] the boxes for the batch=0 is padded from pos=3 . It means, that quality_matrix[0, 3:] all the values from this pattern should not be considered because of the padding. Returns: matches : a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) match_labels : a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) View Source def hungarian_matching ( match_quality_matrix : tf . Tensor , num_valid_boxes : tf . Tensor ) : \"\"\"Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" def hungarian_assignment ( cost_matrix ) : return tf . py_function ( lambda c : linear_sum_assignment ( c ), [ cost_matrix ] , Tout = ( tf . int32 , tf . int32 )) # We do not want to match our padded ground_truth during the hungarian # assignment so we create a mask to increase the cost of fake ground_truth # [ batch_size, M ] mask = ~ tf . sequence_mask ( tf . squeeze ( num_valid_boxes , 1 ), tf . shape ( match_quality_matrix ) [ 1 ] ) # [ batch_size, M, N ] mask = tf . tile ( mask [ ..., None ] , [ 1, 1, tf.shape(match_quality_matrix)[-1 ] ] ) # We set to inf all cost which corresponds to padding . # They won ' t interfere during the linear assignment match_quality_matrix = item_assignment ( match_quality_matrix , mask , 10e5 ) # [ batch_size, num_gt_boxes ] indices = tf . vectorized_map ( hungarian_assignment , match_quality_matrix ) matches = tf . one_hot ( indices [ 1 ] , tf . shape ( match_quality_matrix ) [ -1 ] , dtype = tf . int32 ) match_labels = tf . cast ( tf . reduce_max ( matches , axis = 1 ), tf . int32 ) matches = tf . reduce_max ( matches * indices [ 0 ][ ..., None ] , axis = 1 ) # Remove all the padded groundtruths # e . g : matches = [ 1, 0, 4, 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0, 0, 1, 1 ] mask_padded_boxes = matches >= num_valid_boxes # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return tf . stop_gradient ( matches ), tf . stop_gradient ( match_labels ) Classes Matcher class Matcher ( thresholds : List [ float ], labels : List [ int ], allow_low_quality_matches : bool = False ) element. Each predicted element will have exactly zero or one matches; each ground-truth element may be matched to zero or more predicted elements. The matching is determined by the MxN match_quality_matrix, that characterizes how well each (ground-truth, prediction)-pair match each other. For example, if the elements are boxes, this matrix may contain box intersection-over-union overlap values. The matcher returns (a) a vector of length N containing the index of the ground-truth element m in [0, M) that matches to prediction n in [0, N). (b) a vector of length N containing the labels for each prediction. Arguments: thresholds : a list of thresholds used to stratify predictions into levels. labels : a list of values to label predictions belonging at each level. A label can be one of {-1, 0, 1} signifying {ignore, negative class, positive class}, respectively. allow_low_quality_matches : if True, produce additional matches for predictions with maximum match quality lower than high_threshold. See set_low_quality_matches_ for more details. Example: thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions with iou < 0.3 will be marked with 0 and thus will be considered as false positives while training. All predictions with 0.3 <= iou < 0.5 will be marked with -1 and thus will be ignored. All predictions with 0.5 <= iou will be marked with 1 and thus will be considered as true positives.","title":"Matcher"},{"location":"reference/kerod/core/matcher/#module-kerodcorematcher","text":"None None View Source from typing import List import tensorflow as tf from kerod . utils import item_assignment from scipy . optimize import linear_sum_assignment class Matcher : \"\"\"This class assigns to each predicted \" element \" (e.g., a box) a ground-truth element. Each predicted element will have exactly zero or one matches; each ground-truth element may be matched to zero or more predicted elements. The matching is determined by the MxN match_quality_matrix, that characterizes how well each (ground-truth, prediction)-pair match each other. For example, if the elements are boxes, this matrix may contain box intersection-over-union overlap values. The matcher returns (a) a vector of length N containing the index of the ground-truth element m in [0, M) that matches to prediction n in [0, N). (b) a vector of length N containing the labels for each prediction. Arguments: - *thresholds*: a list of thresholds used to stratify predictions into levels. - *labels*: a list of values to label predictions belonging at each level. A label can be one of {-1, 0, 1} signifying {ignore, negative class, positive class}, respectively. - *allow_low_quality_matches*: if True, produce additional matches for predictions with maximum match quality lower than high_threshold. See set_low_quality_matches_ for more details. Example: thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions with iou < 0.3 will be marked with 0 and thus will be considered as false positives while training. All predictions with 0.3 <= iou < 0.5 will be marked with -1 and thus will be ignored. All predictions with 0.5 <= iou will be marked with 1 and thus will be considered as true positives. \"\"\" def __ init__ ( self , thresholds : List [ float ], labels : List [ int ], allow_low_quality_matches: bool = False ) : # Add - inf and + inf to first and last position in thresholds thresholds = thresholds [ : ] assert thresholds [ 0 ] > 0 thresholds . insert ( 0 , - float ( \"inf\" )) thresholds . append ( float ( \"inf\" )) assert all ( low <= high for ( low , high ) in zip ( thresholds [:- 1 ], thresholds [ 1 : ])) assert all ( l in [ - 1 , 0 , 1 ] for l in labels ) assert len ( labels ) == len ( thresholds ) - 1 self . thresholds = thresholds self . labels = labels self . allow_low_quality_matches = allow_low_quality_matches def __ call__ ( self , match_quality_matrix: tf . Tensor , num_valid_boxes: tf . Tensor ) : \"\"\" Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" assert len ( match_quality_matrix . shape ) == 3 num_valid_boxes = tf . squeeze ( num_valid_boxes , - 1 ) # match_quality_matrix is B ( batch ) x M ( gt ) x N ( predicted ) # Max over gt elements to find best gt candidate for each prediction matches = tf . argmax ( match_quality_matrix , axis = 1 , output_type = tf . int32 ) matched_vals = tf . math . reduce_max ( match_quality_matrix , axis = 1 ) # matched_vals , matches = match_quality_matrix . max ( dim = 0 ) match_labels = matches for ( l , low , high ) in zip ( self . labels , self . thresholds [:- 1 ], self . thresholds [ 1 : ]) : low_high = ( matched_vals >= low ) & ( matched_vals < high ) match_labels = item_assignment ( match_labels , low_high , l ) if self . allow_low_quality_matches: match_labels = self . _ set_low_quality_matches ( match_labels , match_quality_matrix , num_valid_boxes ) # Remove all the padded groundtruths # e . g : matches = [ 1 , 0 , 4 , 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0 , 0 , 1 , 1 ] mask_padded_boxes = matches >= num_valid_boxes [ : , None ] # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return matches , match_labels def _ set_low_quality_matches ( self , match_labels , match_quality_matrix , num_valid_boxes ) : \"\"\" Produce additional matches for predictions that have only low-quality matches. Specifically, for each ground-truth G find the set of predictions that have maximum overlap with it (including ties); for each prediction in that set, if it is unmatched, then match it to the ground-truth G. This function implements the RPN assignment case (i) in Sec. 3.1.2 of the Faster R-CNN paper: https://arxiv.org/pdf/1506.01497v3.pdf. Arguments: - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. \"\"\" # For each gt , find the prediction with which it has highest quality : shape [ batch_size , M ] highest_quality_gt = tf . math . reduce_max ( match_quality_matrix , axis = 2 ) # Find the highest quality match available , even if it is low , including ties . hq_foreach_gt_mask = match_quality_matrix == highest_quality_gt [..., None ] # Create a mask for the valid_boxes , shape = [ batch_size , M ] mask_valid_boxes = tf . sequence_mask ( num_valid_boxes , maxlen = tf . shape ( match_quality_matrix )[ 1 ], dtype = tf . bool ) hq_foreach_gt_mask = hq_foreach_gt_mask & mask_valid_boxes [..., None ] # If an anchor was labeled positive only due to a low - quality match # with gt_A , but it has larger overlap with gt_B , it's matched index will still be gt_B. # This follows the implementation in Detectron, and is found to have no significant impact. # shape = [batch_size, N] masks_for_labels = tf.reduce_max(tf.cast(hq_foreach_gt_mask, tf.int8), 1) match_labels = item_assignment(match_labels, masks_for_labels, 1) return match_labels def hungarian_matching(match_quality_matrix: tf.Tensor, num_valid_boxes: tf.Tensor): \"\"\"Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" def hungarian_assignment(cost_matrix): return tf.py_function(lambda c: linear_sum_assignment(c), [cost_matrix], Tout=(tf.int32, tf.int32)) # We do not want to match our padded ground_truth during the hungarian # assignment so we create a mask to increase the cost of fake ground_truth # [batch_size, M] mask = ~tf.sequence_mask(tf.squeeze(num_valid_boxes, 1), tf.shape(match_quality_matrix)[1]) # [batch_size, M, N] mask = tf.tile(mask[..., None], [1, 1, tf.shape(match_quality_matrix)[-1]]) # We set to inf all cost which corresponds to padding. # They won't interfere during the linear assignment match_quality_matrix = item_assignment ( match_quality_matrix , mask , 10 e5 ) # [ batch_size , num_gt_boxes ] indices = tf . vectorized_map ( hungarian_assignment , match_quality_matrix ) matches = tf . one_hot ( indices [ 1 ], tf . shape ( match_quality_matrix )[ - 1 ], dtype = tf . int32 ) match_labels = tf . cast ( tf . reduce_max ( matches , axis = 1 ), tf . int32 ) matches = tf . reduce_max ( matches * indices [ 0 ][..., None ], axis = 1 ) # Remove all the padded groundtruths # e . g : matches = [ 1 , 0 , 4 , 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0 , 0 , 1 , 1 ] mask_padded_boxes = matches >= num_valid_boxes # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return tf . stop_gradient ( matches ), tf . stop_gradient ( match_labels )","title":"Module kerod.core.matcher"},{"location":"reference/kerod/core/matcher/#functions","text":"","title":"Functions"},{"location":"reference/kerod/core/matcher/#hungarian_matching","text":"def hungarian_matching ( match_quality_matrix : tensorflow . python . framework . ops . Tensor , num_valid_boxes : tensorflow . python . framework . ops . Tensor ) Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: match_quality_matrix : A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. num_valid_boxes : A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and num_valid_boxes is equal to [3, 4] the boxes for the batch=0 is padded from pos=3 . It means, that quality_matrix[0, 3:] all the values from this pattern should not be considered because of the padding. Returns: matches : a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) match_labels : a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) View Source def hungarian_matching ( match_quality_matrix : tf . Tensor , num_valid_boxes : tf . Tensor ) : \"\"\"Find the maximum-weight matching of the match_quality_matrix. A maximum-weight matching is also a perfect matching. Arguments: - *match_quality_matrix*: A tensor or shape [batch_size, M, N], containing the pairwise quality between M ground-truth elements and N predicted elements. - *num_valid_boxes*: A tensor of shape [batch_size, 1] indicating where is the padding on the ground_truth_boxes. E.g: If your quality_matrix is of shape [2, 4, 6] and `num_valid_boxes` is equal to [3, 4] the boxes for the `batch=0` is padded from `pos=3`. It means, that `quality_matrix[0, 3:]` all the values from this pattern should not be considered because of the padding. Returns: - *matches*: a tensor of int32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) \"\"\" def hungarian_assignment ( cost_matrix ) : return tf . py_function ( lambda c : linear_sum_assignment ( c ), [ cost_matrix ] , Tout = ( tf . int32 , tf . int32 )) # We do not want to match our padded ground_truth during the hungarian # assignment so we create a mask to increase the cost of fake ground_truth # [ batch_size, M ] mask = ~ tf . sequence_mask ( tf . squeeze ( num_valid_boxes , 1 ), tf . shape ( match_quality_matrix ) [ 1 ] ) # [ batch_size, M, N ] mask = tf . tile ( mask [ ..., None ] , [ 1, 1, tf.shape(match_quality_matrix)[-1 ] ] ) # We set to inf all cost which corresponds to padding . # They won ' t interfere during the linear assignment match_quality_matrix = item_assignment ( match_quality_matrix , mask , 10e5 ) # [ batch_size, num_gt_boxes ] indices = tf . vectorized_map ( hungarian_assignment , match_quality_matrix ) matches = tf . one_hot ( indices [ 1 ] , tf . shape ( match_quality_matrix ) [ -1 ] , dtype = tf . int32 ) match_labels = tf . cast ( tf . reduce_max ( matches , axis = 1 ), tf . int32 ) matches = tf . reduce_max ( matches * indices [ 0 ][ ..., None ] , axis = 1 ) # Remove all the padded groundtruths # e . g : matches = [ 1, 0, 4, 3 ] num_valid_boxes = [ 3 ] # mask_padded_boxes = [ 0, 0, 1, 1 ] mask_padded_boxes = matches >= num_valid_boxes # Will flag to - 1 all the padded boxes to avoid sampling them match_labels = item_assignment ( match_labels , mask_padded_boxes , - 1 ) return tf . stop_gradient ( matches ), tf . stop_gradient ( match_labels )","title":"hungarian_matching"},{"location":"reference/kerod/core/matcher/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/matcher/#matcher","text":"class Matcher ( thresholds : List [ float ], labels : List [ int ], allow_low_quality_matches : bool = False ) element. Each predicted element will have exactly zero or one matches; each ground-truth element may be matched to zero or more predicted elements. The matching is determined by the MxN match_quality_matrix, that characterizes how well each (ground-truth, prediction)-pair match each other. For example, if the elements are boxes, this matrix may contain box intersection-over-union overlap values. The matcher returns (a) a vector of length N containing the index of the ground-truth element m in [0, M) that matches to prediction n in [0, N). (b) a vector of length N containing the labels for each prediction. Arguments: thresholds : a list of thresholds used to stratify predictions into levels. labels : a list of values to label predictions belonging at each level. A label can be one of {-1, 0, 1} signifying {ignore, negative class, positive class}, respectively. allow_low_quality_matches : if True, produce additional matches for predictions with maximum match quality lower than high_threshold. See set_low_quality_matches_ for more details. Example: thresholds = [0.3, 0.5] labels = [0, -1, 1] All predictions with iou < 0.3 will be marked with 0 and thus will be considered as false positives while training. All predictions with 0.3 <= iou < 0.5 will be marked with -1 and thus will be ignored. All predictions with 0.5 <= iou will be marked with 1 and thus will be considered as true positives.","title":"Matcher"},{"location":"reference/kerod/core/sampling_ops/","text":"Module kerod.core.sampling_ops Method to subsample minibatches by balancing positives and negatives. Subsamples minibatches based on a pre-specified positive fraction in range [0,1]. The class presumes there are many more negatives than positive examples: if the desired sample_size cannot be achieved with the pre-specified positive fraction, it fills the rest with negative examples. If this is not sufficient for obtaining the desired sample_size, it returns fewer examples. The main function to call is Subsample(self, indicator, labels). For convenience one can also call SubsampleWeights(self, weights, labels) which is defined in the minibatch_sampler base class. When is_static is True, it implements a method that guarantees static shapes. It also ensures the length of output of the subsample is always sample_size, even when number of examples set to True in indicator is less than sample_size. View Source # Copyright 2017 The TensorFlow Authors and modified by Emilien Garreau. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \" \"\" Method to subsample minibatches by balancing positives and negatives. Subsamples minibatches based on a pre-specified positive fraction in range [0,1]. The class presumes there are many more negatives than positive examples: if the desired sample_size cannot be achieved with the pre-specified positive fraction, it fills the rest with negative examples. If this is not sufficient for obtaining the desired sample_size, it returns fewer examples. The main function to call is Subsample(self, indicator, labels). For convenience one can also call SubsampleWeights(self, weights, labels) which is defined in the minibatch_sampler base class. When is_static is True, it implements a method that guarantees static shapes. It also ensures the length of output of the subsample is always sample_size, even when number of examples set to True in indicator is less than sample_size. \"\" \" import tensorflow as tf from kerod . utils import ops def subsample_indicator ( indicator , num_samples ) : \" \"\" Subsample indicator vector. Given a boolean indicator vector with M elements set to `True`, the function assigns all but `num_samples` of these previously `True` elements to `False`. If `num_samples` is greater than M, the original indicator vector is returned. Arguments: - *indicator*: a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. - *num_samples*: int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor \"\" \" indices = tf . where ( indicator ) indices = tf . random . shuffle ( indices ) indices = tf . reshape ( indices , [ - 1 ] ) num_samples = tf . minimum ( tf . size ( indices ), num_samples ) selected_indices = tf . slice ( indices , [ 0 ] , tf . reshape ( num_samples , [ 1 ] )) selected_indicator = ops . indices_to_dense_vector ( selected_indices , tf . shape ( indicator ) [ 0 ] ) return tf . equal ( selected_indicator , 1 ) def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) : \" \"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: *sampled_idx_indicator*: boolean tensor of shape [N], True for entries which are sampled. \"\" \" negative_idx = tf . logical_not ( labels ) positive_idx = tf . logical_and ( labels , indicator ) negative_idx = tf . logical_and ( negative_idx , indicator ) # Sample positive and negative samples separately if sample_size is None : max_num_pos = tf . reduce_sum ( tf . cast ( positive_idx , dtype = tf . int32 )) else : max_num_pos = int ( positive_fraction * sample_size ) sampled_pos_idx = subsample_indicator ( positive_idx , max_num_pos ) num_sampled_pos = tf . reduce_sum ( tf . cast ( sampled_pos_idx , tf . int32 )) if sample_size is None : negative_positive_ratio = ( 1 - positive_fraction ) / positive_fraction max_num_neg = tf . cast ( negative_positive_ratio * tf . cast ( num_sampled_pos , dtype = tf . float32 ), dtype = tf . int32 ) else : max_num_neg = sample_size - num_sampled_pos sampled_neg_idx = subsample_indicator ( negative_idx , max_num_neg ) return tf . logical_or ( sampled_pos_idx , sampled_neg_idx ) def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0.5 , dtype = tf . float32 ) : \" \"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [batch_size, N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [batch_size, N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: A boolean tensor of shape [M, N], True for entries which are sampled. \"\" \" def _minibatch_subsample_fn ( inputs ) : indicators , targets = inputs return sample_balanced_positive_negative ( tf . cast ( indicators , tf . bool ), sample_size , tf . cast ( targets , tf . bool ), positive_fraction = positive_fraction ) return tf . cast ( tf . map_fn ( _minibatch_subsample_fn , [ indicators , labels ] , dtype = tf . bool , parallel_iterations = 16 , back_prop = True ), dtype = dtype ) Functions batch_sample_balanced_positive_negative def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0.5 , dtype = tf . float32 ) Subsamples minibatches to a desired balance of positives and negatives. Arguments: indicator : boolean tensor of shape [batch_size, N] whose True entries can be sampled. sample_size : desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. labels : boolean tensor of shape [batch_size, N] denoting positive(=True) and negative (=False) examples. positive_fraction : desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: A boolean tensor of shape [M, N], True for entries which are sampled. View Source def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0 . 5 , dtype = tf . float32 ) : \"\"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments : - * indicator * : boolean tensor of shape [ batch_size , N ] whose True entries can be sampled . - * sample_size * : desired batch size . If None , keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction . - * labels * : boolean tensor of shape [ batch_size , N ] denoting positive ( = True ) and negative ( = False ) examples . - * positive_fraction * : desired fraction of positive examples ( scalar in [ 0 , 1 ] ) in the batch . Returns : A boolean tensor of shape [ M , N ], True for entries which are sampled . \"\"\" def _minibatch_subsample_fn ( inputs ) : indicators , targets = inputs return sample_balanced_positive_negative ( tf . cast ( indicators , tf . bool ) , sample_size , tf . cast ( targets , tf . bool ) , positive_fraction = positive_fraction ) return tf . cast ( tf . map_fn ( _minibatch_subsample_fn , [ indicators , labels ], dtype = tf . bool , parallel_iterations = 16 , back_prop = True ) , dtype = dtype ) sample_balanced_positive_negative def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) Subsamples minibatches to a desired balance of positives and negatives. Arguments: indicator : boolean tensor of shape [N] whose True entries can be sampled. sample_size : desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. labels : boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. positive_fraction : desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: sampled_idx_indicator : boolean tensor of shape [N], True for entries which are sampled. View Source def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) : \"\"\"Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: *sampled_idx_indicator*: boolean tensor of shape [N], True for entries which are sampled. \"\"\" negative_idx = tf . logical_not ( labels ) positive_idx = tf . logical_and ( labels , indicator ) negative_idx = tf . logical_and ( negative_idx , indicator ) # Sample positive and negative samples separately if sample_size is None : max_num_pos = tf . reduce_sum ( tf . cast ( positive_idx , dtype = tf . int32 )) else : max_num_pos = int ( positive_fraction * sample_size ) sampled_pos_idx = subsample_indicator ( positive_idx , max_num_pos ) num_sampled_pos = tf . reduce_sum ( tf . cast ( sampled_pos_idx , tf . int32 )) if sample_size is None : negative_positive_ratio = ( 1 - positive_fraction ) / positive_fraction max_num_neg = tf . cast ( negative_positive_ratio * tf . cast ( num_sampled_pos , dtype = tf . float32 ), dtype = tf . int32 ) else : max_num_neg = sample_size - num_sampled_pos sampled_neg_idx = subsample_indicator ( negative_idx , max_num_neg ) return tf . logical_or ( sampled_pos_idx , sampled_neg_idx ) subsample_indicator def subsample_indicator ( indicator , num_samples ) Subsample indicator vector. Given a boolean indicator vector with M elements set to True , the function assigns all but num_samples of these previously True elements to False . If num_samples is greater than M, the original indicator vector is returned. Arguments: - indicator : a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. num_samples : int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor View Source def subsample_indicator ( indicator , num_samples ) : \" \"\" Subsample indicator vector. Given a boolean indicator vector with M elements set to `True`, the function assigns all but `num_samples` of these previously `True` elements to `False`. If `num_samples` is greater than M, the original indicator vector is returned. Arguments: - *indicator*: a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. - *num_samples*: int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor \"\" \" indices = tf . where ( indicator ) indices = tf . random . shuffle ( indices ) indices = tf . reshape ( indices , [ - 1 ] ) num_samples = tf . minimum ( tf . size ( indices ), num_samples ) selected_indices = tf . slice ( indices , [ 0 ] , tf . reshape ( num_samples , [ 1 ] )) selected_indicator = ops . indices_to_dense_vector ( selected_indices , tf . shape ( indicator ) [ 0 ] ) return tf . equal ( selected_indicator , 1 )","title":"Sampling Ops"},{"location":"reference/kerod/core/sampling_ops/#module-kerodcoresampling_ops","text":"Method to subsample minibatches by balancing positives and negatives. Subsamples minibatches based on a pre-specified positive fraction in range [0,1]. The class presumes there are many more negatives than positive examples: if the desired sample_size cannot be achieved with the pre-specified positive fraction, it fills the rest with negative examples. If this is not sufficient for obtaining the desired sample_size, it returns fewer examples. The main function to call is Subsample(self, indicator, labels). For convenience one can also call SubsampleWeights(self, weights, labels) which is defined in the minibatch_sampler base class. When is_static is True, it implements a method that guarantees static shapes. It also ensures the length of output of the subsample is always sample_size, even when number of examples set to True in indicator is less than sample_size. View Source # Copyright 2017 The TensorFlow Authors and modified by Emilien Garreau. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \" \"\" Method to subsample minibatches by balancing positives and negatives. Subsamples minibatches based on a pre-specified positive fraction in range [0,1]. The class presumes there are many more negatives than positive examples: if the desired sample_size cannot be achieved with the pre-specified positive fraction, it fills the rest with negative examples. If this is not sufficient for obtaining the desired sample_size, it returns fewer examples. The main function to call is Subsample(self, indicator, labels). For convenience one can also call SubsampleWeights(self, weights, labels) which is defined in the minibatch_sampler base class. When is_static is True, it implements a method that guarantees static shapes. It also ensures the length of output of the subsample is always sample_size, even when number of examples set to True in indicator is less than sample_size. \"\" \" import tensorflow as tf from kerod . utils import ops def subsample_indicator ( indicator , num_samples ) : \" \"\" Subsample indicator vector. Given a boolean indicator vector with M elements set to `True`, the function assigns all but `num_samples` of these previously `True` elements to `False`. If `num_samples` is greater than M, the original indicator vector is returned. Arguments: - *indicator*: a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. - *num_samples*: int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor \"\" \" indices = tf . where ( indicator ) indices = tf . random . shuffle ( indices ) indices = tf . reshape ( indices , [ - 1 ] ) num_samples = tf . minimum ( tf . size ( indices ), num_samples ) selected_indices = tf . slice ( indices , [ 0 ] , tf . reshape ( num_samples , [ 1 ] )) selected_indicator = ops . indices_to_dense_vector ( selected_indices , tf . shape ( indicator ) [ 0 ] ) return tf . equal ( selected_indicator , 1 ) def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) : \" \"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: *sampled_idx_indicator*: boolean tensor of shape [N], True for entries which are sampled. \"\" \" negative_idx = tf . logical_not ( labels ) positive_idx = tf . logical_and ( labels , indicator ) negative_idx = tf . logical_and ( negative_idx , indicator ) # Sample positive and negative samples separately if sample_size is None : max_num_pos = tf . reduce_sum ( tf . cast ( positive_idx , dtype = tf . int32 )) else : max_num_pos = int ( positive_fraction * sample_size ) sampled_pos_idx = subsample_indicator ( positive_idx , max_num_pos ) num_sampled_pos = tf . reduce_sum ( tf . cast ( sampled_pos_idx , tf . int32 )) if sample_size is None : negative_positive_ratio = ( 1 - positive_fraction ) / positive_fraction max_num_neg = tf . cast ( negative_positive_ratio * tf . cast ( num_sampled_pos , dtype = tf . float32 ), dtype = tf . int32 ) else : max_num_neg = sample_size - num_sampled_pos sampled_neg_idx = subsample_indicator ( negative_idx , max_num_neg ) return tf . logical_or ( sampled_pos_idx , sampled_neg_idx ) def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0.5 , dtype = tf . float32 ) : \" \"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [batch_size, N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [batch_size, N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: A boolean tensor of shape [M, N], True for entries which are sampled. \"\" \" def _minibatch_subsample_fn ( inputs ) : indicators , targets = inputs return sample_balanced_positive_negative ( tf . cast ( indicators , tf . bool ), sample_size , tf . cast ( targets , tf . bool ), positive_fraction = positive_fraction ) return tf . cast ( tf . map_fn ( _minibatch_subsample_fn , [ indicators , labels ] , dtype = tf . bool , parallel_iterations = 16 , back_prop = True ), dtype = dtype )","title":"Module kerod.core.sampling_ops"},{"location":"reference/kerod/core/sampling_ops/#functions","text":"","title":"Functions"},{"location":"reference/kerod/core/sampling_ops/#batch_sample_balanced_positive_negative","text":"def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0.5 , dtype = tf . float32 ) Subsamples minibatches to a desired balance of positives and negatives. Arguments: indicator : boolean tensor of shape [batch_size, N] whose True entries can be sampled. sample_size : desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. labels : boolean tensor of shape [batch_size, N] denoting positive(=True) and negative (=False) examples. positive_fraction : desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: A boolean tensor of shape [M, N], True for entries which are sampled. View Source def batch_sample_balanced_positive_negative ( indicators , sample_size , labels , positive_fraction = 0 . 5 , dtype = tf . float32 ) : \"\"\" Subsamples minibatches to a desired balance of positives and negatives. Arguments : - * indicator * : boolean tensor of shape [ batch_size , N ] whose True entries can be sampled . - * sample_size * : desired batch size . If None , keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction . - * labels * : boolean tensor of shape [ batch_size , N ] denoting positive ( = True ) and negative ( = False ) examples . - * positive_fraction * : desired fraction of positive examples ( scalar in [ 0 , 1 ] ) in the batch . Returns : A boolean tensor of shape [ M , N ], True for entries which are sampled . \"\"\" def _minibatch_subsample_fn ( inputs ) : indicators , targets = inputs return sample_balanced_positive_negative ( tf . cast ( indicators , tf . bool ) , sample_size , tf . cast ( targets , tf . bool ) , positive_fraction = positive_fraction ) return tf . cast ( tf . map_fn ( _minibatch_subsample_fn , [ indicators , labels ], dtype = tf . bool , parallel_iterations = 16 , back_prop = True ) , dtype = dtype )","title":"batch_sample_balanced_positive_negative"},{"location":"reference/kerod/core/sampling_ops/#sample_balanced_positive_negative","text":"def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) Subsamples minibatches to a desired balance of positives and negatives. Arguments: indicator : boolean tensor of shape [N] whose True entries can be sampled. sample_size : desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. labels : boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. positive_fraction : desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: sampled_idx_indicator : boolean tensor of shape [N], True for entries which are sampled. View Source def sample_balanced_positive_negative ( indicator , sample_size , labels , positive_fraction = 0.5 ) : \"\"\"Subsamples minibatches to a desired balance of positives and negatives. Arguments: - *indicator*: boolean tensor of shape [N] whose True entries can be sampled. - *sample_size*: desired batch size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. - *labels*: boolean tensor of shape [N] denoting positive(=True) and negative (=False) examples. - *positive_fraction*: desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: *sampled_idx_indicator*: boolean tensor of shape [N], True for entries which are sampled. \"\"\" negative_idx = tf . logical_not ( labels ) positive_idx = tf . logical_and ( labels , indicator ) negative_idx = tf . logical_and ( negative_idx , indicator ) # Sample positive and negative samples separately if sample_size is None : max_num_pos = tf . reduce_sum ( tf . cast ( positive_idx , dtype = tf . int32 )) else : max_num_pos = int ( positive_fraction * sample_size ) sampled_pos_idx = subsample_indicator ( positive_idx , max_num_pos ) num_sampled_pos = tf . reduce_sum ( tf . cast ( sampled_pos_idx , tf . int32 )) if sample_size is None : negative_positive_ratio = ( 1 - positive_fraction ) / positive_fraction max_num_neg = tf . cast ( negative_positive_ratio * tf . cast ( num_sampled_pos , dtype = tf . float32 ), dtype = tf . int32 ) else : max_num_neg = sample_size - num_sampled_pos sampled_neg_idx = subsample_indicator ( negative_idx , max_num_neg ) return tf . logical_or ( sampled_pos_idx , sampled_neg_idx )","title":"sample_balanced_positive_negative"},{"location":"reference/kerod/core/sampling_ops/#subsample_indicator","text":"def subsample_indicator ( indicator , num_samples ) Subsample indicator vector. Given a boolean indicator vector with M elements set to True , the function assigns all but num_samples of these previously True elements to False . If num_samples is greater than M, the original indicator vector is returned. Arguments: - indicator : a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. num_samples : int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor View Source def subsample_indicator ( indicator , num_samples ) : \" \"\" Subsample indicator vector. Given a boolean indicator vector with M elements set to `True`, the function assigns all but `num_samples` of these previously `True` elements to `False`. If `num_samples` is greater than M, the original indicator vector is returned. Arguments: - *indicator*: a 1-dimensional boolean tensor indicating which elements are allowed to be sampled and which are not. - *num_samples*: int32 scalar tensor Returns: A boolean tensor with the same shape as input (indicator) tensor \"\" \" indices = tf . where ( indicator ) indices = tf . random . shuffle ( indices ) indices = tf . reshape ( indices , [ - 1 ] ) num_samples = tf . minimum ( tf . size ( indices ), num_samples ) selected_indices = tf . slice ( indices , [ 0 ] , tf . reshape ( num_samples , [ 1 ] )) selected_indicator = ops . indices_to_dense_vector ( selected_indices , tf . shape ( indicator ) [ 0 ] ) return tf . equal ( selected_indicator , 1 )","title":"subsample_indicator"},{"location":"reference/kerod/core/similarity/","text":"Module kerod.core.similarity None None View Source from abc import abstractmethod from typing import Dict import tensorflow as tf from kerod.core.box_ops import compute_giou , compute_iou , convert_to_xyxy_coordinates from kerod.core.standard_fields import BoxField from kerod.utils import get_full_indices class Similarity : def __call__ ( self , inputs1 : Dict [ str , tf . Tensor ], inputs2 : Dict [ str , tf . Tensor ]): return self . call ( inputs1 , inputs2 ) @abstractmethod def call ( self , inputs1 , inputs2 ) -> tf . Tensor : pass class IoUSimilarity ( Similarity ): def call ( self , y_true : Dict [ str , tf . Tensor ], anchors : Dict [ str , tf . Tensor ]): \"\"\"Computes pairwise intersection-over-union between boxes. Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. \"\"\" return compute_iou ( y_true [ BoxField . BOXES ], anchors [ BoxField . BOXES ]) class DetrSimilarity ( Similarity ): def __init__ ( self , weight_class = 1 , weight_l1 = 5 , weight_giou = 2 ): \"\"\"Instantiate a callable object which will compute the similarity according to the default parameters: [End to end object detection with transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Arguments: - *weight_class*: Weight which ponderates the cost of the class similarity. - *weight_l1*: Weight which ponderates the cost of the l1 similarity between boxes. - *weight_giou*: Weight which ponderates the cost of the giou similarity between boxes. \"\"\" self . weight_class = weight_class self . weight_l1 = weight_l1 self . weight_giou = weight_giou def call ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ]) -> tf . Tensor : \"\"\" Compute the cost matrix according to the paper [End to end object detection with transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. \"\"\" classification_logits = y_pred [ BoxField . SCORES ] localization_pred = y_pred [ BoxField . BOXES ] out_prob = tf . nn . softmax ( classification_logits , - 1 ) # Extract the target classes to approximate the classification cost # [batch_size, nb_class, num_detection] out_prob = tf . transpose ( out_prob , [ 0 , 2 , 1 ]) # [batch_size, nb_target, num_detection] cost = tf . gather_nd ( out_prob , get_full_indices ( y_true [ BoxField . LABELS ])) # Compute the classification cost. Contrary to the loss, we don't use the NLL, # but approximate it in 1 - proba[target class]. # The 1 is a constant that doesn't change the matching, it can be ommitted. # [batch_size, num_detection, nb_target] cost_class = - cost gt_boxes = y_true [ BoxField . BOXES ] # Compute the L1 cost between boxes # [batch_size, nb_target, num_detection] cost_bbox = tf . norm ( localization_pred [:, None ] - gt_boxes [:, :, None ], ord = 1 , axis =- 1 , ) # Compute the giou cost betwen boxes # [batch_size, nb_target, num_detection] # loss_giou= 1 - giou but we approximate it with -giou cost_giou = - compute_giou ( convert_to_xyxy_coordinates ( gt_boxes ), convert_to_xyxy_coordinates ( localization_pred )) # Final cost matrix cost_matrix = self . weight_l1 * cost_bbox + self . weight_class * cost_class + self . weight_giou * cost_giou return cost_matrix Classes DetrSimilarity class DetrSimilarity ( weight_class = 1 , weight_l1 = 5 , weight_giou = 2 ) Ancestors (in MRO) kerod.core.similarity.Similarity Methods call def call ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> tensorflow . python . framework . ops . Tensor Compute the cost matrix according to the paper End to end object detection with transformers . Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. View Source def call ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ]) -> tf . Tensor : \"\"\" Compute the cost matrix according to the paper [ End to end object detection with transformers ]( https : //ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Return : A 3 - D tensor of float32 with shape [ batch_size , N , M ] representing pairwise similarity scores defined in DeTr . \"\"\" classification_logits = y_pred [ BoxField . SCORES ] localization_pred = y_pred [ BoxField . BOXES ] out_prob = tf . nn . softmax ( classification_logits , -1 ) # Extract the target classes to approximate the classification cost # [batch_size, nb_class, num_detection] out_prob = tf . transpose ( out_prob , [ 0 , 2 , 1 ]) # [batch_size, nb_target, num_detection] cost = tf . gather_nd ( out_prob , get_full_indices ( y_true [ BoxField . LABELS ])) # Compute the classification cost. Contrary to the loss, we don't use the NLL, # but approximate it in 1 - proba[target class]. # The 1 is a constant that doesn't change the matching, it can be ommitted. # [batch_size, num_detection, nb_target] cost_class = - cost gt_boxes = y_true [ BoxField . BOXES ] # Compute the L1 cost between boxes # [batch_size, nb_target, num_detection] cost_bbox = tf . norm ( localization_pred [ : , None ] - gt_boxes [ : , : , None ], ord = 1 , axis = -1 , ) # Compute the giou cost betwen boxes # [batch_size, nb_target, num_detection] # loss_giou= 1 - giou but we approximate it with -giou cost_giou = - compute_giou ( convert_to_xyxy_coordinates ( gt_boxes ), convert_to_xyxy_coordinates ( localization_pred )) # Final cost matrix cost_matrix = self . weight_l1 * cost_bbox + self . weight_class * cost_class + self . weight_giou * cost_giou return cost_matrix IoUSimilarity class IoUSimilarity ( / , * args , ** kwargs ) Ancestors (in MRO) kerod.core.similarity.Similarity Methods call def call ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], anchors : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) Computes pairwise intersection-over-union between boxes. Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. View Source def call ( self , y_true : Dict [ str , tf . Tensor ], anchors : Dict [ str , tf . Tensor ] ) : \"\"\" Computes pairwise intersection-over-union between boxes. Return : A 3 - D tensor of float32 with shape [ batch_size , N , M ] representing pairwise similarity scores defined in DeTr . \"\"\" return compute_iou ( y_true [ BoxField . BOXES ], anchors [ BoxField . BOXES ] ) Similarity class Similarity ( / , * args , ** kwargs ) Descendants kerod.core.similarity.IoUSimilarity kerod.core.similarity.DetrSimilarity Methods call def call ( self , inputs1 , inputs2 ) -> tensorflow . python . framework . ops . Tensor View Source @abstractmethod def call ( self , inputs1 , inputs2 ) -> tf . Tensor : pass","title":"Similarity"},{"location":"reference/kerod/core/similarity/#module-kerodcoresimilarity","text":"None None View Source from abc import abstractmethod from typing import Dict import tensorflow as tf from kerod.core.box_ops import compute_giou , compute_iou , convert_to_xyxy_coordinates from kerod.core.standard_fields import BoxField from kerod.utils import get_full_indices class Similarity : def __call__ ( self , inputs1 : Dict [ str , tf . Tensor ], inputs2 : Dict [ str , tf . Tensor ]): return self . call ( inputs1 , inputs2 ) @abstractmethod def call ( self , inputs1 , inputs2 ) -> tf . Tensor : pass class IoUSimilarity ( Similarity ): def call ( self , y_true : Dict [ str , tf . Tensor ], anchors : Dict [ str , tf . Tensor ]): \"\"\"Computes pairwise intersection-over-union between boxes. Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. \"\"\" return compute_iou ( y_true [ BoxField . BOXES ], anchors [ BoxField . BOXES ]) class DetrSimilarity ( Similarity ): def __init__ ( self , weight_class = 1 , weight_l1 = 5 , weight_giou = 2 ): \"\"\"Instantiate a callable object which will compute the similarity according to the default parameters: [End to end object detection with transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Arguments: - *weight_class*: Weight which ponderates the cost of the class similarity. - *weight_l1*: Weight which ponderates the cost of the l1 similarity between boxes. - *weight_giou*: Weight which ponderates the cost of the giou similarity between boxes. \"\"\" self . weight_class = weight_class self . weight_l1 = weight_l1 self . weight_giou = weight_giou def call ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ]) -> tf . Tensor : \"\"\" Compute the cost matrix according to the paper [End to end object detection with transformers](https://ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. \"\"\" classification_logits = y_pred [ BoxField . SCORES ] localization_pred = y_pred [ BoxField . BOXES ] out_prob = tf . nn . softmax ( classification_logits , - 1 ) # Extract the target classes to approximate the classification cost # [batch_size, nb_class, num_detection] out_prob = tf . transpose ( out_prob , [ 0 , 2 , 1 ]) # [batch_size, nb_target, num_detection] cost = tf . gather_nd ( out_prob , get_full_indices ( y_true [ BoxField . LABELS ])) # Compute the classification cost. Contrary to the loss, we don't use the NLL, # but approximate it in 1 - proba[target class]. # The 1 is a constant that doesn't change the matching, it can be ommitted. # [batch_size, num_detection, nb_target] cost_class = - cost gt_boxes = y_true [ BoxField . BOXES ] # Compute the L1 cost between boxes # [batch_size, nb_target, num_detection] cost_bbox = tf . norm ( localization_pred [:, None ] - gt_boxes [:, :, None ], ord = 1 , axis =- 1 , ) # Compute the giou cost betwen boxes # [batch_size, nb_target, num_detection] # loss_giou= 1 - giou but we approximate it with -giou cost_giou = - compute_giou ( convert_to_xyxy_coordinates ( gt_boxes ), convert_to_xyxy_coordinates ( localization_pred )) # Final cost matrix cost_matrix = self . weight_l1 * cost_bbox + self . weight_class * cost_class + self . weight_giou * cost_giou return cost_matrix","title":"Module kerod.core.similarity"},{"location":"reference/kerod/core/similarity/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/similarity/#detrsimilarity","text":"class DetrSimilarity ( weight_class = 1 , weight_l1 = 5 , weight_giou = 2 )","title":"DetrSimilarity"},{"location":"reference/kerod/core/similarity/#ancestors-in-mro","text":"kerod.core.similarity.Similarity","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/similarity/#methods","text":"","title":"Methods"},{"location":"reference/kerod/core/similarity/#call","text":"def call ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> tensorflow . python . framework . ops . Tensor Compute the cost matrix according to the paper End to end object detection with transformers . Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. View Source def call ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ]) -> tf . Tensor : \"\"\" Compute the cost matrix according to the paper [ End to end object detection with transformers ]( https : //ai.facebook.com/research/publications/end-to-end-object-detection-with-transformers). Return : A 3 - D tensor of float32 with shape [ batch_size , N , M ] representing pairwise similarity scores defined in DeTr . \"\"\" classification_logits = y_pred [ BoxField . SCORES ] localization_pred = y_pred [ BoxField . BOXES ] out_prob = tf . nn . softmax ( classification_logits , -1 ) # Extract the target classes to approximate the classification cost # [batch_size, nb_class, num_detection] out_prob = tf . transpose ( out_prob , [ 0 , 2 , 1 ]) # [batch_size, nb_target, num_detection] cost = tf . gather_nd ( out_prob , get_full_indices ( y_true [ BoxField . LABELS ])) # Compute the classification cost. Contrary to the loss, we don't use the NLL, # but approximate it in 1 - proba[target class]. # The 1 is a constant that doesn't change the matching, it can be ommitted. # [batch_size, num_detection, nb_target] cost_class = - cost gt_boxes = y_true [ BoxField . BOXES ] # Compute the L1 cost between boxes # [batch_size, nb_target, num_detection] cost_bbox = tf . norm ( localization_pred [ : , None ] - gt_boxes [ : , : , None ], ord = 1 , axis = -1 , ) # Compute the giou cost betwen boxes # [batch_size, nb_target, num_detection] # loss_giou= 1 - giou but we approximate it with -giou cost_giou = - compute_giou ( convert_to_xyxy_coordinates ( gt_boxes ), convert_to_xyxy_coordinates ( localization_pred )) # Final cost matrix cost_matrix = self . weight_l1 * cost_bbox + self . weight_class * cost_class + self . weight_giou * cost_giou return cost_matrix","title":"call"},{"location":"reference/kerod/core/similarity/#iousimilarity","text":"class IoUSimilarity ( / , * args , ** kwargs )","title":"IoUSimilarity"},{"location":"reference/kerod/core/similarity/#ancestors-in-mro_1","text":"kerod.core.similarity.Similarity","title":"Ancestors (in MRO)"},{"location":"reference/kerod/core/similarity/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/core/similarity/#call_1","text":"def call ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], anchors : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) Computes pairwise intersection-over-union between boxes. Return: A 3-D tensor of float32 with shape [batch_size, N, M] representing pairwise similarity scores defined in DeTr. View Source def call ( self , y_true : Dict [ str , tf . Tensor ], anchors : Dict [ str , tf . Tensor ] ) : \"\"\" Computes pairwise intersection-over-union between boxes. Return : A 3 - D tensor of float32 with shape [ batch_size , N , M ] representing pairwise similarity scores defined in DeTr . \"\"\" return compute_iou ( y_true [ BoxField . BOXES ], anchors [ BoxField . BOXES ] )","title":"call"},{"location":"reference/kerod/core/similarity/#similarity","text":"class Similarity ( / , * args , ** kwargs )","title":"Similarity"},{"location":"reference/kerod/core/similarity/#descendants","text":"kerod.core.similarity.IoUSimilarity kerod.core.similarity.DetrSimilarity","title":"Descendants"},{"location":"reference/kerod/core/similarity/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/core/similarity/#call_2","text":"def call ( self , inputs1 , inputs2 ) -> tensorflow . python . framework . ops . Tensor View Source @abstractmethod def call ( self , inputs1 , inputs2 ) -> tf . Tensor : pass","title":"call"},{"location":"reference/kerod/core/standard_fields/","text":"Module kerod.core.standard_fields None None View Source class BoxField: BOXES = 'bbox' KEYPOINTS = 'keypoints' LABELS = 'label' MASKS = 'masks' NUM_BOXES = 'num_boxes' SCORES = 'scores' WEIGHTS = 'weights' class DatasetField: IMAGES = 'images' IMAGES_INFO = 'images_information' IMAGES_PMASK = 'images_padding_mask' Classes BoxField class BoxField ( / , * args , ** kwargs ) Class variables BOXES KEYPOINTS LABELS MASKS NUM_BOXES SCORES WEIGHTS DatasetField class DatasetField ( / , * args , ** kwargs ) Class variables IMAGES IMAGES_INFO IMAGES_PMASK","title":"Standard Fields"},{"location":"reference/kerod/core/standard_fields/#module-kerodcorestandard_fields","text":"None None View Source class BoxField: BOXES = 'bbox' KEYPOINTS = 'keypoints' LABELS = 'label' MASKS = 'masks' NUM_BOXES = 'num_boxes' SCORES = 'scores' WEIGHTS = 'weights' class DatasetField: IMAGES = 'images' IMAGES_INFO = 'images_information' IMAGES_PMASK = 'images_padding_mask'","title":"Module kerod.core.standard_fields"},{"location":"reference/kerod/core/standard_fields/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/standard_fields/#boxfield","text":"class BoxField ( / , * args , ** kwargs )","title":"BoxField"},{"location":"reference/kerod/core/standard_fields/#class-variables","text":"BOXES KEYPOINTS LABELS MASKS NUM_BOXES SCORES WEIGHTS","title":"Class variables"},{"location":"reference/kerod/core/standard_fields/#datasetfield","text":"class DatasetField ( / , * args , ** kwargs )","title":"DatasetField"},{"location":"reference/kerod/core/standard_fields/#class-variables_1","text":"IMAGES IMAGES_INFO IMAGES_PMASK","title":"Class variables"},{"location":"reference/kerod/core/target_assigner/","text":"Module kerod.core.target_assigner [Documentation taken from tensorflow/models/object_detection]. The code has been completely rewritten. Base target assigner module. The job of a TargetAssigner is, for a given set of anchors (bounding boxes) and groundtruth detections (bounding boxes), to assign classification and regression targets to each anchor as well as weights to each anchor (specifying, e.g., which anchors should not contribute to training loss). It assigns classification/regression targets by performing the following steps: Computing pairwise similarity between anchors and groundtruth boxes using a provided RegionSimilarity Calculator Computing a matching based on the similarity matrix using a provided Matcher Assigning regression targets based on the matching and a provided BoxCoder Assigning classification targets based on the matching and groundtruth labels View Source \"\"\"[Documentation taken from tensorflow/models/object_detection]. The code has been completely rewritten. Base target assigner module. The job of a TargetAssigner is, for a given set of anchors (bounding boxes) and groundtruth detections (bounding boxes), to assign classification and regression targets to each anchor as well as weights to each anchor (specifying, e.g., which anchors should not contribute to training loss). It assigns classification/regression targets by performing the following steps: 1. Computing pairwise similarity between anchors and groundtruth boxes using a provided RegionSimilarity Calculator 2. Computing a matching based on the similarity matrix using a provided Matcher 3. Assigning regression targets based on the matching and a provided BoxCoder 4. Assigning classification targets based on the matching and groundtruth labels \"\"\" from typing import Callable import tensorflow as tf from tensorflow.keras import backend as K from kerod.core.matcher import Matcher from kerod.core.standard_fields import BoxField from kerod.utils import item_assignment , get_full_indices class TargetAssigner : \"\"\"Target assigner to compute classification and regression targets. Arguments: similarity_calc: a method wich allow to compute a similarity between two batch of boxes matcher: an od.core.Matcher used to match groundtruth to anchors. box_encoder: a method which allow to encode matching groundtruth oxes with respect to anchors. negative_class_weight: A negative_class can be an unmatched anchors or a padded boxes. All egative classes will have a associated set to this corresponding value for he classification target. positive_class_weight: A positive_class is a matched foreground object \"\"\" def __init__ ( self , similarity_calc : Callable , matcher : Matcher , box_encoder : Callable , negative_class_weight = 0. , positive_class_weight = 1. , dtype = None ): self . _similarity_calc = similarity_calc self . _matcher = matcher self . _box_encoder = box_encoder if dtype is None : dtype = K . floatx () self . dtype = dtype self . negative_class_weight = tf . constant ( negative_class_weight , dtype = dtype ) self . positive_class_weight = tf . constant ( positive_class_weight , dtype = dtype ) @property def box_encoder ( self ): return self . _box_encoder def assign ( self , anchors : dict , groundtruth : dict ): \"\"\"Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: anchors: a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. groundtruth: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: Tuple: - `y_true`: A dict with : - *BoxField.LABELS*: A tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: A tensor with shape [batch_size, num_anchors, box_code_dimension] - `weights`: A dict with: - *BoxField.LABELS*: A tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: A tensor with shape [batch_size, num_anchors] \"\"\" shape = tf . shape ( groundtruth [ BoxField . BOXES ]) batch_size = shape [ 0 ] num_gt_boxes = shape [ 1 ] groundtruth_labels = groundtruth . get ( BoxField . LABELS ) groundtruth_weights = groundtruth . get ( BoxField . WEIGHTS ) if groundtruth_weights is None : groundtruth_weights = tf . ones ([ batch_size , num_gt_boxes ], self . dtype ) match_quality_matrix = self . _similarity_calc ( groundtruth , anchors ) matches , matched_labels = self . _matcher ( match_quality_matrix , groundtruth [ BoxField . NUM_BOXES ]) reg_targets = self . _create_regression_targets ( anchors , groundtruth , matches , matched_labels ) cls_targets = self . _create_classification_targets ( groundtruth_labels , matches , matched_labels ) groundtruth_weights = self . gather ( groundtruth_weights , matches ) reg_weights = self . _create_regression_weights ( groundtruth_weights , matched_labels ) cls_weights = self . _create_classification_weights ( groundtruth_weights , matched_labels ) y_true = { BoxField . LABELS : tf . cast ( cls_targets , tf . int32 ), BoxField . BOXES : tf . cast ( reg_targets , self . dtype ) } weights = { BoxField . LABELS : tf . cast ( cls_weights , self . dtype ), BoxField . BOXES : tf . cast ( reg_weights , self . dtype ) } return y_true , weights def gather ( self , tensor , indices ): indices = get_full_indices ( indices ) return tf . gather_nd ( tensor , indices ) def _create_regression_targets ( self , anchors : dict , groundtruth : dict , matches : tf . Tensor , matched_labels : tf . Tensor ) -> tf . Tensor : \"\"\"Returns a regression target for each anchor. Arguments: - *anchors*: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. - *groundtruth*: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] - *matches*: a tensor of float32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: *reg_targets*: A tensor with shape [N, box_code_dimension] \"\"\" matched_gt_boxes = self . gather ( groundtruth [ BoxField . BOXES ], matches ) matched_reg_targets = self . _box_encoder ( matched_gt_boxes , anchors [ BoxField . BOXES ]) # Zero out the unmatched and ignored regression targets. unmatched_ignored_reg_targets = tf . zeros_like ( matched_reg_targets , dtype = matched_reg_targets . dtype ) matched_anchors_mask = matched_labels >= 1 reg_targets = tf . where ( matched_anchors_mask [ ... , None ], x = matched_reg_targets , y = unmatched_ignored_reg_targets ) return reg_targets def _create_classification_targets ( self , groundtruth_labels : tf . Tensor , matches : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Create classification targets for each anchor. Assign a classification target of for each anchor to the matching groundtruth label that is provided by match. Anchors that are not matched to anything are given the target self._unmatched_cls_target Arguments: - *groundtruth_labels*: a tensor of shape [num_gt_boxes, d_1, ... d_k] with labels for each of the ground_truth boxes. The subshape [d_1, ... d_k] can be empty (corresponding to scalar labels). - *matches*: a tensor of float32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [num_anchors, d_1, d_2 ... d_k], where the subshape [d_1, ..., d_k] is compatible with groundtruth_labels which has shape [num_gt_boxes, d_1, d_2, ... d_k]. \"\"\" gathered_tensor = self . gather ( groundtruth_labels , matches ) # Set all the match values inferior or equal to 0 to background_classes indicator = matched_labels <= 0 gathered_tensor = item_assignment ( gathered_tensor , indicator , 0 ) return gathered_tensor def _create_regression_weights ( self , groundtruth_weights : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Set regression weight for each anchor. Only positive anchors are set to contribute to the regression loss, so this method returns a weight of 1 for every positive anchor and 0 for every negative anchor. Arguments: - *groundtruth_weights*: a float tensor of shape [M] indicating the weight to assign to all anchors match to a particular groundtruth box. - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [batch_size, num_anchors] representing the box regression weights. \"\"\" indicator = matched_labels > 0 weights = tf . where ( indicator , groundtruth_weights , 0 ) return weights def _create_classification_weights ( self , groundtruth_weights : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Create classification weights for each anchor. Positive (matched) anchors are associated with a weight of positive_class_weight and negative (unmatched) anchors are associated with a weight of negative_class_weight. When anchors are ignored, weights are set to zero. By default, both positive/negative weights are set to 1.0, but they can be adjusted to handle class imbalance (which is almost always the case in object detection). Arguments: - *groundtruth_weights*: a float tensor of shape [M] indicating the weight to assign to all anchors match to a particular groundtruth box. - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [batch_size, num_anchors] representing classification weights. \"\"\" indicator = matched_labels < 0 weights = tf . where ( indicator , self . negative_class_weight , groundtruth_weights ) indicator = matched_labels == 0 weights = tf . where ( indicator , self . positive_class_weight , weights ) return weights Classes TargetAssigner class TargetAssigner ( similarity_calc : Callable , matcher : kerod . core . matcher . Matcher , box_encoder : Callable , negative_class_weight = 0.0 , positive_class_weight = 1.0 , dtype = None ) Arguments Name Description similarity_calc a method wich allow to compute a similarity between two batch of boxes matcher an od.core.Matcher used to match groundtruth to anchors. box_encoder a method which allow to encode matching groundtruth oxes with respect to anchors. negative_class_weight A negative_class can be an unmatched anchors or a padded boxes. All egative classes will have a associated set to this corresponding value for he classification target. positive_class_weight A positive_class is a matched foreground object Methods assign def assign ( self , anchors : dict , groundtruth : dict ) Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Parameters: Name Description anchors a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. groundtruth a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: Type Description Tuple - y_true : A dict with : - BoxField.LABELS : A tensor with shape [batch_size, num_anchors] - BoxField.BOXES : A tensor with shape [batch_size, num_anchors, box_code_dimension] - weights : A dict with: - BoxField.LABELS : A tensor with shape [batch_size, num_anchors] - BoxField.BOXES : A tensor with shape [batch_size, num_anchors] View Source def assign ( self , anchors : dict , groundtruth : dict ) : \" \"\" Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: anchors: a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. groundtruth: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: Tuple: - `y_true`: A dict with : - *BoxField.LABELS*: A tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: A tensor with shape [batch_size, num_anchors, box_code_dimension] - `weights`: A dict with: - *BoxField.LABELS*: A tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: A tensor with shape [batch_size, num_anchors] \"\" \" shape = tf . shape ( groundtruth [ BoxField . BOXES ] ) batch_size = shape [ 0 ] num_gt_boxes = shape [ 1 ] groundtruth_labels = groundtruth . get ( BoxField . LABELS ) groundtruth_weights = groundtruth . get ( BoxField . WEIGHTS ) if groundtruth_weights is None : groundtruth_weights = tf . ones ( [ batch_size , num_gt_boxes ] , self . dtype ) match_quality_matrix = self . _similarity_calc ( groundtruth , anchors ) matches , matched_labels = self . _matcher ( match_quality_matrix , groundtruth [ BoxField . NUM_BOXES ] ) reg_targets = self . _create_regression_targets ( anchors , groundtruth , matches , matched_labels ) cls_targets = self . _create_classification_targets ( groundtruth_labels , matches , matched_labels ) groundtruth_weights = self . gather ( groundtruth_weights , matches ) reg_weights = self . _create_regression_weights ( groundtruth_weights , matched_labels ) cls_weights = self . _create_classification_weights ( groundtruth_weights , matched_labels ) y_true = { BoxField . LABELS : tf . cast ( cls_targets , tf . int32 ), BoxField . BOXES : tf . cast ( reg_targets , self . dtype ) } weights = { BoxField . LABELS : tf . cast ( cls_weights , self . dtype ), BoxField . BOXES : tf . cast ( reg_weights , self . dtype ) } return y_true , weights gather def gather ( self , tensor , indices ) View Source def gather ( self , tensor , indices ) : indices = get_full_indices ( indices ) return tf . gather_nd ( tensor , indices )","title":"Target Assigner"},{"location":"reference/kerod/core/target_assigner/#module-kerodcoretarget_assigner","text":"[Documentation taken from tensorflow/models/object_detection]. The code has been completely rewritten. Base target assigner module. The job of a TargetAssigner is, for a given set of anchors (bounding boxes) and groundtruth detections (bounding boxes), to assign classification and regression targets to each anchor as well as weights to each anchor (specifying, e.g., which anchors should not contribute to training loss). It assigns classification/regression targets by performing the following steps: Computing pairwise similarity between anchors and groundtruth boxes using a provided RegionSimilarity Calculator Computing a matching based on the similarity matrix using a provided Matcher Assigning regression targets based on the matching and a provided BoxCoder Assigning classification targets based on the matching and groundtruth labels View Source \"\"\"[Documentation taken from tensorflow/models/object_detection]. The code has been completely rewritten. Base target assigner module. The job of a TargetAssigner is, for a given set of anchors (bounding boxes) and groundtruth detections (bounding boxes), to assign classification and regression targets to each anchor as well as weights to each anchor (specifying, e.g., which anchors should not contribute to training loss). It assigns classification/regression targets by performing the following steps: 1. Computing pairwise similarity between anchors and groundtruth boxes using a provided RegionSimilarity Calculator 2. Computing a matching based on the similarity matrix using a provided Matcher 3. Assigning regression targets based on the matching and a provided BoxCoder 4. Assigning classification targets based on the matching and groundtruth labels \"\"\" from typing import Callable import tensorflow as tf from tensorflow.keras import backend as K from kerod.core.matcher import Matcher from kerod.core.standard_fields import BoxField from kerod.utils import item_assignment , get_full_indices class TargetAssigner : \"\"\"Target assigner to compute classification and regression targets. Arguments: similarity_calc: a method wich allow to compute a similarity between two batch of boxes matcher: an od.core.Matcher used to match groundtruth to anchors. box_encoder: a method which allow to encode matching groundtruth oxes with respect to anchors. negative_class_weight: A negative_class can be an unmatched anchors or a padded boxes. All egative classes will have a associated set to this corresponding value for he classification target. positive_class_weight: A positive_class is a matched foreground object \"\"\" def __init__ ( self , similarity_calc : Callable , matcher : Matcher , box_encoder : Callable , negative_class_weight = 0. , positive_class_weight = 1. , dtype = None ): self . _similarity_calc = similarity_calc self . _matcher = matcher self . _box_encoder = box_encoder if dtype is None : dtype = K . floatx () self . dtype = dtype self . negative_class_weight = tf . constant ( negative_class_weight , dtype = dtype ) self . positive_class_weight = tf . constant ( positive_class_weight , dtype = dtype ) @property def box_encoder ( self ): return self . _box_encoder def assign ( self , anchors : dict , groundtruth : dict ): \"\"\"Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: anchors: a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. groundtruth: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: Tuple: - `y_true`: A dict with : - *BoxField.LABELS*: A tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: A tensor with shape [batch_size, num_anchors, box_code_dimension] - `weights`: A dict with: - *BoxField.LABELS*: A tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: A tensor with shape [batch_size, num_anchors] \"\"\" shape = tf . shape ( groundtruth [ BoxField . BOXES ]) batch_size = shape [ 0 ] num_gt_boxes = shape [ 1 ] groundtruth_labels = groundtruth . get ( BoxField . LABELS ) groundtruth_weights = groundtruth . get ( BoxField . WEIGHTS ) if groundtruth_weights is None : groundtruth_weights = tf . ones ([ batch_size , num_gt_boxes ], self . dtype ) match_quality_matrix = self . _similarity_calc ( groundtruth , anchors ) matches , matched_labels = self . _matcher ( match_quality_matrix , groundtruth [ BoxField . NUM_BOXES ]) reg_targets = self . _create_regression_targets ( anchors , groundtruth , matches , matched_labels ) cls_targets = self . _create_classification_targets ( groundtruth_labels , matches , matched_labels ) groundtruth_weights = self . gather ( groundtruth_weights , matches ) reg_weights = self . _create_regression_weights ( groundtruth_weights , matched_labels ) cls_weights = self . _create_classification_weights ( groundtruth_weights , matched_labels ) y_true = { BoxField . LABELS : tf . cast ( cls_targets , tf . int32 ), BoxField . BOXES : tf . cast ( reg_targets , self . dtype ) } weights = { BoxField . LABELS : tf . cast ( cls_weights , self . dtype ), BoxField . BOXES : tf . cast ( reg_weights , self . dtype ) } return y_true , weights def gather ( self , tensor , indices ): indices = get_full_indices ( indices ) return tf . gather_nd ( tensor , indices ) def _create_regression_targets ( self , anchors : dict , groundtruth : dict , matches : tf . Tensor , matched_labels : tf . Tensor ) -> tf . Tensor : \"\"\"Returns a regression target for each anchor. Arguments: - *anchors*: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. - *groundtruth*: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] - *matches*: a tensor of float32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: *reg_targets*: A tensor with shape [N, box_code_dimension] \"\"\" matched_gt_boxes = self . gather ( groundtruth [ BoxField . BOXES ], matches ) matched_reg_targets = self . _box_encoder ( matched_gt_boxes , anchors [ BoxField . BOXES ]) # Zero out the unmatched and ignored regression targets. unmatched_ignored_reg_targets = tf . zeros_like ( matched_reg_targets , dtype = matched_reg_targets . dtype ) matched_anchors_mask = matched_labels >= 1 reg_targets = tf . where ( matched_anchors_mask [ ... , None ], x = matched_reg_targets , y = unmatched_ignored_reg_targets ) return reg_targets def _create_classification_targets ( self , groundtruth_labels : tf . Tensor , matches : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Create classification targets for each anchor. Assign a classification target of for each anchor to the matching groundtruth label that is provided by match. Anchors that are not matched to anything are given the target self._unmatched_cls_target Arguments: - *groundtruth_labels*: a tensor of shape [num_gt_boxes, d_1, ... d_k] with labels for each of the ground_truth boxes. The subshape [d_1, ... d_k] can be empty (corresponding to scalar labels). - *matches*: a tensor of float32 and shape [batch_size, N], where matches[b, i] is a matched ground-truth index in [b, 0, M) - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [num_anchors, d_1, d_2 ... d_k], where the subshape [d_1, ..., d_k] is compatible with groundtruth_labels which has shape [num_gt_boxes, d_1, d_2, ... d_k]. \"\"\" gathered_tensor = self . gather ( groundtruth_labels , matches ) # Set all the match values inferior or equal to 0 to background_classes indicator = matched_labels <= 0 gathered_tensor = item_assignment ( gathered_tensor , indicator , 0 ) return gathered_tensor def _create_regression_weights ( self , groundtruth_weights : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Set regression weight for each anchor. Only positive anchors are set to contribute to the regression loss, so this method returns a weight of 1 for every positive anchor and 0 for every negative anchor. Arguments: - *groundtruth_weights*: a float tensor of shape [M] indicating the weight to assign to all anchors match to a particular groundtruth box. - *match_labels*: a tensor of int32 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [batch_size, num_anchors] representing the box regression weights. \"\"\" indicator = matched_labels > 0 weights = tf . where ( indicator , groundtruth_weights , 0 ) return weights def _create_classification_weights ( self , groundtruth_weights : tf . Tensor , matched_labels : tf . Tensor ): \"\"\"Create classification weights for each anchor. Positive (matched) anchors are associated with a weight of positive_class_weight and negative (unmatched) anchors are associated with a weight of negative_class_weight. When anchors are ignored, weights are set to zero. By default, both positive/negative weights are set to 1.0, but they can be adjusted to handle class imbalance (which is almost always the case in object detection). Arguments: - *groundtruth_weights*: a float tensor of shape [M] indicating the weight to assign to all anchors match to a particular groundtruth box. - *match_labels*: a tensor of int8 and shape [batch_size, N], where match_labels[i] indicates whether a prediction is a true (1) or false positive (0) or ignored (-1) Returns: A tensor of shape [batch_size, num_anchors] representing classification weights. \"\"\" indicator = matched_labels < 0 weights = tf . where ( indicator , self . negative_class_weight , groundtruth_weights ) indicator = matched_labels == 0 weights = tf . where ( indicator , self . positive_class_weight , weights ) return weights","title":"Module kerod.core.target_assigner"},{"location":"reference/kerod/core/target_assigner/#classes","text":"","title":"Classes"},{"location":"reference/kerod/core/target_assigner/#targetassigner","text":"class TargetAssigner ( similarity_calc : Callable , matcher : kerod . core . matcher . Matcher , box_encoder : Callable , negative_class_weight = 0.0 , positive_class_weight = 1.0 , dtype = None )","title":"TargetAssigner"},{"location":"reference/kerod/core/target_assigner/#arguments","text":"Name Description similarity_calc a method wich allow to compute a similarity between two batch of boxes matcher an od.core.Matcher used to match groundtruth to anchors. box_encoder a method which allow to encode matching groundtruth oxes with respect to anchors. negative_class_weight A negative_class can be an unmatched anchors or a padded boxes. All egative classes will have a associated set to this corresponding value for he classification target. positive_class_weight A positive_class is a matched foreground object","title":"Arguments"},{"location":"reference/kerod/core/target_assigner/#methods","text":"","title":"Methods"},{"location":"reference/kerod/core/target_assigner/#assign","text":"def assign ( self , anchors : dict , groundtruth : dict ) Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Parameters: Name Description anchors a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. groundtruth a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: Type Description Tuple - y_true : A dict with : - BoxField.LABELS : A tensor with shape [batch_size, num_anchors] - BoxField.BOXES : A tensor with shape [batch_size, num_anchors, box_code_dimension] - weights : A dict with: - BoxField.LABELS : A tensor with shape [batch_size, num_anchors] - BoxField.BOXES : A tensor with shape [batch_size, num_anchors] View Source def assign ( self , anchors : dict , groundtruth : dict ) : \" \"\" Assign classification and regression targets to each anchor. For a given set of anchors and groundtruth detections, match anchors to groundtruth and assign classification and regression targets to each anchor as well as weights based on the resulting match (specifying, e.g., which anchors should not contribute to training loss). Anchors that are not matched to anything are given a classification target of self._unmatched_cls_target which can be specified via the constructor. Arguments: anchors: a dict representing a batch of M anchors 1. BoxField.BOXES: A tensor of shape [batch_size, num_anchors, (y1, x1, y2, x2)] representing the boxes and resized to the image shape. groundtruth: a dict representing a batch of M groundtruth boxes 1. BoxField.BOXES: A tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] representing the boxes and resized to the image shape 2. BoxField.LABELS: A tensor of shape [batch_size, num_gt, ] 3. BoxField.NUM_BOXES: A tensor of shape [batch_size]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [batch_size, num_gt] Returns: Tuple: - `y_true`: A dict with : - *BoxField.LABELS*: A tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: A tensor with shape [batch_size, num_anchors, box_code_dimension] - `weights`: A dict with: - *BoxField.LABELS*: A tensor with shape [batch_size, num_anchors] - *BoxField.BOXES*: A tensor with shape [batch_size, num_anchors] \"\" \" shape = tf . shape ( groundtruth [ BoxField . BOXES ] ) batch_size = shape [ 0 ] num_gt_boxes = shape [ 1 ] groundtruth_labels = groundtruth . get ( BoxField . LABELS ) groundtruth_weights = groundtruth . get ( BoxField . WEIGHTS ) if groundtruth_weights is None : groundtruth_weights = tf . ones ( [ batch_size , num_gt_boxes ] , self . dtype ) match_quality_matrix = self . _similarity_calc ( groundtruth , anchors ) matches , matched_labels = self . _matcher ( match_quality_matrix , groundtruth [ BoxField . NUM_BOXES ] ) reg_targets = self . _create_regression_targets ( anchors , groundtruth , matches , matched_labels ) cls_targets = self . _create_classification_targets ( groundtruth_labels , matches , matched_labels ) groundtruth_weights = self . gather ( groundtruth_weights , matches ) reg_weights = self . _create_regression_weights ( groundtruth_weights , matched_labels ) cls_weights = self . _create_classification_weights ( groundtruth_weights , matched_labels ) y_true = { BoxField . LABELS : tf . cast ( cls_targets , tf . int32 ), BoxField . BOXES : tf . cast ( reg_targets , self . dtype ) } weights = { BoxField . LABELS : tf . cast ( cls_weights , self . dtype ), BoxField . BOXES : tf . cast ( reg_weights , self . dtype ) } return y_true , weights","title":"assign"},{"location":"reference/kerod/core/target_assigner/#gather","text":"def gather ( self , tensor , indices ) View Source def gather ( self , tensor , indices ) : indices = get_full_indices ( indices ) return tf . gather_nd ( tensor , indices )","title":"gather"},{"location":"reference/kerod/dataset/","text":"Module kerod.dataset None None Sub-modules kerod.dataset.augmentation kerod.dataset.preprocessing kerod.dataset.utils","title":"Index"},{"location":"reference/kerod/dataset/#module-keroddataset","text":"None None","title":"Module kerod.dataset"},{"location":"reference/kerod/dataset/#sub-modules","text":"kerod.dataset.augmentation kerod.dataset.preprocessing kerod.dataset.utils","title":"Sub-modules"},{"location":"reference/kerod/dataset/augmentation/","text":"Module kerod.dataset.augmentation None None View Source from typing import Dict import tensorflow as tf from kerod.core import box_ops from kerod.core.standard_fields import BoxField from kerod.dataset.utils import filter_bad_area def random_horizontal_flip ( image : tf . Tensor , boxes : tf . Tensor , seed = None ): \"\"\"Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: - *image*: rank 3 float32 tensor with shape [height, width, channels]. - *boxes*: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: - *image*: image which is the same shape as input image. - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. \"\"\" with tf . name_scope ( 'RandomHorizontalFlip' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > .5 : image = tf . image . flip_left_right ( image ) boxes = box_ops . flip_left_right ( boxes ) return image , boxes def _random_crop ( value : tf . Tensor , size : tf . Tensor , seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *value*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `value`. - *seed*: A shape [2] Tensor, the seed to the random number generator. Must have Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *top_left_corner*: A 2-D tensor of int and shape [1, (y, x)] \"\"\" with tf . name_scope ( \"RandomCrop\" ): value = tf . convert_to_tensor ( value , name = \"value\" ) size = tf . convert_to_tensor ( size , dtype = tf . int32 , name = \"size\" ) shape = tf . shape ( value ) limit = shape - size + 1 offset = tf . random . uniform ( tf . shape ( shape ), dtype = size . dtype , maxval = size . dtype . max , seed = seed ) % limit return tf . slice ( value , offset , size ), offset [: 2 ] def random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ], seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *groundtruths*: Diction 1. bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. 2. label: A tensor of shape [N_crop <= N, ] \"\"\" boxes = groundtruths [ BoxField . BOXES ] crop , top_left_corner = _random_crop ( image , size , seed = seed ) shape = tf . cast ( tf . shape ( image )[: 2 ], boxes . dtype ) size = tf . tile ( tf . constant ( size [: 2 ], boxes . dtype )[ None ], ( 1 , 2 )) # scale the boxes to the size of the image boxes *= tf . tile ( shape [ tf . newaxis ], ( 1 , 2 )) top_left_corner = tf . tile ( top_left_corner [ tf . newaxis ], ( 1 , 2 )) top_left_corner = tf . cast ( top_left_corner , boxes . dtype ) # Translate according to top_left_corner # top_left_corner is now (y=0, x=0) cropped_boxes = boxes - top_left_corner # MinClip to 0 and MaxClip to size all the coords outside the crop. cropped_boxes = tf . maximum ( cropped_boxes , tf . cast ( 0 , boxes . dtype )) cropped_boxes = tf . minimum ( cropped_boxes , size ) # Renomarlized to have the boxes between 0 and 1 cropped_boxes = cropped_boxes / size # Copy groundtruths cropped_gt = { key : val for key , val in groundtruths . items ()} cropped_gt [ BoxField . BOXES ] = cropped_boxes return crop , filter_bad_area ( cropped_gt ) def random_random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ], seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Will `randomly` perform a random crop of a tensor to a given size. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop or not. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *image*: Either a cropped tensor or the same image of the same rank as `value` and shape `size`. - *boxes*: 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. \"\"\" with tf . name_scope ( 'RandomRandomCrop' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > .5 : image , groundtruths = random_crop ( image , size , groundtruths , seed = seed ) return image , groundtruths Functions random_crop def random_crop ( image : tensorflow . python . framework . ops . Tensor , size : tensorflow . python . framework . ops . Tensor , groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ], seed = None ) -> ( < class ' tensorflow . python . framework . ops . Tensor '>, <class ' tensorflow . python . framework . ops . Tensor '>) Randomly crops a tensor to a given size in a deterministic manner. Slices a shape size portion out of value at a uniformly chosen offset. Requires value.shape >= size . If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with size = [crop_height, crop_width, 3] . Guarantees the same results given the same seed independent of how many times the function is called, and independent of global seed settings (e.g. tf.random.set_seed ). Arguments: image : Input tensor to crop. size : 1-D tensor with size the rank of image . groundtruths : A dict with the following keys: bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. label: A tensorf of shape [N, ] seed : A shape [2] Tensor, the seed to the random number generator. Returns: crop : A cropped tensor of the same rank as value and shape size . groundtruths : Diction bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. label: A tensor of shape [N_crop <= N, ] View Source def random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ] , seed = None ) -> ( tf . Tensor , tf . Tensor ) : \" \"\" Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *groundtruths*: Diction 1. bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. 2. label: A tensor of shape [N_crop <= N, ] \"\" \" boxes = groundtruths [ BoxField . BOXES ] crop , top_left_corner = _random_crop ( image , size , seed = seed ) shape = tf . cast ( tf . shape ( image ) [ : 2 ] , boxes . dtype ) size = tf . tile ( tf . constant ( size [ : 2 ] , boxes . dtype ) [ None ] , ( 1 , 2 )) # scale the boxes to the size of the image boxes *= tf . tile ( shape [ tf . newaxis ] , ( 1 , 2 )) top_left_corner = tf . tile ( top_left_corner [ tf . newaxis ] , ( 1 , 2 )) top_left_corner = tf . cast ( top_left_corner , boxes . dtype ) # Translate according to top_left_corner # top_left_corner is now (y=0, x=0) cropped_boxes = boxes - top_left_corner # MinClip to 0 and MaxClip to size all the coords outside the crop. cropped_boxes = tf . maximum ( cropped_boxes , tf . cast ( 0 , boxes . dtype )) cropped_boxes = tf . minimum ( cropped_boxes , size ) # Renomarlized to have the boxes between 0 and 1 cropped_boxes = cropped_boxes / size # Copy groundtruths cropped_gt = { key : val for key , val in groundtruths . items () } cropped_gt [ BoxField . BOXES ] = cropped_boxes return crop , filter_bad_area ( cropped_gt ) random_horizontal_flip def random_horizontal_flip ( image : tensorflow . python . framework . ops . Tensor , boxes : tensorflow . python . framework . ops . Tensor , seed = None ) Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: image : rank 3 float32 tensor with shape [height, width, channels]. boxes : rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: image : image which is the same shape as input image. boxes : rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. View Source def random_horizontal_flip ( image : tf . Tensor , boxes : tf . Tensor , seed = None ): \"\"\"Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: - *image*: rank 3 float32 tensor with shape [height, width, channels]. - *boxes*: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: - *image*: image which is the same shape as input image. - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. \"\"\" with tf . name_scope ( 'RandomHorizontalFlip' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > . 5 : image = tf . image . flip_left_right ( image ) boxes = box_ops . flip_left_right ( boxes ) return image , boxes random_random_crop def random_random_crop ( image : tensorflow . python . framework . ops . Tensor , size : tensorflow . python . framework . ops . Tensor , groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ], seed = None ) -> ( < class ' tensorflow . python . framework . ops . Tensor '>, <class ' tensorflow . python . framework . ops . Tensor '>) Will randomly perform a random crop of a tensor to a given size. Slices a shape size portion out of value at a uniformly chosen offset. Requires value.shape >= size . If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with size = [crop_height, crop_width, 3] . Guarantees the same results given the same seed independent of how many times the function is called, and independent of global seed settings (e.g. tf.random.set_seed ). Arguments: image : Input tensor to crop or not. size : 1-D tensor with size the rank of image . groundtruths : A dict with the following keys: bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. label: A tensorf of shape [N, ] seed : A shape [2] Tensor, the seed to the random number generator. Returns: image : Either a cropped tensor or the same image of the same rank as value and shape size . boxes : 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. View Source def random_random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ] , seed = None ) -> ( tf . Tensor , tf . Tensor ) : \" \"\" Will `randomly` perform a random crop of a tensor to a given size. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop or not. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *image*: Either a cropped tensor or the same image of the same rank as `value` and shape `size`. - *boxes*: 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. \"\" \" with tf . name_scope ( 'RandomRandomCrop' ) : uniform_random = tf . random . uniform ( [] , 0 , 1.0 , seed = seed ) if uniform_random > .5 : image , groundtruths = random_crop ( image , size , groundtruths , seed = seed ) return image , groundtruths","title":"Augmentation"},{"location":"reference/kerod/dataset/augmentation/#module-keroddatasetaugmentation","text":"None None View Source from typing import Dict import tensorflow as tf from kerod.core import box_ops from kerod.core.standard_fields import BoxField from kerod.dataset.utils import filter_bad_area def random_horizontal_flip ( image : tf . Tensor , boxes : tf . Tensor , seed = None ): \"\"\"Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: - *image*: rank 3 float32 tensor with shape [height, width, channels]. - *boxes*: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: - *image*: image which is the same shape as input image. - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. \"\"\" with tf . name_scope ( 'RandomHorizontalFlip' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > .5 : image = tf . image . flip_left_right ( image ) boxes = box_ops . flip_left_right ( boxes ) return image , boxes def _random_crop ( value : tf . Tensor , size : tf . Tensor , seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *value*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `value`. - *seed*: A shape [2] Tensor, the seed to the random number generator. Must have Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *top_left_corner*: A 2-D tensor of int and shape [1, (y, x)] \"\"\" with tf . name_scope ( \"RandomCrop\" ): value = tf . convert_to_tensor ( value , name = \"value\" ) size = tf . convert_to_tensor ( size , dtype = tf . int32 , name = \"size\" ) shape = tf . shape ( value ) limit = shape - size + 1 offset = tf . random . uniform ( tf . shape ( shape ), dtype = size . dtype , maxval = size . dtype . max , seed = seed ) % limit return tf . slice ( value , offset , size ), offset [: 2 ] def random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ], seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *groundtruths*: Diction 1. bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. 2. label: A tensor of shape [N_crop <= N, ] \"\"\" boxes = groundtruths [ BoxField . BOXES ] crop , top_left_corner = _random_crop ( image , size , seed = seed ) shape = tf . cast ( tf . shape ( image )[: 2 ], boxes . dtype ) size = tf . tile ( tf . constant ( size [: 2 ], boxes . dtype )[ None ], ( 1 , 2 )) # scale the boxes to the size of the image boxes *= tf . tile ( shape [ tf . newaxis ], ( 1 , 2 )) top_left_corner = tf . tile ( top_left_corner [ tf . newaxis ], ( 1 , 2 )) top_left_corner = tf . cast ( top_left_corner , boxes . dtype ) # Translate according to top_left_corner # top_left_corner is now (y=0, x=0) cropped_boxes = boxes - top_left_corner # MinClip to 0 and MaxClip to size all the coords outside the crop. cropped_boxes = tf . maximum ( cropped_boxes , tf . cast ( 0 , boxes . dtype )) cropped_boxes = tf . minimum ( cropped_boxes , size ) # Renomarlized to have the boxes between 0 and 1 cropped_boxes = cropped_boxes / size # Copy groundtruths cropped_gt = { key : val for key , val in groundtruths . items ()} cropped_gt [ BoxField . BOXES ] = cropped_boxes return crop , filter_bad_area ( cropped_gt ) def random_random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ], seed = None ) -> ( tf . Tensor , tf . Tensor ): \"\"\"Will `randomly` perform a random crop of a tensor to a given size. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop or not. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *image*: Either a cropped tensor or the same image of the same rank as `value` and shape `size`. - *boxes*: 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. \"\"\" with tf . name_scope ( 'RandomRandomCrop' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > .5 : image , groundtruths = random_crop ( image , size , groundtruths , seed = seed ) return image , groundtruths","title":"Module kerod.dataset.augmentation"},{"location":"reference/kerod/dataset/augmentation/#functions","text":"","title":"Functions"},{"location":"reference/kerod/dataset/augmentation/#random_crop","text":"def random_crop ( image : tensorflow . python . framework . ops . Tensor , size : tensorflow . python . framework . ops . Tensor , groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ], seed = None ) -> ( < class ' tensorflow . python . framework . ops . Tensor '>, <class ' tensorflow . python . framework . ops . Tensor '>) Randomly crops a tensor to a given size in a deterministic manner. Slices a shape size portion out of value at a uniformly chosen offset. Requires value.shape >= size . If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with size = [crop_height, crop_width, 3] . Guarantees the same results given the same seed independent of how many times the function is called, and independent of global seed settings (e.g. tf.random.set_seed ). Arguments: image : Input tensor to crop. size : 1-D tensor with size the rank of image . groundtruths : A dict with the following keys: bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. label: A tensorf of shape [N, ] seed : A shape [2] Tensor, the seed to the random number generator. Returns: crop : A cropped tensor of the same rank as value and shape size . groundtruths : Diction bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. label: A tensor of shape [N_crop <= N, ] View Source def random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ] , seed = None ) -> ( tf . Tensor , tf . Tensor ) : \" \"\" Randomly crops a tensor to a given size in a deterministic manner. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *crop*: A cropped tensor of the same rank as `value` and shape `size`. - *groundtruths*: Diction 1. bbox: 2-D float32 tensor containing the bounding boxes -> [N_crop <=N, 4]. If cropped, the boxes with no area or outside the crop or removed. 2. label: A tensor of shape [N_crop <= N, ] \"\" \" boxes = groundtruths [ BoxField . BOXES ] crop , top_left_corner = _random_crop ( image , size , seed = seed ) shape = tf . cast ( tf . shape ( image ) [ : 2 ] , boxes . dtype ) size = tf . tile ( tf . constant ( size [ : 2 ] , boxes . dtype ) [ None ] , ( 1 , 2 )) # scale the boxes to the size of the image boxes *= tf . tile ( shape [ tf . newaxis ] , ( 1 , 2 )) top_left_corner = tf . tile ( top_left_corner [ tf . newaxis ] , ( 1 , 2 )) top_left_corner = tf . cast ( top_left_corner , boxes . dtype ) # Translate according to top_left_corner # top_left_corner is now (y=0, x=0) cropped_boxes = boxes - top_left_corner # MinClip to 0 and MaxClip to size all the coords outside the crop. cropped_boxes = tf . maximum ( cropped_boxes , tf . cast ( 0 , boxes . dtype )) cropped_boxes = tf . minimum ( cropped_boxes , size ) # Renomarlized to have the boxes between 0 and 1 cropped_boxes = cropped_boxes / size # Copy groundtruths cropped_gt = { key : val for key , val in groundtruths . items () } cropped_gt [ BoxField . BOXES ] = cropped_boxes return crop , filter_bad_area ( cropped_gt )","title":"random_crop"},{"location":"reference/kerod/dataset/augmentation/#random_horizontal_flip","text":"def random_horizontal_flip ( image : tensorflow . python . framework . ops . Tensor , boxes : tensorflow . python . framework . ops . Tensor , seed = None ) Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: image : rank 3 float32 tensor with shape [height, width, channels]. boxes : rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: image : image which is the same shape as input image. boxes : rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. View Source def random_horizontal_flip ( image : tf . Tensor , boxes : tf . Tensor , seed = None ): \"\"\"Randomly flips the image and detections horizontally. The probability of flipping the image is 50%. Arguments: - *image*: rank 3 float32 tensor with shape [height, width, channels]. - *boxes*: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. Returns: - *image*: image which is the same shape as input image. - *boxes*: rank 2 float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form meaning their coordinates vary between [0, 1]. \"\"\" with tf . name_scope ( 'RandomHorizontalFlip' ): uniform_random = tf . random . uniform ([], 0 , 1.0 , seed = seed ) if uniform_random > . 5 : image = tf . image . flip_left_right ( image ) boxes = box_ops . flip_left_right ( boxes ) return image , boxes","title":"random_horizontal_flip"},{"location":"reference/kerod/dataset/augmentation/#random_random_crop","text":"def random_random_crop ( image : tensorflow . python . framework . ops . Tensor , size : tensorflow . python . framework . ops . Tensor , groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ], seed = None ) -> ( < class ' tensorflow . python . framework . ops . Tensor '>, <class ' tensorflow . python . framework . ops . Tensor '>) Will randomly perform a random crop of a tensor to a given size. Slices a shape size portion out of value at a uniformly chosen offset. Requires value.shape >= size . If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with size = [crop_height, crop_width, 3] . Guarantees the same results given the same seed independent of how many times the function is called, and independent of global seed settings (e.g. tf.random.set_seed ). Arguments: image : Input tensor to crop or not. size : 1-D tensor with size the rank of image . groundtruths : A dict with the following keys: bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. label: A tensorf of shape [N, ] seed : A shape [2] Tensor, the seed to the random number generator. Returns: image : Either a cropped tensor or the same image of the same rank as value and shape size . boxes : 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. View Source def random_random_crop ( image : tf . Tensor , size : tf . Tensor , groundtruths : Dict [ str , tf . Tensor ] , seed = None ) -> ( tf . Tensor , tf . Tensor ) : \" \"\" Will `randomly` perform a random crop of a tensor to a given size. Slices a shape `size` portion out of `value` at a uniformly chosen offset. Requires `value.shape >= size`. If a dimension should not be cropped, pass the full size of that dimension. For example, RGB images can be cropped with `size = [crop_height, crop_width, 3]`. Guarantees the same results given the same `seed` independent of how many times the function is called, and independent of global seed settings (e.g. `tf.random.set_seed`). Arguments: - *image*: Input tensor to crop or not. - *size*: 1-D tensor with size the rank of `image`. - *groundtruths*: A dict with the following keys: 1. bbox: rank 2 float32 tensor with shape [N, 4] containing the bounding boxes. Boxes are in normalized form meaning their coordinates vary between [0, 1]. Each row is in the form of [ymin, xmin, ymax, xmax]. 2. label: A tensorf of shape [N, ] - *seed*: A shape [2] Tensor, the seed to the random number generator. Returns: - *image*: Either a cropped tensor or the same image of the same rank as `value` and shape `size`. - *boxes*: 2-D float32 tensor containing the bounding boxes -> [N, 4]. Boxes are in normalized form, meaning their coordinates vary between [0, 1]. If cropped, the boxes with no area or outside the crop or removed. \"\" \" with tf . name_scope ( 'RandomRandomCrop' ) : uniform_random = tf . random . uniform ( [] , 0 , 1.0 , seed = seed ) if uniform_random > .5 : image , groundtruths = random_crop ( image , size , groundtruths , seed = seed ) return image , groundtruths","title":"random_random_crop"},{"location":"reference/kerod/dataset/preprocessing/","text":"Module kerod.dataset.preprocessing None None View Source import tensorflow as tf from kerod . core import constants from kerod . core . standard_fields import BoxField , DatasetField from kerod . dataset . utils import filter_crowded_boxes , filter_bad_area from kerod . dataset import augmentation as aug def resize_to_min_dim ( image , short_edge_length , max_dimension ) : \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : - *image*: A np.array of size [height, width, channels]. - *short_edge_length*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above `kerod.core.constants.MAX_IMAGE_SIZE` \"\"\" if max_dimension > constants . MAX_IMAGE_DIMENSION : raise ValueError ( f \"The max_dimension can only be inferior or equal to {constants.MAX_IMAGE_DIMENSION}\" ) shape = tf . shape ( image ) height = tf . cast ( shape [ 0 ], tf . float32 ) width = tf . cast ( shape [ 1 ], tf . float32 ) im_size_min = tf . minimum ( height , width ) im_size_max = tf . maximum ( height , width ) scale = short_edge_length / im_size_min # Prevent the biggest axis from being more than MAX_SIZE if tf . math . round ( scale * im_size_max ) > max_dimension: scale = max_dimension / im_size_max target_height = tf . cast ( height * scale , dtype = tf . int32 ) target_width = tf . cast ( width * scale , dtype = tf . int32 ) return tf . image . resize ( tf . expand_dims ( image , axis = 0 ), size = [ target_height , target_width ], method = tf . image . ResizeMethod . BILINEAR )[ 0 ] def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) : \"\"\"This operations performs a classical preprocessing operations for localization datasets: - COCO - Pascal Voc You can download easily those dataset using [tensorflow dataset](https://www.tensorflow.org/datasets/catalog/overview). Arguments: - *inputs*: It can be either a [FeaturesDict](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeaturesDict) or a dict. but it should have the following structures. ```python inputs = FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'objects': Sequence({ 'area': Tensor(shape=(), dtype=tf.int64), # area 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), # The values are between 0 and 1 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80), }), }) ``` - *bgr*: Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with `tf.image.decode_image` will open an image in RGB. However, OpenCV will open it in BGR by default. - *horizontal_flip*: Activate the random horizontal flip. - *random_crop_size*: 1-D tensor with size the rank of `image` (e.g: (400, 600, 0)). - *padded_mask*: If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: - *inputs*: 1. image: A 3D tensor of float32 and shape [None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. 3. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training \"\"\" image = inputs [ 'image' ][ : , : , ::- 1 ] if bgr else inputs [ 'image' ] image = tf . cast ( image , tf . float32 ) targets = inputs [ 'objects' ] if horizontal_flip: image , targets [ BoxField . BOXES ] = aug . random_horizontal_flip ( image , targets [ BoxField . BOXES ]) if random_crop_size is not None : if tf . shape ( image )[ 0 ] < random_crop_size [ 0 ] or tf . shape ( image )[ 1 ] < random_crop_size [ 1 ] : image = resize_to_min_dim ( image , max ( random_crop_size ), 1333.0 ) image , targets = aug . random_random_crop ( image , random_crop_size , targets ) if 'is_crowd' in targets : targets = filter_crowded_boxes ( targets ) targets = filter_bad_area ( targets ) image = resize_to_min_dim ( image , 800.0 , 1333.0 ) image_information = tf . cast ( tf . shape ( image )[ : 2 ], dtype = tf . float32 ) inputs = { DatasetField . IMAGES : image , DatasetField . IMAGES_INFO : image_information } if padded_mask: inputs [ DatasetField . IMAGES_PMASK ] = tf . ones (( tf . shape ( image )[ 0 ], tf . shape ( image )[ 1 ]), dtype = tf . int8 ) ground_truths = { BoxField . BOXES : targets [ BoxField . BOXES ] * tf . tile ( image_information [ tf . newaxis ], [ 1 , 2 ]), BoxField . LABELS : tf . cast ( targets [ BoxField . LABELS ], tf . int32 ), BoxField . NUM_BOXES : tf . shape ( targets [ BoxField . LABELS ]), BoxField . WEIGHTS : tf . fill ( tf . shape ( targets [ BoxField . LABELS ]), 1.0 ) } return inputs , ground_truths def expand_dims_for_single_batch ( inputs , ground_truths ) : \"\"\"In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: - `expand_dims`: ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` > Execution time: 0.002636657891998766 - `batch` ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.batch(1) ``` > Execution time: 0.004332915792008862 - `padded_batch` ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.padded_batch(batch_size, padded_shapes=padded_shapes) ``` > Execution time: 0.0055130551019974515 Returns: - *inputs*: The features and the ground_truths are mixed together 1. DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] 2. DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [1, num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [1] \"\"\" inputs = { DatasetField . IMAGES : inputs [ DatasetField . IMAGES ][ None ], DatasetField . IMAGES_INFO : inputs [ DatasetField . IMAGES_INFO ][ None ] } ground_truths = { BoxField . BOXES : ground_truths [ BoxField . BOXES ][ None ], BoxField . LABELS : ground_truths [ BoxField . LABELS ][ None ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ][ None ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ][ None ] } return inputs , ground_truths Functions expand_dims_for_single_batch def expand_dims_for_single_batch ( inputs , ground_truths ) In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: expand_dims : ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . map ( expand_dims_for_single_batch , num_parallel_calls = tf . data . experimental . AUTOTUNE ) Execution time: 0.002636657891998766 batch ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . batch ( 1 ) Execution time: 0.004332915792008862 padded_batch ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . padded_batch ( batch_size , padded_shapes = padded_shapes ) Execution time: 0.0055130551019974515 Returns: inputs : The features and the ground_truths are mixed together DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a padded_batch operations. The models needs those information in order to clip the boxes to the proper dimension. ground_truths : BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape BoxField.LABELS: A tensor of shape [1, num_boxes, ] BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training BoxField.WEIGHTS: A tensor of shape [1] View Source def expand_dims_for_single_batch ( inputs , ground_truths ): \"\"\"In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: - `expand_dims`: ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` > Execution time: 0.002636657891998766 - `batch` ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.batch(1) ``` > Execution time: 0.004332915792008862 - `padded_batch` ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.padded_batch(batch_size, padded_shapes=padded_shapes) ``` > Execution time: 0.0055130551019974515 Returns: - *inputs*: The features and the ground_truths are mixed together 1. DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] 2. DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [1, num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [1] \"\"\" inputs = { DatasetField . IMAGES : inputs [ DatasetField . IMAGES ][ None ], DatasetField . IMAGES_INFO : inputs [ DatasetField . IMAGES_INFO ][ None ] } ground_truths = { BoxField . BOXES : ground_truths [ BoxField . BOXES ][ None ], BoxField . LABELS : ground_truths [ BoxField . LABELS ][ None ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ][ None ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ][ None ] } return inputs , ground_truths preprocess def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) This operations performs a classical preprocessing operations for localization datasets: COCO Pascal Voc You can download easily those dataset using tensorflow dataset . Arguments: inputs : It can be either a FeaturesDict or a dict. but it should have the following structures. inputs = FeaturesDict ({ 'image' : Image ( shape = ( None , None , 3 ), dtype = tf . uint8 ), 'objects' : Sequence ({ 'area' : Tensor ( shape = (), dtype = tf . int64 ), # area 'bbox' : BBoxFeature ( shape = ( 4 ,), dtype = tf . float32 ), # The values are between 0 and 1 'label' : ClassLabel ( shape = (), dtype = tf . int64 , num_classes = 80 ), }), }) bgr : Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with tf.image.decode_image will open an image in RGB. However, OpenCV will open it in BGR by default. horizontal_flip : Activate the random horizontal flip. random_crop_size : 1-D tensor with size the rank of image (e.g: (400, 600, 0)). padded_mask : If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: inputs : image: A 3D tensor of float32 and shape [None, None, 3] image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a padded_batch operations. The models needs those information in order to clip the boxes to the proper dimension. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. ground_truths : BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape BoxField.LABELS: A tensor of shape [num_boxes, ] BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training View Source def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) : \"\"\"This operations performs a classical preprocessing operations for localization datasets: - COCO - Pascal Voc You can download easily those dataset using [tensorflow dataset](https://www.tensorflow.org/datasets/catalog/overview). Arguments: - *inputs*: It can be either a [FeaturesDict](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeaturesDict) or a dict. but it should have the following structures. ```python inputs = FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'objects': Sequence({ 'area': Tensor(shape=(), dtype=tf.int64), # area 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), # The values are between 0 and 1 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80), }), }) ``` - *bgr*: Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with `tf.image.decode_image` will open an image in RGB. However, OpenCV will open it in BGR by default. - *horizontal_flip*: Activate the random horizontal flip. - *random_crop_size*: 1-D tensor with size the rank of `image` (e.g: (400, 600, 0)). - *padded_mask*: If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: - *inputs*: 1. image: A 3D tensor of float32 and shape [None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. 3. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training \"\"\" image = inputs [ 'image' ][ : , : , ::- 1 ] if bgr else inputs [ 'image' ] image = tf . cast ( image , tf . float32 ) targets = inputs [ 'objects' ] if horizontal_flip: image , targets [ BoxField . BOXES ] = aug . random_horizontal_flip ( image , targets [ BoxField . BOXES ]) if random_crop_size is not None : if tf . shape ( image )[ 0 ] < random_crop_size [ 0 ] or tf . shape ( image )[ 1 ] < random_crop_size [ 1 ] : image = resize_to_min_dim ( image , max ( random_crop_size ), 1333.0 ) image , targets = aug . random_random_crop ( image , random_crop_size , targets ) if 'is_crowd' in targets : targets = filter_crowded_boxes ( targets ) targets = filter_bad_area ( targets ) image = resize_to_min_dim ( image , 800.0 , 1333.0 ) image_information = tf . cast ( tf . shape ( image )[ : 2 ], dtype = tf . float32 ) inputs = { DatasetField . IMAGES : image , DatasetField . IMAGES_INFO : image_information } if padded_mask: inputs [ DatasetField . IMAGES_PMASK ] = tf . ones (( tf . shape ( image )[ 0 ], tf . shape ( image )[ 1 ]), dtype = tf . int8 ) ground_truths = { BoxField . BOXES : targets [ BoxField . BOXES ] * tf . tile ( image_information [ tf . newaxis ], [ 1 , 2 ]), BoxField . LABELS : tf . cast ( targets [ BoxField . LABELS ], tf . int32 ), BoxField . NUM_BOXES : tf . shape ( targets [ BoxField . LABELS ]), BoxField . WEIGHTS : tf . fill ( tf . shape ( targets [ BoxField . LABELS ]), 1.0 ) } return inputs , ground_truths resize_to_min_dim def resize_to_min_dim ( image , short_edge_length , max_dimension ) Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : image : A np.array of size [height, width, channels]. short_edge_length : minimum image dimension. max_dimension : If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - resized_image : The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above kerod.core.constants.MAX_IMAGE_SIZE View Source def resize_to_min_dim ( image , short_edge_length , max_dimension ): \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : - *image*: A np.array of size [height, width, channels]. - *short_edge_length*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above `kerod.core.constants.MAX_IMAGE_SIZE` \"\"\" if max_dimension > constants . MAX_IMAGE_DIMENSION : raise ValueError ( f \"The max_dimension can only be inferior or equal to {constants.MAX_IMAGE_DIMENSION}\" ) shape = tf . shape ( image ) height = tf . cast ( shape [ 0 ], tf . float32 ) width = tf . cast ( shape [ 1 ], tf . float32 ) im_size_min = tf . minimum ( height , width ) im_size_max = tf . maximum ( height , width ) scale = short_edge_length / im_size_min # Prevent the biggest axis from being more than MAX_SIZE if tf . math . round ( scale * im_size_max ) > max_dimension : scale = max_dimension / im_size_max target_height = tf . cast ( height * scale , dtype = tf . int32 ) target_width = tf . cast ( width * scale , dtype = tf . int32 ) return tf . image . resize ( tf . expand_dims ( image , axis = 0 ), size = [ target_height , target_width ], method = tf . image . ResizeMethod . BILINEAR )[ 0 ]","title":"Preprocessing"},{"location":"reference/kerod/dataset/preprocessing/#module-keroddatasetpreprocessing","text":"None None View Source import tensorflow as tf from kerod . core import constants from kerod . core . standard_fields import BoxField , DatasetField from kerod . dataset . utils import filter_crowded_boxes , filter_bad_area from kerod . dataset import augmentation as aug def resize_to_min_dim ( image , short_edge_length , max_dimension ) : \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : - *image*: A np.array of size [height, width, channels]. - *short_edge_length*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above `kerod.core.constants.MAX_IMAGE_SIZE` \"\"\" if max_dimension > constants . MAX_IMAGE_DIMENSION : raise ValueError ( f \"The max_dimension can only be inferior or equal to {constants.MAX_IMAGE_DIMENSION}\" ) shape = tf . shape ( image ) height = tf . cast ( shape [ 0 ], tf . float32 ) width = tf . cast ( shape [ 1 ], tf . float32 ) im_size_min = tf . minimum ( height , width ) im_size_max = tf . maximum ( height , width ) scale = short_edge_length / im_size_min # Prevent the biggest axis from being more than MAX_SIZE if tf . math . round ( scale * im_size_max ) > max_dimension: scale = max_dimension / im_size_max target_height = tf . cast ( height * scale , dtype = tf . int32 ) target_width = tf . cast ( width * scale , dtype = tf . int32 ) return tf . image . resize ( tf . expand_dims ( image , axis = 0 ), size = [ target_height , target_width ], method = tf . image . ResizeMethod . BILINEAR )[ 0 ] def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) : \"\"\"This operations performs a classical preprocessing operations for localization datasets: - COCO - Pascal Voc You can download easily those dataset using [tensorflow dataset](https://www.tensorflow.org/datasets/catalog/overview). Arguments: - *inputs*: It can be either a [FeaturesDict](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeaturesDict) or a dict. but it should have the following structures. ```python inputs = FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'objects': Sequence({ 'area': Tensor(shape=(), dtype=tf.int64), # area 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), # The values are between 0 and 1 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80), }), }) ``` - *bgr*: Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with `tf.image.decode_image` will open an image in RGB. However, OpenCV will open it in BGR by default. - *horizontal_flip*: Activate the random horizontal flip. - *random_crop_size*: 1-D tensor with size the rank of `image` (e.g: (400, 600, 0)). - *padded_mask*: If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: - *inputs*: 1. image: A 3D tensor of float32 and shape [None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. 3. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training \"\"\" image = inputs [ 'image' ][ : , : , ::- 1 ] if bgr else inputs [ 'image' ] image = tf . cast ( image , tf . float32 ) targets = inputs [ 'objects' ] if horizontal_flip: image , targets [ BoxField . BOXES ] = aug . random_horizontal_flip ( image , targets [ BoxField . BOXES ]) if random_crop_size is not None : if tf . shape ( image )[ 0 ] < random_crop_size [ 0 ] or tf . shape ( image )[ 1 ] < random_crop_size [ 1 ] : image = resize_to_min_dim ( image , max ( random_crop_size ), 1333.0 ) image , targets = aug . random_random_crop ( image , random_crop_size , targets ) if 'is_crowd' in targets : targets = filter_crowded_boxes ( targets ) targets = filter_bad_area ( targets ) image = resize_to_min_dim ( image , 800.0 , 1333.0 ) image_information = tf . cast ( tf . shape ( image )[ : 2 ], dtype = tf . float32 ) inputs = { DatasetField . IMAGES : image , DatasetField . IMAGES_INFO : image_information } if padded_mask: inputs [ DatasetField . IMAGES_PMASK ] = tf . ones (( tf . shape ( image )[ 0 ], tf . shape ( image )[ 1 ]), dtype = tf . int8 ) ground_truths = { BoxField . BOXES : targets [ BoxField . BOXES ] * tf . tile ( image_information [ tf . newaxis ], [ 1 , 2 ]), BoxField . LABELS : tf . cast ( targets [ BoxField . LABELS ], tf . int32 ), BoxField . NUM_BOXES : tf . shape ( targets [ BoxField . LABELS ]), BoxField . WEIGHTS : tf . fill ( tf . shape ( targets [ BoxField . LABELS ]), 1.0 ) } return inputs , ground_truths def expand_dims_for_single_batch ( inputs , ground_truths ) : \"\"\"In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: - `expand_dims`: ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` > Execution time: 0.002636657891998766 - `batch` ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.batch(1) ``` > Execution time: 0.004332915792008862 - `padded_batch` ```python ds_train = tfds.load(name=\" voc \", split=\" train \", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.padded_batch(batch_size, padded_shapes=padded_shapes) ``` > Execution time: 0.0055130551019974515 Returns: - *inputs*: The features and the ground_truths are mixed together 1. DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] 2. DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [1, num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [1] \"\"\" inputs = { DatasetField . IMAGES : inputs [ DatasetField . IMAGES ][ None ], DatasetField . IMAGES_INFO : inputs [ DatasetField . IMAGES_INFO ][ None ] } ground_truths = { BoxField . BOXES : ground_truths [ BoxField . BOXES ][ None ], BoxField . LABELS : ground_truths [ BoxField . LABELS ][ None ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ][ None ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ][ None ] } return inputs , ground_truths","title":"Module kerod.dataset.preprocessing"},{"location":"reference/kerod/dataset/preprocessing/#functions","text":"","title":"Functions"},{"location":"reference/kerod/dataset/preprocessing/#expand_dims_for_single_batch","text":"def expand_dims_for_single_batch ( inputs , ground_truths ) In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: expand_dims : ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . map ( expand_dims_for_single_batch , num_parallel_calls = tf . data . experimental . AUTOTUNE ) Execution time: 0.002636657891998766 batch ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . batch ( 1 ) Execution time: 0.004332915792008862 padded_batch ds_train = tfds . load ( name = \"voc\" , split = \"train\" , shuffle_files = True ) ds_train = ds_train . map ( preprocess , num_parallel_calls = tf . data . experimental . AUTOTUNE ) ds_train = ds_train . padded_batch ( batch_size , padded_shapes = padded_shapes ) Execution time: 0.0055130551019974515 Returns: inputs : The features and the ground_truths are mixed together DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a padded_batch operations. The models needs those information in order to clip the boxes to the proper dimension. ground_truths : BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape BoxField.LABELS: A tensor of shape [1, num_boxes, ] BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training BoxField.WEIGHTS: A tensor of shape [1] View Source def expand_dims_for_single_batch ( inputs , ground_truths ): \"\"\"In order to train your model you need to add a batch dimension to the output of the preprocess function. For a single batch operation this method is faster: - `expand_dims`: ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.map(expand_dims_for_single_batch, num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` > Execution time: 0.002636657891998766 - `batch` ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.batch(1) ``` > Execution time: 0.004332915792008862 - `padded_batch` ```python ds_train = tfds.load(name=\"voc\", split=\"train\", shuffle_files=True) ds_train = ds_train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE) ds_train = ds_train.padded_batch(batch_size, padded_shapes=padded_shapes) ``` > Execution time: 0.0055130551019974515 Returns: - *inputs*: The features and the ground_truths are mixed together 1. DatasetField.IMAGES: A 3D tensor of float32 and shape [None, None, 3] 2. DatasetField.IMAGES_INFO: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [1, num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [1, num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape [1, 1]. It is usefull to unpad the data in case of a batched training 4. BoxField.WEIGHTS: A tensor of shape [1] \"\"\" inputs = { DatasetField . IMAGES : inputs [ DatasetField . IMAGES ][ None ], DatasetField . IMAGES_INFO : inputs [ DatasetField . IMAGES_INFO ][ None ] } ground_truths = { BoxField . BOXES : ground_truths [ BoxField . BOXES ][ None ], BoxField . LABELS : ground_truths [ BoxField . LABELS ][ None ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ][ None ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ][ None ] } return inputs , ground_truths","title":"expand_dims_for_single_batch"},{"location":"reference/kerod/dataset/preprocessing/#preprocess","text":"def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) This operations performs a classical preprocessing operations for localization datasets: COCO Pascal Voc You can download easily those dataset using tensorflow dataset . Arguments: inputs : It can be either a FeaturesDict or a dict. but it should have the following structures. inputs = FeaturesDict ({ 'image' : Image ( shape = ( None , None , 3 ), dtype = tf . uint8 ), 'objects' : Sequence ({ 'area' : Tensor ( shape = (), dtype = tf . int64 ), # area 'bbox' : BBoxFeature ( shape = ( 4 ,), dtype = tf . float32 ), # The values are between 0 and 1 'label' : ClassLabel ( shape = (), dtype = tf . int64 , num_classes = 80 ), }), }) bgr : Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with tf.image.decode_image will open an image in RGB. However, OpenCV will open it in BGR by default. horizontal_flip : Activate the random horizontal flip. random_crop_size : 1-D tensor with size the rank of image (e.g: (400, 600, 0)). padded_mask : If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: inputs : image: A 3D tensor of float32 and shape [None, None, 3] image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a padded_batch operations. The models needs those information in order to clip the boxes to the proper dimension. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. ground_truths : BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape BoxField.LABELS: A tensor of shape [num_boxes, ] BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training View Source def preprocess ( inputs , bgr = True , horizontal_flip = True , random_crop_size = None , padded_mask = False ) : \"\"\"This operations performs a classical preprocessing operations for localization datasets: - COCO - Pascal Voc You can download easily those dataset using [tensorflow dataset](https://www.tensorflow.org/datasets/catalog/overview). Arguments: - *inputs*: It can be either a [FeaturesDict](https://www.tensorflow.org/datasets/api_docs/python/tfds/features/FeaturesDict) or a dict. but it should have the following structures. ```python inputs = FeaturesDict({ 'image': Image(shape=(None, None, 3), dtype=tf.uint8), 'objects': Sequence({ 'area': Tensor(shape=(), dtype=tf.int64), # area 'bbox': BBoxFeature(shape=(4,), dtype=tf.float32), # The values are between 0 and 1 'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=80), }), }) ``` - *bgr*: Convert your input image to BGR (od.model.faster_rcnn.FasterRcnnFPNResnet50 needs it). If you have open your image with `tf.image.decode_image` will open an image in RGB. However, OpenCV will open it in BGR by default. - *horizontal_flip*: Activate the random horizontal flip. - *random_crop_size*: 1-D tensor with size the rank of `image` (e.g: (400, 600, 0)). - *padded_mask*: If set to true return a mask of 1 of the image. When padded we will know which parts is from the original image. Returns: - *inputs*: 1. image: A 3D tensor of float32 and shape [None, None, 3] 2. image_informations: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. It can be usefull if it followed by a `padded_batch` operations. The models needs those information in order to clip the boxes to the proper dimension. 3. images_padding_mask: If padded_mask set to true return a 2D tensor of int8 and shape [None, None, 3]. Mask of the image if a padding is performed we will know where the original image was. - *ground_truths*: 1. BoxField.BOXES: A tensor of shape [num_boxes, (y1, x1, y2, x2)] and resized to the image shape 2. BoxField.LABELS: A tensor of shape [num_boxes, ] 3. BoxField.NUM_BOXES: A tensor of shape (). It is usefull to unpad the data in case of a batched training \"\"\" image = inputs [ 'image' ][ : , : , ::- 1 ] if bgr else inputs [ 'image' ] image = tf . cast ( image , tf . float32 ) targets = inputs [ 'objects' ] if horizontal_flip: image , targets [ BoxField . BOXES ] = aug . random_horizontal_flip ( image , targets [ BoxField . BOXES ]) if random_crop_size is not None : if tf . shape ( image )[ 0 ] < random_crop_size [ 0 ] or tf . shape ( image )[ 1 ] < random_crop_size [ 1 ] : image = resize_to_min_dim ( image , max ( random_crop_size ), 1333.0 ) image , targets = aug . random_random_crop ( image , random_crop_size , targets ) if 'is_crowd' in targets : targets = filter_crowded_boxes ( targets ) targets = filter_bad_area ( targets ) image = resize_to_min_dim ( image , 800.0 , 1333.0 ) image_information = tf . cast ( tf . shape ( image )[ : 2 ], dtype = tf . float32 ) inputs = { DatasetField . IMAGES : image , DatasetField . IMAGES_INFO : image_information } if padded_mask: inputs [ DatasetField . IMAGES_PMASK ] = tf . ones (( tf . shape ( image )[ 0 ], tf . shape ( image )[ 1 ]), dtype = tf . int8 ) ground_truths = { BoxField . BOXES : targets [ BoxField . BOXES ] * tf . tile ( image_information [ tf . newaxis ], [ 1 , 2 ]), BoxField . LABELS : tf . cast ( targets [ BoxField . LABELS ], tf . int32 ), BoxField . NUM_BOXES : tf . shape ( targets [ BoxField . LABELS ]), BoxField . WEIGHTS : tf . fill ( tf . shape ( targets [ BoxField . LABELS ]), 1.0 ) } return inputs , ground_truths","title":"preprocess"},{"location":"reference/kerod/dataset/preprocessing/#resize_to_min_dim","text":"def resize_to_min_dim ( image , short_edge_length , max_dimension ) Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : image : A np.array of size [height, width, channels]. short_edge_length : minimum image dimension. max_dimension : If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - resized_image : The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above kerod.core.constants.MAX_IMAGE_SIZE View Source def resize_to_min_dim ( image , short_edge_length , max_dimension ): \"\"\"Resize an image given to the min size maintaining the aspect ratio. If one of the image dimensions is bigger than the max_dimension after resizing, it will scale the image such that its biggest dimension is equal to the max_dimension. Arguments : - *image*: A np.array of size [height, width, channels]. - *short_edge_length*: minimum image dimension. - *max_dimension*: If the resized largest size is over max_dimension. Will use to max_dimension to compute the resizing ratio. Returns: - *resized_image*: The input image resized with the aspect_ratio preserved in float32 Raises: ValueError: If the max_dimension is above `kerod.core.constants.MAX_IMAGE_SIZE` \"\"\" if max_dimension > constants . MAX_IMAGE_DIMENSION : raise ValueError ( f \"The max_dimension can only be inferior or equal to {constants.MAX_IMAGE_DIMENSION}\" ) shape = tf . shape ( image ) height = tf . cast ( shape [ 0 ], tf . float32 ) width = tf . cast ( shape [ 1 ], tf . float32 ) im_size_min = tf . minimum ( height , width ) im_size_max = tf . maximum ( height , width ) scale = short_edge_length / im_size_min # Prevent the biggest axis from being more than MAX_SIZE if tf . math . round ( scale * im_size_max ) > max_dimension : scale = max_dimension / im_size_max target_height = tf . cast ( height * scale , dtype = tf . int32 ) target_width = tf . cast ( width * scale , dtype = tf . int32 ) return tf . image . resize ( tf . expand_dims ( image , axis = 0 ), size = [ target_height , target_width ], method = tf . image . ResizeMethod . BILINEAR )[ 0 ]","title":"resize_to_min_dim"},{"location":"reference/kerod/dataset/utils/","text":"Module kerod.dataset.utils None None View Source from typing import Dict import tensorflow as tf from kerod.core.box_ops import compute_area from kerod.core.standard_fields import BoxField def _filter ( _dict , _filter ): keys = { BoxField . BOXES , BoxField . LABELS , BoxField . MASKS , 'is_crowd' } filtered_dict = {} for key in _dict . keys (): if key in keys : filtered_dict [ key ] = tf . gather_nd ( _dict [ key ], _filter ) else : filtered_dict [ key ] = _dict [ key ] return filtered_dict def filter_crowded_boxes ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ]: \"\"\"Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: - *groundtruths*: A dict with the following keys 1. boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. labels: A tensor of shape [num_boxes, ] 3. crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. `True` is for crowded box. Returns: - *groundtruths*: Filtered groundtruths \"\"\" ind_uncrowded_boxes = tf . where ( tf . equal ( groundtruths [ 'is_crowd' ], False )) return _filter ( groundtruths , ind_uncrowded_boxes ) def filter_bad_area ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ]: \"\"\"Remove all the boxes that have an area less or equal to 0. Arguments: - *groundtruths*: A dict with the following keys: 1. bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. label: A tensor of shape [num_boxes, ] Returns: - *groundtruths*: All the groundtruths which match have not been filtered. \"\"\" area = compute_area ( groundtruths [ BoxField . BOXES ]) filter_area = tf . where ( area > 0 ) return _filter ( groundtruths , filter_area ) Functions filter_bad_area def filter_bad_area ( groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> Dict [ str , tensorflow . python . framework . ops . Tensor ] Remove all the boxes that have an area less or equal to 0. Arguments: groundtruths : A dict with the following keys: bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] label: A tensor of shape [num_boxes, ] Returns: groundtruths : All the groundtruths which match have not been filtered. View Source def filter_bad_area ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ] : \"\"\"Remove all the boxes that have an area less or equal to 0. Arguments: - *groundtruths*: A dict with the following keys: 1. bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. label: A tensor of shape [num_boxes, ] Returns: - *groundtruths*: All the groundtruths which match have not been filtered. \"\"\" area = compute_area ( groundtruths [ BoxField . BOXES ]) filter_area = tf . where ( area > 0 ) return _filter ( groundtruths , filter_area ) filter_crowded_boxes def filter_crowded_boxes ( groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> Dict [ str , tensorflow . python . framework . ops . Tensor ] Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: groundtruths : A dict with the following keys boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] labels: A tensor of shape [num_boxes, ] crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. True is for crowded box. Returns: groundtruths : Filtered groundtruths View Source def filter_crowded_boxes ( groundtruths : Dict [ str , tf . Tensor ] ) -> Dict [ str , tf . Tensor ] : \" \"\" Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: - *groundtruths*: A dict with the following keys 1. boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. labels: A tensor of shape [num_boxes, ] 3. crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. `True` is for crowded box. Returns: - *groundtruths*: Filtered groundtruths \"\" \" ind_uncrowded_boxes = tf . where ( tf . equal ( groundtruths [ 'is_crowd' ] , False )) return _filter ( groundtruths , ind_uncrowded_boxes )","title":"Utils"},{"location":"reference/kerod/dataset/utils/#module-keroddatasetutils","text":"None None View Source from typing import Dict import tensorflow as tf from kerod.core.box_ops import compute_area from kerod.core.standard_fields import BoxField def _filter ( _dict , _filter ): keys = { BoxField . BOXES , BoxField . LABELS , BoxField . MASKS , 'is_crowd' } filtered_dict = {} for key in _dict . keys (): if key in keys : filtered_dict [ key ] = tf . gather_nd ( _dict [ key ], _filter ) else : filtered_dict [ key ] = _dict [ key ] return filtered_dict def filter_crowded_boxes ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ]: \"\"\"Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: - *groundtruths*: A dict with the following keys 1. boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. labels: A tensor of shape [num_boxes, ] 3. crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. `True` is for crowded box. Returns: - *groundtruths*: Filtered groundtruths \"\"\" ind_uncrowded_boxes = tf . where ( tf . equal ( groundtruths [ 'is_crowd' ], False )) return _filter ( groundtruths , ind_uncrowded_boxes ) def filter_bad_area ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ]: \"\"\"Remove all the boxes that have an area less or equal to 0. Arguments: - *groundtruths*: A dict with the following keys: 1. bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. label: A tensor of shape [num_boxes, ] Returns: - *groundtruths*: All the groundtruths which match have not been filtered. \"\"\" area = compute_area ( groundtruths [ BoxField . BOXES ]) filter_area = tf . where ( area > 0 ) return _filter ( groundtruths , filter_area )","title":"Module kerod.dataset.utils"},{"location":"reference/kerod/dataset/utils/#functions","text":"","title":"Functions"},{"location":"reference/kerod/dataset/utils/#filter_bad_area","text":"def filter_bad_area ( groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> Dict [ str , tensorflow . python . framework . ops . Tensor ] Remove all the boxes that have an area less or equal to 0. Arguments: groundtruths : A dict with the following keys: bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] label: A tensor of shape [num_boxes, ] Returns: groundtruths : All the groundtruths which match have not been filtered. View Source def filter_bad_area ( groundtruths : Dict [ str , tf . Tensor ]) -> Dict [ str , tf . Tensor ] : \"\"\"Remove all the boxes that have an area less or equal to 0. Arguments: - *groundtruths*: A dict with the following keys: 1. bbox: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. label: A tensor of shape [num_boxes, ] Returns: - *groundtruths*: All the groundtruths which match have not been filtered. \"\"\" area = compute_area ( groundtruths [ BoxField . BOXES ]) filter_area = tf . where ( area > 0 ) return _filter ( groundtruths , filter_area )","title":"filter_bad_area"},{"location":"reference/kerod/dataset/utils/#filter_crowded_boxes","text":"def filter_crowded_boxes ( groundtruths : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> Dict [ str , tensorflow . python . framework . ops . Tensor ] Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: groundtruths : A dict with the following keys boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] labels: A tensor of shape [num_boxes, ] crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. True is for crowded box. Returns: groundtruths : Filtered groundtruths View Source def filter_crowded_boxes ( groundtruths : Dict [ str , tf . Tensor ] ) -> Dict [ str , tf . Tensor ] : \" \"\" Coco has boxes flagged as crowded which are not used during the training. This function will discard them. Arguments: - *groundtruths*: A dict with the following keys 1. boxes: A tensor of shape [num_boxes, (y1, x1, y2, x2)] 2. labels: A tensor of shape [num_boxes, ] 3. crowd: Boolean tensor which indicates if the boxes is crowded are not. Crowded means that the boxes contains multiple entities which are to difficult to localize one by one. `True` is for crowded box. Returns: - *groundtruths*: Filtered groundtruths \"\" \" ind_uncrowded_boxes = tf . where ( tf . equal ( groundtruths [ 'is_crowd' ] , False )) return _filter ( groundtruths , ind_uncrowded_boxes )","title":"filter_crowded_boxes"},{"location":"reference/kerod/layers/","text":"Module kerod.layers None None View Source from kerod.layers.anchors import Anchors from kerod.layers.attentions import MultiHeadAttention from kerod.layers.positional_encoding import ( PositionEmbeddingLearned , PositionEmbeddingSine ) from kerod.layers.smca.reference_points import SMCAReferencePoints from kerod.layers.smca.weight_map import DynamicalWeightMaps from kerod.layers.transformer import ( DecoderLayer , EncoderLayer , Transformer ) from kerod.layers.detection.fast_rcnn import FastRCNN from kerod.layers.detection.rpn import RegionProposalNetwork Sub-modules kerod.layers.anchors kerod.layers.attentions kerod.layers.detection kerod.layers.patches kerod.layers.positional_encoding kerod.layers.post_processing kerod.layers.smca kerod.layers.transformer","title":"Index"},{"location":"reference/kerod/layers/#module-kerodlayers","text":"None None View Source from kerod.layers.anchors import Anchors from kerod.layers.attentions import MultiHeadAttention from kerod.layers.positional_encoding import ( PositionEmbeddingLearned , PositionEmbeddingSine ) from kerod.layers.smca.reference_points import SMCAReferencePoints from kerod.layers.smca.weight_map import DynamicalWeightMaps from kerod.layers.transformer import ( DecoderLayer , EncoderLayer , Transformer ) from kerod.layers.detection.fast_rcnn import FastRCNN from kerod.layers.detection.rpn import RegionProposalNetwork","title":"Module kerod.layers"},{"location":"reference/kerod/layers/#sub-modules","text":"kerod.layers.anchors kerod.layers.attentions kerod.layers.detection kerod.layers.patches kerod.layers.positional_encoding kerod.layers.post_processing kerod.layers.smca kerod.layers.transformer","title":"Sub-modules"},{"location":"reference/kerod/layers/anchors/","text":"Module kerod.layers.anchors None None View Source import tensorflow as tf from kerod.core.constants import MAX_IMAGE_DIMENSION from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} def generate_anchors ( stride : int , scales : tf . Tensor , ratios : tf . Tensor , max_size : int ): \"\"\"Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image. At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments: stride: Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales: The scale of the anchors e.g: 8, 16, 32 ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size: Maximum size of the input image. The anchors will computed once and for all. Returns: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" ratios , scales = tf . meshgrid ( ratios , scales ) scales = tf . reshape ( scales , [ - 1 ]) ratios = tf . reshape ( ratios , [ - 1 ]) ratio_sqrts = tf . sqrt ( ratios ) widths = scales / ratio_sqrts heights = ratios * widths base_anchors = tf . stack ([ - widths , - heights , widths , heights ], axis =- 1 ) * 0.5 max_stride = tf . math . ceil ( max_size / stride ) y_centers = tf . cast ( tf . range ( max_stride ), dtype = scales . dtype ) * stride x_centers = tf . cast ( tf . range ( max_stride ), dtype = scales . dtype ) * stride x_centers , y_centers = tf . meshgrid ( x_centers , y_centers ) shifts = tf . stack ([ x_centers , y_centers , x_centers , y_centers ], axis =- 1 ) anchors = tf . expand_dims ( base_anchors , 0 ) + tf . expand_dims ( shifts , 2 ) return tf . gather ( anchors , [ 1 , 0 , 3 , 2 ], axis =- 1 ) class Anchors ( tf . keras . layers . Layer ): \"\"\"Will generate a determistic grid and store it in memory to avoid recompute it at each run. At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments: stride: Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales: The scale of the anchors e.g: 8, 16, 32 ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size: Maximum size of the input image. The anchors will computed once and for all. Call arguments: inputs: A tensor of shape [batch_size, height, widht, channel] Call returns: tf.Tensor: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" def __init__ ( self , stride , scales , ratios , ** kwargs ): super () . __init__ ( ** kwargs ) self . _stride = stride self . _scales = scales self . _ratios = ratios self . _anchors = generate_anchors ( stride , tf . constant ([ scales ], self . _compute_dtype ), tf . constant ( ratios , self . _compute_dtype ), max_size = MAX_IMAGE_DIMENSION ) def call ( self , inputs ): \"\"\"Return anchors based on the shape of the input tensors Arguments: inputs: A tensor of shape [batch_size, height, widht, channel] Returns: tf.Tensor: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" shape = tf . shape ( inputs ) height , width = shape [ 1 ], shape [ 2 ] anchors = self . _anchors [: height , : width ] return tf . reshape ( anchors , ( - 1 , 4 )) def get_config ( self ): config = super () . get_config () config [ 'stride' ] = self . _stride config [ 'scales' ] = self . _scales config [ 'ratios' ] = self . _ratios return config remove_unwanted_doc ( Anchors , __pdoc__ ) Variables MAX_IMAGE_DIMENSION Functions generate_anchors def generate_anchors ( stride : int , scales : tensorflow . python . framework . ops . Tensor , ratios : tensorflow . python . framework . ops . Tensor , max_size : int ) Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image. At each forward according to the shape of the tensor we can extract the corresponding anchors. Parameters: Name Description stride Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales The scale of the anchors e.g: 8, 16, 32 ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size Maximum size of the input image. The anchors will computed once and for all. Returns: Type Description None A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. View Source def generate_anchors ( stride : int , scales : tf . Tensor , ratios : tf . Tensor , max_size : int ) : \"\"\" Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image . At each forward according to the shape of the tensor we can extract the corresponding anchors . Arguments : stride : Downscaling ratio compared to the original image . At stride 16 your original image will be 16 times bigger than your actual tensor scales : The scale of the anchors e . g : 8 , 16 , 32 ratios : The ratios are the different shapes that you want to apply on your anchors . e . g : ( 0 . 5 , 1 , 2 ) max_size : Maximum size of the input image . The anchors will computed once and for all . Returns : A tensor of shape [ num_scales * num_ratios * height * width , 4 ]. The anchors have the format [ y_min , x_min , y_max , x_max ]. \"\"\" ratios , scales = tf . meshgrid ( ratios , scales ) scales = tf . reshape ( scales , [ - 1 ] ) ratios = tf . reshape ( ratios , [ - 1 ] ) ratio_sqrts = tf . sqrt ( ratios ) widths = scales / ratio_sqrts heights = ratios * widths base_anchors = tf . stack ( [ - widths , - heights , widths , heights ], axis =- 1 ) * 0 . 5 max_stride = tf . math . ceil ( max_size / stride ) y_centers = tf . cast ( tf . range ( max_stride ) , dtype = scales . dtype ) * stride x_centers = tf . cast ( tf . range ( max_stride ) , dtype = scales . dtype ) * stride x_centers , y_centers = tf . meshgrid ( x_centers , y_centers ) shifts = tf . stack ( [ x_centers , y_centers , x_centers , y_centers ], axis =- 1 ) anchors = tf . expand_dims ( base_anchors , 0 ) + tf . expand_dims ( shifts , 2 ) return tf . gather ( anchors , [ 1 , 0 , 3 , 2 ], axis =- 1 ) Classes Anchors class Anchors ( stride , scales , ratios , ** kwargs ) At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments Name Description stride Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales The scale of the anchors e.g: 8, 16, 32 ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size Maximum size of the input image. The anchors will computed once and for all. Call arguments Name Description inputs A tensor of shape [batch_size, height, widht, channel] Call returns Type Description tf.Tensor A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , inputs ) Return anchors based on the shape of the input tensors Parameters: Name Description inputs A tensor of shape [batch_size, height, widht, channel] Returns: Type Description tf.Tensor A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. View Source def call ( self , inputs ) : \"\"\" Return anchors based on the shape of the input tensors Arguments : inputs : A tensor of shape [ batch_size , height , widht , channel ] Returns : tf . Tensor : A tensor of shape [ num_scales * num_ratios * height * width , 4 ]. The anchors have the format [ y_min , x_min , y_max , x_max ]. \"\"\" shape = tf . shape ( inputs ) height , width = shape [ 1 ], shape [ 2 ] anchors = self . _anchors [: height , : width ] return tf . reshape ( anchors , ( - 1 , 4 ))","title":"Anchors"},{"location":"reference/kerod/layers/anchors/#module-kerodlayersanchors","text":"None None View Source import tensorflow as tf from kerod.core.constants import MAX_IMAGE_DIMENSION from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} def generate_anchors ( stride : int , scales : tf . Tensor , ratios : tf . Tensor , max_size : int ): \"\"\"Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image. At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments: stride: Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales: The scale of the anchors e.g: 8, 16, 32 ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size: Maximum size of the input image. The anchors will computed once and for all. Returns: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" ratios , scales = tf . meshgrid ( ratios , scales ) scales = tf . reshape ( scales , [ - 1 ]) ratios = tf . reshape ( ratios , [ - 1 ]) ratio_sqrts = tf . sqrt ( ratios ) widths = scales / ratio_sqrts heights = ratios * widths base_anchors = tf . stack ([ - widths , - heights , widths , heights ], axis =- 1 ) * 0.5 max_stride = tf . math . ceil ( max_size / stride ) y_centers = tf . cast ( tf . range ( max_stride ), dtype = scales . dtype ) * stride x_centers = tf . cast ( tf . range ( max_stride ), dtype = scales . dtype ) * stride x_centers , y_centers = tf . meshgrid ( x_centers , y_centers ) shifts = tf . stack ([ x_centers , y_centers , x_centers , y_centers ], axis =- 1 ) anchors = tf . expand_dims ( base_anchors , 0 ) + tf . expand_dims ( shifts , 2 ) return tf . gather ( anchors , [ 1 , 0 , 3 , 2 ], axis =- 1 ) class Anchors ( tf . keras . layers . Layer ): \"\"\"Will generate a determistic grid and store it in memory to avoid recompute it at each run. At each forward according to the shape of the tensor we can extract the corresponding anchors. Arguments: stride: Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales: The scale of the anchors e.g: 8, 16, 32 ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size: Maximum size of the input image. The anchors will computed once and for all. Call arguments: inputs: A tensor of shape [batch_size, height, widht, channel] Call returns: tf.Tensor: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" def __init__ ( self , stride , scales , ratios , ** kwargs ): super () . __init__ ( ** kwargs ) self . _stride = stride self . _scales = scales self . _ratios = ratios self . _anchors = generate_anchors ( stride , tf . constant ([ scales ], self . _compute_dtype ), tf . constant ( ratios , self . _compute_dtype ), max_size = MAX_IMAGE_DIMENSION ) def call ( self , inputs ): \"\"\"Return anchors based on the shape of the input tensors Arguments: inputs: A tensor of shape [batch_size, height, widht, channel] Returns: tf.Tensor: A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. \"\"\" shape = tf . shape ( inputs ) height , width = shape [ 1 ], shape [ 2 ] anchors = self . _anchors [: height , : width ] return tf . reshape ( anchors , ( - 1 , 4 )) def get_config ( self ): config = super () . get_config () config [ 'stride' ] = self . _stride config [ 'scales' ] = self . _scales config [ 'ratios' ] = self . _ratios return config remove_unwanted_doc ( Anchors , __pdoc__ )","title":"Module kerod.layers.anchors"},{"location":"reference/kerod/layers/anchors/#variables","text":"MAX_IMAGE_DIMENSION","title":"Variables"},{"location":"reference/kerod/layers/anchors/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/anchors/#generate_anchors","text":"def generate_anchors ( stride : int , scales : tensorflow . python . framework . ops . Tensor , ratios : tensorflow . python . framework . ops . Tensor , max_size : int ) Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image. At each forward according to the shape of the tensor we can extract the corresponding anchors. Parameters: Name Description stride Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales The scale of the anchors e.g: 8, 16, 32 ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size Maximum size of the input image. The anchors will computed once and for all. Returns: Type Description None A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. View Source def generate_anchors ( stride : int , scales : tf . Tensor , ratios : tf . Tensor , max_size : int ) : \"\"\" Will generate a determistic grid based on the input images dimension. The anchors will be generated once and for all on the biggest possible image . At each forward according to the shape of the tensor we can extract the corresponding anchors . Arguments : stride : Downscaling ratio compared to the original image . At stride 16 your original image will be 16 times bigger than your actual tensor scales : The scale of the anchors e . g : 8 , 16 , 32 ratios : The ratios are the different shapes that you want to apply on your anchors . e . g : ( 0 . 5 , 1 , 2 ) max_size : Maximum size of the input image . The anchors will computed once and for all . Returns : A tensor of shape [ num_scales * num_ratios * height * width , 4 ]. The anchors have the format [ y_min , x_min , y_max , x_max ]. \"\"\" ratios , scales = tf . meshgrid ( ratios , scales ) scales = tf . reshape ( scales , [ - 1 ] ) ratios = tf . reshape ( ratios , [ - 1 ] ) ratio_sqrts = tf . sqrt ( ratios ) widths = scales / ratio_sqrts heights = ratios * widths base_anchors = tf . stack ( [ - widths , - heights , widths , heights ], axis =- 1 ) * 0 . 5 max_stride = tf . math . ceil ( max_size / stride ) y_centers = tf . cast ( tf . range ( max_stride ) , dtype = scales . dtype ) * stride x_centers = tf . cast ( tf . range ( max_stride ) , dtype = scales . dtype ) * stride x_centers , y_centers = tf . meshgrid ( x_centers , y_centers ) shifts = tf . stack ( [ x_centers , y_centers , x_centers , y_centers ], axis =- 1 ) anchors = tf . expand_dims ( base_anchors , 0 ) + tf . expand_dims ( shifts , 2 ) return tf . gather ( anchors , [ 1 , 0 , 3 , 2 ], axis =- 1 )","title":"generate_anchors"},{"location":"reference/kerod/layers/anchors/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/anchors/#anchors","text":"class Anchors ( stride , scales , ratios , ** kwargs ) At each forward according to the shape of the tensor we can extract the corresponding anchors.","title":"Anchors"},{"location":"reference/kerod/layers/anchors/#arguments","text":"Name Description stride Downscaling ratio compared to the original image. At stride 16 your original image will be 16 times bigger than your actual tensor scales The scale of the anchors e.g: 8, 16, 32 ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) max_size Maximum size of the input image. The anchors will computed once and for all.","title":"Arguments"},{"location":"reference/kerod/layers/anchors/#call-arguments","text":"Name Description inputs A tensor of shape [batch_size, height, widht, channel]","title":"Call arguments"},{"location":"reference/kerod/layers/anchors/#call-returns","text":"Type Description tf.Tensor A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max].","title":"Call returns"},{"location":"reference/kerod/layers/anchors/#ancestors-in-mro","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/anchors/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/anchors/#call","text":"def call ( self , inputs ) Return anchors based on the shape of the input tensors Parameters: Name Description inputs A tensor of shape [batch_size, height, widht, channel] Returns: Type Description tf.Tensor A tensor of shape [num_scales * num_ratios * height * width, 4]. The anchors have the format [y_min, x_min, y_max, x_max]. View Source def call ( self , inputs ) : \"\"\" Return anchors based on the shape of the input tensors Arguments : inputs : A tensor of shape [ batch_size , height , widht , channel ] Returns : tf . Tensor : A tensor of shape [ num_scales * num_ratios * height * width , 4 ]. The anchors have the format [ y_min , x_min , y_max , x_max ]. \"\"\" shape = tf . shape ( inputs ) height , width = shape [ 1 ], shape [ 2 ] anchors = self . _anchors [: height , : width ] return tf . reshape ( anchors , ( - 1 , 4 ))","title":"call"},{"location":"reference/kerod/layers/attentions/","text":"Module kerod.layers.attentions None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class MultiHeadAttention ( tf . keras . layers . Layer ): \"\"\"Allows the model to jointly attend to information from different representation subspaces. See reference: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) Arguments: d_model: The number of expected features in the decoder inputs num_heads: The number of heads in the multiheadattention models. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. attention_axes: axes over which the attention is applied. `None` means attention over all axes, but batch, heads, and features. Call arguments: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Call returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dropout_rate = 0. , attention_axes =- 1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . d_model = d_model assert d_model % self . num_heads == 0 self . depth = d_model // self . num_heads self . query = tf . keras . layers . Dense ( d_model ) self . key = tf . keras . layers . Dense ( d_model ) self . value = tf . keras . layers . Dense ( d_model ) self . dense = tf . keras . layers . Dense ( d_model ) self . dropout = tf . keras . layers . Dropout ( dropout_rate ) self . softmax = tf . keras . layers . Softmax ( axis = attention_axes ) def split_heads ( self , tgt : tf . Tensor , batch_size : int ): \"\"\"Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) \"\"\" tgt = tf . reshape ( tgt , ( batch_size , - 1 , self . num_heads , self . depth )) return tf . transpose ( tgt , perm = [ 0 , 2 , 1 , 3 ]) def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ): \"\"\" Args: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\"\" batch_size = tf . shape ( query )[ 0 ] # (batch_size, num_heads, seq_len_q, depth) query = self . split_heads ( self . query ( query ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) key = self . split_heads ( self . key ( key ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) value = self . split_heads ( self . value ( value ), batch_size ) # scaled dot product attention # (batch_size, nh, seq_len_q, depth) x (batch_size, nh, depth, seq_len_k) # = (batch_size, nh, seq_len_q, seq_len_k) matmul_qk = tf . matmul ( query , key , transpose_b = True ) # Here we normalize by depth_k suppose K and Q are two matrices # with mean=0 and var=1. After QK^T will have a matrix with # mean=0 and var= 1 * depth_k. QK^T/sqrt(depth_k) => mean=0 and var=1 scaled_attention_logits = matmul_qk / tf . math . sqrt ( tf . cast ( self . depth , self . compute_dtype )) if attn_mask is not None : scaled_attention_logits += attn_mask if key_padding_mask is not None : # Apply -inf if the pixels is a padding # False means padded so we take: not key_padding_mask scaled_attention_logits = tf . where ( ~ key_padding_mask [:, None , None ], tf . zeros_like ( scaled_attention_logits ) + float ( '-inf' ), scaled_attention_logits ) # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. # (..., seq_len_q, seq_len_k) attention_weights = self . softmax ( scaled_attention_logits ) attention_weights = self . dropout ( attention_weights , training = training ) scaled_attention = tf . matmul ( attention_weights , value ) # (batch_size, seq_len_q, nh, depth) scaled_attention = tf . transpose ( scaled_attention , perm = [ 0 , 2 , 1 , 3 ]) # (batch_size, seq_len_q, d_model) concat_attention = tf . reshape ( scaled_attention , ( batch_size , - 1 , self . d_model )) return self . dense ( concat_attention ) remove_unwanted_doc ( MultiHeadAttention , __pdoc__ ) Classes MultiHeadAttention class MultiHeadAttention ( d_model : int , num_heads : int , dropout_rate = 0.0 , attention_axes =- 1 , ** kwargs ) See reference: Attention Is All You Need Arguments Name Description d_model The number of expected features in the decoder inputs num_heads The number of heads in the multiheadattention models. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. attention_axes axes over which the attention is applied. None means attention over all axes, but batch, heads, and features. Call arguments Name Description value A 3-D tensor of shape [batch_size, seq_len, depth_v] key A 3-D tensor of shape [batch_size, seq_len, depth] query A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Call returns Type Description tf.Tensor A 3-D tensor of shape [batch_size, seq_len_q, d_model] Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ) Parameters: Name Description value A 3-D tensor of shape [batch_size, seq_len, depth_v] key A 3-D tensor of shape [batch_size, seq_len, depth] query A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: Type Description tf.Tensor A 3-D tensor of shape [batch_size, seq_len_q, d_model] View Source def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ) : \" \"\" Args: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\" \" batch_size = tf . shape ( query ) [ 0 ] # (batch_size, num_heads, seq_len_q, depth) query = self . split_heads ( self . query ( query ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) key = self . split_heads ( self . key ( key ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) value = self . split_heads ( self . value ( value ), batch_size ) # scaled dot product attention # (batch_size, nh, seq_len_q, depth) x (batch_size, nh, depth, seq_len_k) # = (batch_size, nh, seq_len_q, seq_len_k) matmul_qk = tf . matmul ( query , key , transpose_b = True ) # Here we normalize by depth_k suppose K and Q are two matrices # with mean=0 and var=1. After QK^T will have a matrix with # mean=0 and var= 1 * depth_k. QK^T/sqrt(depth_k) => mean=0 and var=1 scaled_attention_logits = matmul_qk / tf . math . sqrt ( tf . cast ( self . depth , self . compute_dtype )) if attn_mask is not None : scaled_attention_logits += attn_mask if key_padding_mask is not None : # Apply -inf if the pixels is a padding # False means padded so we take: not key_padding_mask scaled_attention_logits = tf . where ( ~ key_padding_mask [ : , None , None ] , tf . zeros_like ( scaled_attention_logits ) + float ( '-inf' ), scaled_attention_logits ) # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. # (..., seq_len_q, seq_len_k) attention_weights = self . softmax ( scaled_attention_logits ) attention_weights = self . dropout ( attention_weights , training = training ) scaled_attention = tf . matmul ( attention_weights , value ) # (batch_size, seq_len_q, nh, depth) scaled_attention = tf . transpose ( scaled_attention , perm = [ 0 , 2 , 1 , 3 ] ) # (batch_size, seq_len_q, d_model) concat_attention = tf . reshape ( scaled_attention , ( batch_size , - 1 , self . d_model )) return self . dense ( concat_attention ) split_heads def split_heads ( self , tgt : tensorflow . python . framework . ops . Tensor , batch_size : int ) Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) View Source def split_heads ( self , tgt : tf . Tensor , batch_size : int ) : \"\"\" Split the last dimension into (num_heads, depth). Transpose the result such that the shape is ( batch_size , num_heads , seq_len , depth ) \"\"\" tgt = tf . reshape ( tgt , ( batch_size , - 1 , self . num_heads , self . depth )) return tf . transpose ( tgt , perm = [ 0 , 2 , 1 , 3 ] )","title":"Attentions"},{"location":"reference/kerod/layers/attentions/#module-kerodlayersattentions","text":"None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class MultiHeadAttention ( tf . keras . layers . Layer ): \"\"\"Allows the model to jointly attend to information from different representation subspaces. See reference: [Attention Is All You Need](https://arxiv.org/abs/1706.03762) Arguments: d_model: The number of expected features in the decoder inputs num_heads: The number of heads in the multiheadattention models. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. attention_axes: axes over which the attention is applied. `None` means attention over all axes, but batch, heads, and features. Call arguments: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Call returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dropout_rate = 0. , attention_axes =- 1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . d_model = d_model assert d_model % self . num_heads == 0 self . depth = d_model // self . num_heads self . query = tf . keras . layers . Dense ( d_model ) self . key = tf . keras . layers . Dense ( d_model ) self . value = tf . keras . layers . Dense ( d_model ) self . dense = tf . keras . layers . Dense ( d_model ) self . dropout = tf . keras . layers . Dropout ( dropout_rate ) self . softmax = tf . keras . layers . Softmax ( axis = attention_axes ) def split_heads ( self , tgt : tf . Tensor , batch_size : int ): \"\"\"Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) \"\"\" tgt = tf . reshape ( tgt , ( batch_size , - 1 , self . num_heads , self . depth )) return tf . transpose ( tgt , perm = [ 0 , 2 , 1 , 3 ]) def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ): \"\"\" Args: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\"\" batch_size = tf . shape ( query )[ 0 ] # (batch_size, num_heads, seq_len_q, depth) query = self . split_heads ( self . query ( query ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) key = self . split_heads ( self . key ( key ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) value = self . split_heads ( self . value ( value ), batch_size ) # scaled dot product attention # (batch_size, nh, seq_len_q, depth) x (batch_size, nh, depth, seq_len_k) # = (batch_size, nh, seq_len_q, seq_len_k) matmul_qk = tf . matmul ( query , key , transpose_b = True ) # Here we normalize by depth_k suppose K and Q are two matrices # with mean=0 and var=1. After QK^T will have a matrix with # mean=0 and var= 1 * depth_k. QK^T/sqrt(depth_k) => mean=0 and var=1 scaled_attention_logits = matmul_qk / tf . math . sqrt ( tf . cast ( self . depth , self . compute_dtype )) if attn_mask is not None : scaled_attention_logits += attn_mask if key_padding_mask is not None : # Apply -inf if the pixels is a padding # False means padded so we take: not key_padding_mask scaled_attention_logits = tf . where ( ~ key_padding_mask [:, None , None ], tf . zeros_like ( scaled_attention_logits ) + float ( '-inf' ), scaled_attention_logits ) # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. # (..., seq_len_q, seq_len_k) attention_weights = self . softmax ( scaled_attention_logits ) attention_weights = self . dropout ( attention_weights , training = training ) scaled_attention = tf . matmul ( attention_weights , value ) # (batch_size, seq_len_q, nh, depth) scaled_attention = tf . transpose ( scaled_attention , perm = [ 0 , 2 , 1 , 3 ]) # (batch_size, seq_len_q, d_model) concat_attention = tf . reshape ( scaled_attention , ( batch_size , - 1 , self . d_model )) return self . dense ( concat_attention ) remove_unwanted_doc ( MultiHeadAttention , __pdoc__ )","title":"Module kerod.layers.attentions"},{"location":"reference/kerod/layers/attentions/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/attentions/#multiheadattention","text":"class MultiHeadAttention ( d_model : int , num_heads : int , dropout_rate = 0.0 , attention_axes =- 1 , ** kwargs ) See reference: Attention Is All You Need","title":"MultiHeadAttention"},{"location":"reference/kerod/layers/attentions/#arguments","text":"Name Description d_model The number of expected features in the decoder inputs num_heads The number of heads in the multiheadattention models. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. attention_axes axes over which the attention is applied. None means attention over all axes, but batch, heads, and features.","title":"Arguments"},{"location":"reference/kerod/layers/attentions/#call-arguments","text":"Name Description value A 3-D tensor of shape [batch_size, seq_len, depth_v] key A 3-D tensor of shape [batch_size, seq_len, depth] query A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight.","title":"Call arguments"},{"location":"reference/kerod/layers/attentions/#call-returns","text":"Type Description tf.Tensor A 3-D tensor of shape [batch_size, seq_len_q, d_model]","title":"Call returns"},{"location":"reference/kerod/layers/attentions/#ancestors-in-mro","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/attentions/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/attentions/#call","text":"def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ) Parameters: Name Description value A 3-D tensor of shape [batch_size, seq_len, depth_v] key A 3-D tensor of shape [batch_size, seq_len, depth] query A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. attn_mask A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: Type Description tf.Tensor A 3-D tensor of shape [batch_size, seq_len_q, d_model] View Source def call ( self , value , key , query , key_padding_mask = None , attn_mask = None , training = None ) : \" \"\" Args: value: A 3-D tensor of shape [batch_size, seq_len, depth_v] key: A 3-D tensor of shape [batch_size, seq_len, depth] query: A 3-D tensor of shape [batch_size, seq_len_q, depth] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. attn_mask: A 4-D float tensor of shape [batch_size, num_heads, seq_len_q, seq_len]. If provided, it will be added to the attention weight. Returns: tf.Tensor: A 3-D tensor of shape [batch_size, seq_len_q, d_model] \"\" \" batch_size = tf . shape ( query ) [ 0 ] # (batch_size, num_heads, seq_len_q, depth) query = self . split_heads ( self . query ( query ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) key = self . split_heads ( self . key ( key ), batch_size ) # (batch_size, num_heads, seq_len_k, depth) value = self . split_heads ( self . value ( value ), batch_size ) # scaled dot product attention # (batch_size, nh, seq_len_q, depth) x (batch_size, nh, depth, seq_len_k) # = (batch_size, nh, seq_len_q, seq_len_k) matmul_qk = tf . matmul ( query , key , transpose_b = True ) # Here we normalize by depth_k suppose K and Q are two matrices # with mean=0 and var=1. After QK^T will have a matrix with # mean=0 and var= 1 * depth_k. QK^T/sqrt(depth_k) => mean=0 and var=1 scaled_attention_logits = matmul_qk / tf . math . sqrt ( tf . cast ( self . depth , self . compute_dtype )) if attn_mask is not None : scaled_attention_logits += attn_mask if key_padding_mask is not None : # Apply -inf if the pixels is a padding # False means padded so we take: not key_padding_mask scaled_attention_logits = tf . where ( ~ key_padding_mask [ : , None , None ] , tf . zeros_like ( scaled_attention_logits ) + float ( '-inf' ), scaled_attention_logits ) # softmax is normalized on the last axis (seq_len_k) so that the scores # add up to 1. # (..., seq_len_q, seq_len_k) attention_weights = self . softmax ( scaled_attention_logits ) attention_weights = self . dropout ( attention_weights , training = training ) scaled_attention = tf . matmul ( attention_weights , value ) # (batch_size, seq_len_q, nh, depth) scaled_attention = tf . transpose ( scaled_attention , perm = [ 0 , 2 , 1 , 3 ] ) # (batch_size, seq_len_q, d_model) concat_attention = tf . reshape ( scaled_attention , ( batch_size , - 1 , self . d_model )) return self . dense ( concat_attention )","title":"call"},{"location":"reference/kerod/layers/attentions/#split_heads","text":"def split_heads ( self , tgt : tensorflow . python . framework . ops . Tensor , batch_size : int ) Split the last dimension into (num_heads, depth). Transpose the result such that the shape is (batch_size, num_heads, seq_len, depth) View Source def split_heads ( self , tgt : tf . Tensor , batch_size : int ) : \"\"\" Split the last dimension into (num_heads, depth). Transpose the result such that the shape is ( batch_size , num_heads , seq_len , depth ) \"\"\" tgt = tf . reshape ( tgt , ( batch_size , - 1 , self . num_heads , self . depth )) return tf . transpose ( tgt , perm = [ 0 , 2 , 1 , 3 ] )","title":"split_heads"},{"location":"reference/kerod/layers/patches/","text":"Module kerod.layers.patches None None View Source import tensorflow as tf class Patches ( tf . keras . layers . Layer ): \"\"\"Extract `patches` from `images`. This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output. Argument: patch_size: Inputs: images: A 3-D tensor of shape [batch_size, height, width, nb_channel] Output: patches: A 3-D tensor of shape [batch_size, ] \"\"\" def __init__ ( self , patch_size : int ): super () . __init__ () self . patch_size = patch_size def call ( self , images ): batch_size = tf . shape ( images )[ 0 ] patches = tf . image . extract_patches ( images = images , sizes = [ 1 , self . patch_size , self . patch_size , 1 ], strides = [ 1 , self . patch_size , self . patch_size , 1 ], rates = [ 1 , 1 , 1 , 1 ], padding = \"VALID\" , ) patch_dims = patches . shape [ - 1 ] patches = tf . reshape ( patches , [ batch_size , - 1 , patch_dims ]) return patches Classes Patches class Patches ( patch_size : int ) This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output. Arguments Name Description patch_size Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Static methods from_config def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Description config A Python dictionary, typically the output of get_config. Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config ) with_name_scope def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Description method The method to wrap. Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope ) Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) build def build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Description input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). View Source @trackable.no_automatic_dependency_tracking @generic_utils.default def build ( self , input_shape ) : \" \"\" Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). \"\" \" # Only record the build input shapes of overridden build methods. if not hasattr ( self . build , '_is_default' ) : self . _build_input_shape = input_shape self . built = True call def call ( self , images ) View Source def call ( self , images ) : batch_size = tf . shape ( images ) [ 0 ] patches = tf . image . extract_patches ( images = images , sizes = [ 1 , self . patch_size , self . patch_size , 1 ], strides = [ 1 , self . patch_size , self . patch_size , 1 ], rates = [ 1 , 1 , 1 , 1 ], padding = \" VALID \" , ) patch_dims = patches . shape [ - 1 ] patches = tf . reshape ( patches , [ batch_size , - 1 , patch_dims ] ) return patches compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) get_config def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Returns: Python dictionary. \"\" \" all_args = tf_inspect . getfullargspec ( self . __init__ ). args config = { 'name' : self . name , 'trainable' : self . trainable , } if hasattr ( self , '_batch_input_shape' ) : config [ 'batch_input_shape' ] = self . _batch_input_shape config [ 'dtype' ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , 'dynamic' ) : # Only include `dynamic` in the `config` if it is `True` if self . dynamic : config [ 'dynamic' ] = self . dynamic elif 'dynamic' in all_args : all_args . remove ( 'dynamic' ) expected_args = config . keys () # Finds all arguments in the `__init__` that are not in the config: extra_args = [ arg for arg in all_args if arg not in expected_args ] # Check that either the only argument in the `__init__` is `self`, # or that `get_config` has been overridden: if len ( extra_args ) > 1 and hasattr ( self . get_config , '_is_default' ) : raise NotImplementedError ( 'Layer %s has arguments in `__init__` and ' 'therefore must override `get_config`.' % self . __class__ . __name__ ) return config get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates get_weights def get_weights ( self ) Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of numpy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"Patches"},{"location":"reference/kerod/layers/patches/#module-kerodlayerspatches","text":"None None View Source import tensorflow as tf class Patches ( tf . keras . layers . Layer ): \"\"\"Extract `patches` from `images`. This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output. Argument: patch_size: Inputs: images: A 3-D tensor of shape [batch_size, height, width, nb_channel] Output: patches: A 3-D tensor of shape [batch_size, ] \"\"\" def __init__ ( self , patch_size : int ): super () . __init__ () self . patch_size = patch_size def call ( self , images ): batch_size = tf . shape ( images )[ 0 ] patches = tf . image . extract_patches ( images = images , sizes = [ 1 , self . patch_size , self . patch_size , 1 ], strides = [ 1 , self . patch_size , self . patch_size , 1 ], rates = [ 1 , 1 , 1 , 1 ], padding = \"VALID\" , ) patch_dims = patches . shape [ - 1 ] patches = tf . reshape ( patches , [ batch_size , - 1 , patch_dims ]) return patches","title":"Module kerod.layers.patches"},{"location":"reference/kerod/layers/patches/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/patches/#patches","text":"class Patches ( patch_size : int ) This op collects patches from the input image, as if applying a convolution. All extracted patches are stacked in the depth (last) dimension of the output.","title":"Patches"},{"location":"reference/kerod/layers/patches/#arguments","text":"Name Description patch_size","title":"Arguments"},{"location":"reference/kerod/layers/patches/#ancestors-in-mro","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/patches/#static-methods","text":"","title":"Static methods"},{"location":"reference/kerod/layers/patches/#from_config","text":"def from_config ( config ) Creates a layer from its config. This method is the reverse of get_config , capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by set_weights ). Parameters: Name Description config A Python dictionary, typically the output of get_config. Returns: Type Description None A layer instance. View Source @classmethod def from_config ( cls , config ) : \" \"\" Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. \"\" \" return cls ( ** config )","title":"from_config"},{"location":"reference/kerod/layers/patches/#with_name_scope","text":"def with_name_scope ( method ) Decorator to automatically enter the module name scope. class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def call (self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce tf.Variable s and tf.Tensor s whose names included the module name: mod = MyModule() mod(tf.ones([1, 2])) mod.w Parameters: Name Description method The method to wrap. Returns: Type Description None The original method wrapped such that it enters the module's name scope. View Source @classmethod def with_name_scope ( cls , method ) : \"\"\"Decorator to automatically enter the module name scope. >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. \"\"\" def method_with_name_scope ( self , * args , ** kwargs ) : with self . name_scope : return method ( self , * args , ** kwargs ) return tf_decorator . make_decorator ( method , method_with_name_scope )","title":"with_name_scope"},{"location":"reference/kerod/layers/patches/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/patches/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/layers/patches/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/layers/patches/#add_update","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/layers/patches/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/layers/patches/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/layers/patches/#apply","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/layers/patches/#build","text":"def build ( self , input_shape ) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of Layer or Model can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of Layer subclasses. Parameters: Name Description input_shape Instance of TensorShape , or list of instances of TensorShape if the layer expects a list of inputs (one instance per input). View Source @trackable.no_automatic_dependency_tracking @generic_utils.default def build ( self , input_shape ) : \" \"\" Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). \"\" \" # Only record the build input shapes of overridden build methods. if not hasattr ( self . build , '_is_default' ) : self . _build_input_shape = input_shape self . built = True","title":"build"},{"location":"reference/kerod/layers/patches/#call","text":"def call ( self , images ) View Source def call ( self , images ) : batch_size = tf . shape ( images ) [ 0 ] patches = tf . image . extract_patches ( images = images , sizes = [ 1 , self . patch_size , self . patch_size , 1 ], strides = [ 1 , self . patch_size , self . patch_size , 1 ], rates = [ 1 , 1 , 1 , 1 ], padding = \" VALID \" , ) patch_dims = patches . shape [ - 1 ] patches = tf . reshape ( patches , [ batch_size , - 1 , patch_dims ] ) return patches","title":"call"},{"location":"reference/kerod/layers/patches/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/layers/patches/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/layers/patches/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/layers/patches/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/layers/patches/#get_config","text":"def get_config ( self ) Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by Network (one layer of abstraction above). Returns: Type Description None Python dictionary. View Source @generic_utils.default def get_config ( self ) : \" \"\" Returns the config of the layer. A layer config is a Python dictionary (serializable) containing the configuration of a layer. The same layer can be reinstantiated later (without its trained weights) from this configuration. The config of a layer does not include connectivity information, nor the layer class name. These are handled by `Network` (one layer of abstraction above). Returns: Python dictionary. \"\" \" all_args = tf_inspect . getfullargspec ( self . __init__ ). args config = { 'name' : self . name , 'trainable' : self . trainable , } if hasattr ( self , '_batch_input_shape' ) : config [ 'batch_input_shape' ] = self . _batch_input_shape config [ 'dtype' ] = policy . serialize ( self . _dtype_policy ) if hasattr ( self , 'dynamic' ) : # Only include `dynamic` in the `config` if it is `True` if self . dynamic : config [ 'dynamic' ] = self . dynamic elif 'dynamic' in all_args : all_args . remove ( 'dynamic' ) expected_args = config . keys () # Finds all arguments in the `__init__` that are not in the config: extra_args = [ arg for arg in all_args if arg not in expected_args ] # Check that either the only argument in the `__init__` is `self`, # or that `get_config` has been overridden: if len ( extra_args ) > 1 and hasattr ( self . get_config , '_is_default' ) : raise NotImplementedError ( 'Layer %s has arguments in `__init__` and ' 'therefore must override `get_config`.' % self . __class__ . __name__ ) return config","title":"get_config"},{"location":"reference/kerod/layers/patches/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/layers/patches/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/layers/patches/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/layers/patches/#get_losses_for","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/layers/patches/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/layers/patches/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/layers/patches/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/layers/patches/#get_updates_for","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/layers/patches/#get_weights","text":"def get_weights ( self ) Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Type Description None Weights values as a list of numpy arrays. View Source def get_weights ( self ): \"\"\"Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. \"\"\" weights = self . weights output_weights = [] for weight in weights : if isinstance ( weight , base_layer_utils . TrackableWeightHandler ): output_weights . extend ( weight . get_tensors ()) else : output_weights . append ( weight ) return backend . batch_get_value ( output_weights )","title":"get_weights"},{"location":"reference/kerod/layers/patches/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/layers/positional_encoding/","text":"Module kerod.layers.positional_encoding None None View Source import math import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class PositionEmbeddingLearned ( tf . keras . layers . Layer ): \"\"\"Absolute pos embedding, learned. Arguments: output_dim: Dimension of the dense embedding. Call arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Call returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" def __init__ ( self , output_dim = 512 , ** kwargs ): super () . __init__ ( ** kwargs ) if output_dim % 2 != 0 : raise ValueError ( \"x an y embedding will be concatened to form a single vector \" f \"of shape output_dim. Please use a multiple of 2 (e.g { output_dim } )\" ) self . dim = int ( output_dim / 2 ) self . row_embed = tf . keras . layers . Embedding ( 50 , self . dim ) self . col_embed = tf . keras . layers . Embedding ( 50 , self . dim ) def call ( self , inputs ): \"\"\"Based on the shape of the input tensor return a positional embedding. Arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" batch_size , h , w = tf . shape ( inputs )[ 0 ], tf . shape ( inputs )[ 1 ], tf . shape ( inputs )[ 2 ] i = tf . range ( w ) j = tf . range ( h ) x_emb = self . col_embed ( i ) y_emb = self . row_embed ( j ) single_img_emb = tf . concat ([ tf . tile ( x_emb [ None ], ( h , 1 , 1 )), tf . tile ( y_emb [:, None ], ( 1 , w , 1 )), ], axis =- 1 ) batch_emb = tf . tile ( single_img_emb [ None ], ( batch_size , 1 , 1 , 1 )) return batch_emb class PositionEmbeddingSine ( tf . keras . layers . Layer ): \"\"\" This is a more standard version of the position embedding, very similar to the one used by the Attention is all you need paper, generalized to work on images. ```python import matplotlib.pyplot as plt from kerod.layers.positional_encoding import PositionEmbeddingSine dim = 128 embedding = PositionEmbeddingSine(dim) pos_encoding = embedding(tf.ones((1, 10, 10))) plt.pcolormesh(tf.reshape(pos_encoding, (1, -1, dim))[0], cmap='RdBu') plt.xlabel('Depth') plt.xlim((0, dim)) plt.ylabel('Position') plt.colorbar() plt.show() ``` ![Visualization Positional Encoding](https://raw.githubusercontent.com/EmGarr/kerod/master/docs/img/2d_pos_encoding.png) Arguments: output_dim: Dimension of the dense embedding. Call arguments: masks: A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Call returns: tf.Tensor: The encoding a tensor of float and shape [batch_size, w, h, output_dim] \"\"\" def __init__ ( self , output_dim = 64 , temperature = 10000 ): super () . __init__ () self . temperature = temperature self . scale = 2 * math . pi if output_dim % 2 != 0 : raise ValueError ( \"x an y embedding will be concatened to form a single vector \" f \"of shape output_dim. Please use a multiple of 2 (e.g { output_dim } )\" ) self . dim = int ( output_dim / 2 ) dim_t = tf . range ( self . dim , dtype = tf . float32 ) self . dim_t = self . temperature ** ( 2 * ( dim_t // 2 ) / self . dim ) def call ( self , masks ): \"\"\"From a masks tensor compute the positional encoding Arguments: masks: A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Returns: tf.Tensor: The encoding a tensor of float and shape [batch_size, w, h, output_dim] \"\"\" masks = tf . cast ( masks , self . compute_dtype ) y_embed = tf . math . cumsum ( masks , axis = 1 ) x_embed = tf . math . cumsum ( masks , axis = 2 ) # Normalize x_embed and y_embed by the maximum values of the cumsum eps = 1e-6 y_embed = y_embed / ( y_embed [:, - 1 :, :] + eps ) * self . scale x_embed = x_embed / ( x_embed [:, :, - 1 :] + eps ) * self . scale pos_x = x_embed [ ... , None ] / self . dim_t pos_y = y_embed [ ... , None ] / self . dim_t pos_x = tf . stack ([ tf . math . sin ( pos_x [ ... , 0 :: 2 ]), tf . math . cos ( pos_x [ ... , 1 :: 2 ]), ], axis = 4 ) pos_y = tf . stack ([ tf . math . sin ( pos_y [ ... , 0 :: 2 ]), tf . math . cos ( pos_y [ ... , 1 :: 2 ]), ], axis = 4 ) batch_size , h , w = tf . shape ( masks )[ 0 ], tf . shape ( masks )[ 1 ], tf . shape ( masks )[ 2 ] pos_x = tf . reshape ( pos_x , ( batch_size , h , w , - 1 )) pos_y = tf . reshape ( pos_y , ( batch_size , h , w , - 1 )) pos_emb = tf . concat ([ pos_y , pos_x ], axis =- 1 ) return pos_emb remove_unwanted_doc ( PositionEmbeddingLearned , __pdoc__ ) remove_unwanted_doc ( PositionEmbeddingSine , __pdoc__ ) Classes PositionEmbeddingLearned class PositionEmbeddingLearned ( output_dim = 512 , ** kwargs ) Arguments Name Description output_dim Dimension of the dense embedding. Call arguments Name Description inputs A 4-D Tensor of shape [batch_size, h, w, channel] Call returns Type Description tf.Tensor The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , inputs ) Based on the shape of the input tensor return a positional embedding. Parameters: Name Description inputs A 4-D Tensor of shape [batch_size, h, w, channel] Returns: Type Description tf.Tensor The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] View Source def call ( self , inputs ) : \"\"\"Based on the shape of the input tensor return a positional embedding. Arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" batch_size , h , w = tf . shape ( inputs ) [ 0 ] , tf . shape ( inputs ) [ 1 ] , tf . shape ( inputs ) [ 2 ] i = tf . range ( w ) j = tf . range ( h ) x_emb = self . col_embed ( i ) y_emb = self . row_embed ( j ) single_img_emb = tf . concat ( [ tf.tile(x_emb[None ] , ( h , 1 , 1 )), tf . tile ( y_emb [ :, None ] , ( 1 , w , 1 )), ] , axis =- 1 ) batch_emb = tf . tile ( single_img_emb [ None ] , ( batch_size , 1 , 1 , 1 )) return batch_emb PositionEmbeddingSine class PositionEmbeddingSine ( output_dim = 64 , temperature = 10000 ) used by the Attention is all you need paper, generalized to work on images. import matplotlib.pyplot as plt from kerod.layers.positional_encoding import PositionEmbeddingSine dim = 128 embedding = PositionEmbeddingSine ( dim ) pos_encoding = embedding ( tf . ones (( 1 , 10 , 10 ))) plt . pcolormesh ( tf . reshape ( pos_encoding , ( 1 , - 1 , dim ))[ 0 ], cmap = 'RdBu' ) plt . xlabel ( 'Depth' ) plt . xlim (( 0 , dim )) plt . ylabel ( 'Position' ) plt . colorbar () plt . show () Arguments Name Description output_dim Dimension of the dense embedding. Call arguments Name Description masks A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Call returns Type Description tf.Tensor The encoding a tensor of float and shape [batch_size, w, h, output_dim] Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , masks ) From a masks tensor compute the positional encoding Parameters: Name Description masks A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Returns: Type Description tf.Tensor The encoding a tensor of float and shape [batch_size, w, h, output_dim] View Source def call ( self , masks ) : \"\"\" From a masks tensor compute the positional encoding Arguments : masks : A tensor of bool and shape [ batch_size , w , h ] where False means padding and True pixel from the image Returns : tf . Tensor : The encoding a tensor of float and shape [ batch_size , w , h , output_dim ] \"\"\" masks = tf . cast ( masks , self . compute_dtype ) y_embed = tf . math . cumsum ( masks , axis = 1 ) x_embed = tf . math . cumsum ( masks , axis = 2 ) # Normalize x_embed and y_embed by the maximum values of the cumsum eps = 1 e - 6 y_embed = y_embed / ( y_embed [:, - 1 :, :] + eps ) * self . scale x_embed = x_embed / ( x_embed [:, :, - 1 :] + eps ) * self . scale pos_x = x_embed [..., None ] / self . dim_t pos_y = y_embed [..., None ] / self . dim_t pos_x = tf . stack ( [ tf . math . sin ( pos_x [..., 0 :: 2 ] ) , tf . math . cos ( pos_x [..., 1 :: 2 ] ) , ], axis = 4 ) pos_y = tf . stack ( [ tf . math . sin ( pos_y [..., 0 :: 2 ] ) , tf . math . cos ( pos_y [..., 1 :: 2 ] ) , ], axis = 4 ) batch_size , h , w = tf . shape ( masks ) [ 0 ], tf . shape ( masks ) [ 1 ], tf . shape ( masks ) [ 2 ] pos_x = tf . reshape ( pos_x , ( batch_size , h , w , - 1 )) pos_y = tf . reshape ( pos_y , ( batch_size , h , w , - 1 )) pos_emb = tf . concat ( [ pos_y , pos_x ], axis =- 1 ) return pos_emb","title":"Positional Encoding"},{"location":"reference/kerod/layers/positional_encoding/#module-kerodlayerspositional_encoding","text":"None None View Source import math import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class PositionEmbeddingLearned ( tf . keras . layers . Layer ): \"\"\"Absolute pos embedding, learned. Arguments: output_dim: Dimension of the dense embedding. Call arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Call returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" def __init__ ( self , output_dim = 512 , ** kwargs ): super () . __init__ ( ** kwargs ) if output_dim % 2 != 0 : raise ValueError ( \"x an y embedding will be concatened to form a single vector \" f \"of shape output_dim. Please use a multiple of 2 (e.g { output_dim } )\" ) self . dim = int ( output_dim / 2 ) self . row_embed = tf . keras . layers . Embedding ( 50 , self . dim ) self . col_embed = tf . keras . layers . Embedding ( 50 , self . dim ) def call ( self , inputs ): \"\"\"Based on the shape of the input tensor return a positional embedding. Arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" batch_size , h , w = tf . shape ( inputs )[ 0 ], tf . shape ( inputs )[ 1 ], tf . shape ( inputs )[ 2 ] i = tf . range ( w ) j = tf . range ( h ) x_emb = self . col_embed ( i ) y_emb = self . row_embed ( j ) single_img_emb = tf . concat ([ tf . tile ( x_emb [ None ], ( h , 1 , 1 )), tf . tile ( y_emb [:, None ], ( 1 , w , 1 )), ], axis =- 1 ) batch_emb = tf . tile ( single_img_emb [ None ], ( batch_size , 1 , 1 , 1 )) return batch_emb class PositionEmbeddingSine ( tf . keras . layers . Layer ): \"\"\" This is a more standard version of the position embedding, very similar to the one used by the Attention is all you need paper, generalized to work on images. ```python import matplotlib.pyplot as plt from kerod.layers.positional_encoding import PositionEmbeddingSine dim = 128 embedding = PositionEmbeddingSine(dim) pos_encoding = embedding(tf.ones((1, 10, 10))) plt.pcolormesh(tf.reshape(pos_encoding, (1, -1, dim))[0], cmap='RdBu') plt.xlabel('Depth') plt.xlim((0, dim)) plt.ylabel('Position') plt.colorbar() plt.show() ``` ![Visualization Positional Encoding](https://raw.githubusercontent.com/EmGarr/kerod/master/docs/img/2d_pos_encoding.png) Arguments: output_dim: Dimension of the dense embedding. Call arguments: masks: A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Call returns: tf.Tensor: The encoding a tensor of float and shape [batch_size, w, h, output_dim] \"\"\" def __init__ ( self , output_dim = 64 , temperature = 10000 ): super () . __init__ () self . temperature = temperature self . scale = 2 * math . pi if output_dim % 2 != 0 : raise ValueError ( \"x an y embedding will be concatened to form a single vector \" f \"of shape output_dim. Please use a multiple of 2 (e.g { output_dim } )\" ) self . dim = int ( output_dim / 2 ) dim_t = tf . range ( self . dim , dtype = tf . float32 ) self . dim_t = self . temperature ** ( 2 * ( dim_t // 2 ) / self . dim ) def call ( self , masks ): \"\"\"From a masks tensor compute the positional encoding Arguments: masks: A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Returns: tf.Tensor: The encoding a tensor of float and shape [batch_size, w, h, output_dim] \"\"\" masks = tf . cast ( masks , self . compute_dtype ) y_embed = tf . math . cumsum ( masks , axis = 1 ) x_embed = tf . math . cumsum ( masks , axis = 2 ) # Normalize x_embed and y_embed by the maximum values of the cumsum eps = 1e-6 y_embed = y_embed / ( y_embed [:, - 1 :, :] + eps ) * self . scale x_embed = x_embed / ( x_embed [:, :, - 1 :] + eps ) * self . scale pos_x = x_embed [ ... , None ] / self . dim_t pos_y = y_embed [ ... , None ] / self . dim_t pos_x = tf . stack ([ tf . math . sin ( pos_x [ ... , 0 :: 2 ]), tf . math . cos ( pos_x [ ... , 1 :: 2 ]), ], axis = 4 ) pos_y = tf . stack ([ tf . math . sin ( pos_y [ ... , 0 :: 2 ]), tf . math . cos ( pos_y [ ... , 1 :: 2 ]), ], axis = 4 ) batch_size , h , w = tf . shape ( masks )[ 0 ], tf . shape ( masks )[ 1 ], tf . shape ( masks )[ 2 ] pos_x = tf . reshape ( pos_x , ( batch_size , h , w , - 1 )) pos_y = tf . reshape ( pos_y , ( batch_size , h , w , - 1 )) pos_emb = tf . concat ([ pos_y , pos_x ], axis =- 1 ) return pos_emb remove_unwanted_doc ( PositionEmbeddingLearned , __pdoc__ ) remove_unwanted_doc ( PositionEmbeddingSine , __pdoc__ )","title":"Module kerod.layers.positional_encoding"},{"location":"reference/kerod/layers/positional_encoding/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/positional_encoding/#positionembeddinglearned","text":"class PositionEmbeddingLearned ( output_dim = 512 , ** kwargs )","title":"PositionEmbeddingLearned"},{"location":"reference/kerod/layers/positional_encoding/#arguments","text":"Name Description output_dim Dimension of the dense embedding.","title":"Arguments"},{"location":"reference/kerod/layers/positional_encoding/#call-arguments","text":"Name Description inputs A 4-D Tensor of shape [batch_size, h, w, channel]","title":"Call arguments"},{"location":"reference/kerod/layers/positional_encoding/#call-returns","text":"Type Description tf.Tensor The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim]","title":"Call returns"},{"location":"reference/kerod/layers/positional_encoding/#ancestors-in-mro","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/positional_encoding/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/positional_encoding/#call","text":"def call ( self , inputs ) Based on the shape of the input tensor return a positional embedding. Parameters: Name Description inputs A 4-D Tensor of shape [batch_size, h, w, channel] Returns: Type Description tf.Tensor The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] View Source def call ( self , inputs ) : \"\"\"Based on the shape of the input tensor return a positional embedding. Arguments: inputs: A 4-D Tensor of shape [batch_size, h, w, channel] Returns: tf.Tensor: The positional embedding a 4-D Tensor of shape [batch_size, h, w, output_dim] \"\"\" batch_size , h , w = tf . shape ( inputs ) [ 0 ] , tf . shape ( inputs ) [ 1 ] , tf . shape ( inputs ) [ 2 ] i = tf . range ( w ) j = tf . range ( h ) x_emb = self . col_embed ( i ) y_emb = self . row_embed ( j ) single_img_emb = tf . concat ( [ tf.tile(x_emb[None ] , ( h , 1 , 1 )), tf . tile ( y_emb [ :, None ] , ( 1 , w , 1 )), ] , axis =- 1 ) batch_emb = tf . tile ( single_img_emb [ None ] , ( batch_size , 1 , 1 , 1 )) return batch_emb","title":"call"},{"location":"reference/kerod/layers/positional_encoding/#positionembeddingsine","text":"class PositionEmbeddingSine ( output_dim = 64 , temperature = 10000 ) used by the Attention is all you need paper, generalized to work on images. import matplotlib.pyplot as plt from kerod.layers.positional_encoding import PositionEmbeddingSine dim = 128 embedding = PositionEmbeddingSine ( dim ) pos_encoding = embedding ( tf . ones (( 1 , 10 , 10 ))) plt . pcolormesh ( tf . reshape ( pos_encoding , ( 1 , - 1 , dim ))[ 0 ], cmap = 'RdBu' ) plt . xlabel ( 'Depth' ) plt . xlim (( 0 , dim )) plt . ylabel ( 'Position' ) plt . colorbar () plt . show ()","title":"PositionEmbeddingSine"},{"location":"reference/kerod/layers/positional_encoding/#arguments_1","text":"Name Description output_dim Dimension of the dense embedding.","title":"Arguments"},{"location":"reference/kerod/layers/positional_encoding/#call-arguments_1","text":"Name Description masks A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image","title":"Call arguments"},{"location":"reference/kerod/layers/positional_encoding/#call-returns_1","text":"Type Description tf.Tensor The encoding a tensor of float and shape [batch_size, w, h, output_dim]","title":"Call returns"},{"location":"reference/kerod/layers/positional_encoding/#ancestors-in-mro_1","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/positional_encoding/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/layers/positional_encoding/#call_1","text":"def call ( self , masks ) From a masks tensor compute the positional encoding Parameters: Name Description masks A tensor of bool and shape [batch_size, w, h] where False means padding and True pixel from the image Returns: Type Description tf.Tensor The encoding a tensor of float and shape [batch_size, w, h, output_dim] View Source def call ( self , masks ) : \"\"\" From a masks tensor compute the positional encoding Arguments : masks : A tensor of bool and shape [ batch_size , w , h ] where False means padding and True pixel from the image Returns : tf . Tensor : The encoding a tensor of float and shape [ batch_size , w , h , output_dim ] \"\"\" masks = tf . cast ( masks , self . compute_dtype ) y_embed = tf . math . cumsum ( masks , axis = 1 ) x_embed = tf . math . cumsum ( masks , axis = 2 ) # Normalize x_embed and y_embed by the maximum values of the cumsum eps = 1 e - 6 y_embed = y_embed / ( y_embed [:, - 1 :, :] + eps ) * self . scale x_embed = x_embed / ( x_embed [:, :, - 1 :] + eps ) * self . scale pos_x = x_embed [..., None ] / self . dim_t pos_y = y_embed [..., None ] / self . dim_t pos_x = tf . stack ( [ tf . math . sin ( pos_x [..., 0 :: 2 ] ) , tf . math . cos ( pos_x [..., 1 :: 2 ] ) , ], axis = 4 ) pos_y = tf . stack ( [ tf . math . sin ( pos_y [..., 0 :: 2 ] ) , tf . math . cos ( pos_y [..., 1 :: 2 ] ) , ], axis = 4 ) batch_size , h , w = tf . shape ( masks ) [ 0 ], tf . shape ( masks ) [ 1 ], tf . shape ( masks ) [ 2 ] pos_x = tf . reshape ( pos_x , ( batch_size , h , w , - 1 )) pos_y = tf . reshape ( pos_y , ( batch_size , h , w , - 1 )) pos_emb = tf . concat ( [ pos_y , pos_x ], axis =- 1 ) return pos_emb","title":"call"},{"location":"reference/kerod/layers/transformer/","text":"Module kerod.layers.transformer None None View Source import tensorflow as tf from kerod.layers import MultiHeadAttention from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class EncoderLayer ( tf . keras . layers . Layer ): \"\"\"Will build a TransformerEncoderLayer according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) Arguments: d_model: The number of expected features in the encoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Call returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . mha = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( dim_feedforward , activation = 'relu' ), tf . keras . layers . Dropout ( dropout_rate ), tf . keras . layers . Dense ( d_model ) ]) self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . dropout1 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout2 = tf . keras . layers . Dropout ( dropout_rate ) def call ( self , src , pos_emb , key_padding_mask = None , training = None ): \"\"\"Forward of the EncoderLayer Args: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" x_pos_emb = src + pos_emb attn_output = self . mha ( src , x_pos_emb , x_pos_emb , key_padding_mask = key_padding_mask , training = training ) # (batch_size, input_seq_len, d_model) attn_output = self . dropout1 ( attn_output , training = training ) out1 = self . layernorm1 ( src + attn_output ) # (batch_size, input_seq_len, d_model) ffn_output = self . ffn ( out1 ) # (batch_size, input_seq_len, d_model) ffn_output = self . dropout2 ( ffn_output , training = training ) out2 = self . layernorm2 ( out1 + ffn_output ) # (batch_size, input_seq_len, d_model) return out2 class DecoderLayer ( tf . keras . layers . Layer ): \"\"\"Will build a TransformerDecoderLayer according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) Arguments: d_model: The number of expected features in the encoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the DecoderLayer. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn). Call returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . mha1 = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . mha2 = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( dim_feedforward , activation = 'relu' ), tf . keras . layers . Dropout ( dropout_rate ), tf . keras . layers . Dense ( d_model ) ]) self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm3 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . dropout1 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout2 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout3 = tf . keras . layers . Dropout ( dropout_rate ) def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ): \"\"\" Forward of the DecoderLayer Arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" tgt_object_queries = dec_out + object_queries # (batch_size, M, d_model) self_attn = self . mha1 ( dec_out , tgt_object_queries , tgt_object_queries , training = training ) self_attn = self . dropout1 ( self_attn , training = training ) self_attn = self . layernorm1 ( self_attn + dec_out ) # (batch_size, M, d_model) co_attn = self . mha2 ( memory , memory + pos_embed , self_attn + object_queries , key_padding_mask = key_padding_mask , attn_mask = coattn_mask , training = training ) co_attn = self . dropout2 ( co_attn , training = training ) co_attn = self . layernorm2 ( co_attn + self_attn ) # (batch_size, M, d_model) ffn_output = self . ffn ( co_attn ) # (batch_size, M, d_model) ffn_output = self . dropout3 ( ffn_output , training = training ) out3 = self . layernorm3 ( ffn_output + co_attn ) # (batch_size, M, d_model) return out3 class Transformer ( tf . keras . layers . Layer ): \"\"\"Will build a Transformer according to the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Args: num_layers: the number of sub-layers in the decoder and the encoder. d_model: The number of expected features in the encoder/decoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer and DecoderLayer dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Call returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\"\" def __init__ ( self , num_layers = 6 , d_model = 256 , num_heads = 8 , dim_feedforward = 2048 , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . layer_norm = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . enc_layers = [ EncoderLayer ( d_model , num_heads , dim_feedforward , dropout_rate ) for _ in range ( num_layers ) ] self . dec_layers = [ DecoderLayer ( d_model , num_heads , dim_feedforward , dropout_rate ) for _ in range ( num_layers ) ] def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ): \"\"\" Args: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\"\" memory = flatten_tensor for enc in self . enc_layers : # (batch_size, seq_len, d_model) memory = enc ( memory , pos_embed , key_padding_mask = key_padding_mask , training = training ) # At the beginning we set target to 0 # In the first decoder layer Q and K will be equal # to dec_out + object_queries=object_queries dec_out = tf . zeros_like ( object_queries ) layers_output = [] for layer in self . dec_layers : dec_out = layer ( dec_out , memory , pos_embed , object_queries , key_padding_mask = key_padding_mask , coattn_mask = coattn_mask , training = training ) dec_out = self . layer_norm ( dec_out ) if training : layers_output . append ( dec_out ) if training : return tf . concat ( layers_output , axis = 1 ), memory return dec_out , memory remove_unwanted_doc ( EncoderLayer , __pdoc__ ) remove_unwanted_doc ( DecoderLayer , __pdoc__ ) remove_unwanted_doc ( Transformer , __pdoc__ ) Classes DecoderLayer class DecoderLayer ( d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ) End-to-End Object Detection with Transformers Arguments Name Description d_model The number of expected features in the encoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward The dim of the feedforward neuralnetworks in the DecoderLayer. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments Name Description dec_out A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn). Call returns Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) Forward of the DecoderLayer Parameters: Name Description dec_out A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of True will be ignored while the position with the value of False will be unchanged. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] View Source def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) : \" \"\" Forward of the DecoderLayer Arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\" \" tgt_object_queries = dec_out + object_queries # (batch_size, M, d_model) self_attn = self . mha1 ( dec_out , tgt_object_queries , tgt_object_queries , training = training ) self_attn = self . dropout1 ( self_attn , training = training ) self_attn = self . layernorm1 ( self_attn + dec_out ) # (batch_size, M, d_model) co_attn = self . mha2 ( memory , memory + pos_embed , self_attn + object_queries , key_padding_mask = key_padding_mask , attn_mask = coattn_mask , training = training ) co_attn = self . dropout2 ( co_attn , training = training ) co_attn = self . layernorm2 ( co_attn + self_attn ) # (batch_size, M, d_model) ffn_output = self . ffn ( co_attn ) # (batch_size, M, d_model) ffn_output = self . dropout3 ( ffn_output , training = training ) out3 = self . layernorm3 ( ffn_output + co_attn ) # (batch_size, M, d_model) return out3 EncoderLayer class EncoderLayer ( d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ) End-to-End Object Detection with Transformers Arguments Name Description d_model The number of expected features in the encoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments Name Description src A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed A 3-D Tensor of float32 and shape [batch_size, N, dim] positional encoding of the encoder key_padding_mask [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Call returns Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , src , pos_emb , key_padding_mask = None , training = None ) Forward of the EncoderLayer Parameters: Name Description src A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] View Source def call ( self , src , pos_emb , key_padding_mask = None , training = None ) : \"\"\"Forward of the EncoderLayer Args: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" x_pos_emb = src + pos_emb attn_output = self . mha ( src , x_pos_emb , x_pos_emb , key_padding_mask = key_padding_mask , training = training ) # ( batch_size , input_seq_len , d_model ) attn_output = self . dropout1 ( attn_output , training = training ) out1 = self . layernorm1 ( src + attn_output ) # ( batch_size , input_seq_len , d_model ) ffn_output = self . ffn ( out1 ) # ( batch_size , input_seq_len , d_model ) ffn_output = self . dropout2 ( ffn_output , training = training ) out2 = self . layernorm2 ( out1 + ffn_output ) # ( batch_size , input_seq_len , d_model ) return out2 Transformer class Transformer ( num_layers = 6 , d_model = 256 , num_heads = 8 , dim_feedforward = 2048 , dropout_rate = 0.1 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . Arguments Name Description num_layers the number of sub-layers in the decoder and the encoder. d_model The number of expected features in the encoder/decoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward The dim of the feedforward neuralnetworks in the EncoderLayer and DecoderLayer dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments Name Description flatten_tensor A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Call returns Type Description Tuple - decoder_output : 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - encoder_output : 3-D float32 Tensor of shape [batch_size, batch_size, d_model] Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) Parameters: Name Description flatten_tensor A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Type Description Tuple - decoder_output : 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - encoder_output : 3-D float32 Tensor of shape [batch_size, batch_size, d_model] View Source def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) : \" \"\" Args: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\" \" memory = flatten_tensor for enc in self . enc_layers : # (batch_size, seq_len, d_model) memory = enc ( memory , pos_embed , key_padding_mask = key_padding_mask , training = training ) # At the beginning we set target to 0 # In the first decoder layer Q and K will be equal # to dec_out + object_queries=object_queries dec_out = tf . zeros_like ( object_queries ) layers_output = [] for layer in self . dec_layers : dec_out = layer ( dec_out , memory , pos_embed , object_queries , key_padding_mask = key_padding_mask , coattn_mask = coattn_mask , training = training ) dec_out = self . layer_norm ( dec_out ) if training : layers_output . append ( dec_out ) if training : return tf . concat ( layers_output , axis = 1 ), memory return dec_out , memory","title":"Transformer"},{"location":"reference/kerod/layers/transformer/#module-kerodlayerstransformer","text":"None None View Source import tensorflow as tf from kerod.layers import MultiHeadAttention from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class EncoderLayer ( tf . keras . layers . Layer ): \"\"\"Will build a TransformerEncoderLayer according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) Arguments: d_model: The number of expected features in the encoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Call returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . mha = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( dim_feedforward , activation = 'relu' ), tf . keras . layers . Dropout ( dropout_rate ), tf . keras . layers . Dense ( d_model ) ]) self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . dropout1 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout2 = tf . keras . layers . Dropout ( dropout_rate ) def call ( self , src , pos_emb , key_padding_mask = None , training = None ): \"\"\"Forward of the EncoderLayer Args: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" x_pos_emb = src + pos_emb attn_output = self . mha ( src , x_pos_emb , x_pos_emb , key_padding_mask = key_padding_mask , training = training ) # (batch_size, input_seq_len, d_model) attn_output = self . dropout1 ( attn_output , training = training ) out1 = self . layernorm1 ( src + attn_output ) # (batch_size, input_seq_len, d_model) ffn_output = self . ffn ( out1 ) # (batch_size, input_seq_len, d_model) ffn_output = self . dropout2 ( ffn_output , training = training ) out2 = self . layernorm2 ( out1 + ffn_output ) # (batch_size, input_seq_len, d_model) return out2 class DecoderLayer ( tf . keras . layers . Layer ): \"\"\"Will build a TransformerDecoderLayer according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) Arguments: d_model: The number of expected features in the encoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the DecoderLayer. dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn). Call returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" def __init__ ( self , d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . mha1 = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . mha2 = MultiHeadAttention ( d_model , num_heads , dropout_rate = dropout_rate ) self . ffn = tf . keras . Sequential ([ tf . keras . layers . Dense ( dim_feedforward , activation = 'relu' ), tf . keras . layers . Dropout ( dropout_rate ), tf . keras . layers . Dense ( d_model ) ]) self . layernorm1 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm2 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . layernorm3 = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . dropout1 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout2 = tf . keras . layers . Dropout ( dropout_rate ) self . dropout3 = tf . keras . layers . Dropout ( dropout_rate ) def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ): \"\"\" Forward of the DecoderLayer Arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" tgt_object_queries = dec_out + object_queries # (batch_size, M, d_model) self_attn = self . mha1 ( dec_out , tgt_object_queries , tgt_object_queries , training = training ) self_attn = self . dropout1 ( self_attn , training = training ) self_attn = self . layernorm1 ( self_attn + dec_out ) # (batch_size, M, d_model) co_attn = self . mha2 ( memory , memory + pos_embed , self_attn + object_queries , key_padding_mask = key_padding_mask , attn_mask = coattn_mask , training = training ) co_attn = self . dropout2 ( co_attn , training = training ) co_attn = self . layernorm2 ( co_attn + self_attn ) # (batch_size, M, d_model) ffn_output = self . ffn ( co_attn ) # (batch_size, M, d_model) ffn_output = self . dropout3 ( ffn_output , training = training ) out3 = self . layernorm3 ( ffn_output + co_attn ) # (batch_size, M, d_model) return out3 class Transformer ( tf . keras . layers . Layer ): \"\"\"Will build a Transformer according to the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Args: num_layers: the number of sub-layers in the decoder and the encoder. d_model: The number of expected features in the encoder/decoder inputs num_heads: The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer and DecoderLayer dropout_rate: Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer. Call arguments: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Call returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\"\" def __init__ ( self , num_layers = 6 , d_model = 256 , num_heads = 8 , dim_feedforward = 2048 , dropout_rate = 0.1 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . layer_norm = tf . keras . layers . LayerNormalization ( epsilon = 1e-5 ) self . enc_layers = [ EncoderLayer ( d_model , num_heads , dim_feedforward , dropout_rate ) for _ in range ( num_layers ) ] self . dec_layers = [ DecoderLayer ( d_model , num_heads , dim_feedforward , dropout_rate ) for _ in range ( num_layers ) ] def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ): \"\"\" Args: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\"\" memory = flatten_tensor for enc in self . enc_layers : # (batch_size, seq_len, d_model) memory = enc ( memory , pos_embed , key_padding_mask = key_padding_mask , training = training ) # At the beginning we set target to 0 # In the first decoder layer Q and K will be equal # to dec_out + object_queries=object_queries dec_out = tf . zeros_like ( object_queries ) layers_output = [] for layer in self . dec_layers : dec_out = layer ( dec_out , memory , pos_embed , object_queries , key_padding_mask = key_padding_mask , coattn_mask = coattn_mask , training = training ) dec_out = self . layer_norm ( dec_out ) if training : layers_output . append ( dec_out ) if training : return tf . concat ( layers_output , axis = 1 ), memory return dec_out , memory remove_unwanted_doc ( EncoderLayer , __pdoc__ ) remove_unwanted_doc ( DecoderLayer , __pdoc__ ) remove_unwanted_doc ( Transformer , __pdoc__ )","title":"Module kerod.layers.transformer"},{"location":"reference/kerod/layers/transformer/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/transformer/#decoderlayer","text":"class DecoderLayer ( d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ) End-to-End Object Detection with Transformers","title":"DecoderLayer"},{"location":"reference/kerod/layers/transformer/#arguments","text":"Name Description d_model The number of expected features in the encoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward The dim of the feedforward neuralnetworks in the DecoderLayer. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer.","title":"Arguments"},{"location":"reference/kerod/layers/transformer/#call-arguments","text":"Name Description dec_out A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. The positions with the value of True will be ignored while the position with the value of False will be unchanged. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn).","title":"Call arguments"},{"location":"reference/kerod/layers/transformer/#call-returns","text":"Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model]","title":"Call returns"},{"location":"reference/kerod/layers/transformer/#ancestors-in-mro","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/transformer/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/transformer/#call","text":"def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) Forward of the DecoderLayer Parameters: Name Description dec_out A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of True will be ignored while the position with the value of False will be unchanged. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] View Source def call ( self , dec_out , memory , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) : \" \"\" Forward of the DecoderLayer Arguments: dec_out: A 3-D Tensor of float32 and shape [batch_size, M, d_model] the sequence of the decoder memory: A 3-D Tensor of float32 and shape [batch_size, N, d_model] the sequence from the last layer of the encoder (memory) pos_embed: A 3-D Tensor of float32 and shape [batch_size, N, d_model] positional encoding of the encoder object_queries: A 3-D Tensor of float32 and shape [batch_size, M, d_model] key_padding_mask: A 2-D bool Tensor of shape [batch_size, seq_len]. the positions with the value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, M, N, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\" \" tgt_object_queries = dec_out + object_queries # (batch_size, M, d_model) self_attn = self . mha1 ( dec_out , tgt_object_queries , tgt_object_queries , training = training ) self_attn = self . dropout1 ( self_attn , training = training ) self_attn = self . layernorm1 ( self_attn + dec_out ) # (batch_size, M, d_model) co_attn = self . mha2 ( memory , memory + pos_embed , self_attn + object_queries , key_padding_mask = key_padding_mask , attn_mask = coattn_mask , training = training ) co_attn = self . dropout2 ( co_attn , training = training ) co_attn = self . layernorm2 ( co_attn + self_attn ) # (batch_size, M, d_model) ffn_output = self . ffn ( co_attn ) # (batch_size, M, d_model) ffn_output = self . dropout3 ( ffn_output , training = training ) out3 = self . layernorm3 ( ffn_output + co_attn ) # (batch_size, M, d_model) return out3","title":"call"},{"location":"reference/kerod/layers/transformer/#encoderlayer","text":"class EncoderLayer ( d_model : int , num_heads : int , dim_feedforward : int , dropout_rate = 0.1 , ** kwargs ) End-to-End Object Detection with Transformers","title":"EncoderLayer"},{"location":"reference/kerod/layers/transformer/#arguments_1","text":"Name Description d_model The number of expected features in the encoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward: The dim of the feedforward neuralnetworks in the EncoderLayer. dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer.","title":"Arguments"},{"location":"reference/kerod/layers/transformer/#call-arguments_1","text":"Name Description src A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed A 3-D Tensor of float32 and shape [batch_size, N, dim] positional encoding of the encoder key_padding_mask [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image.","title":"Call arguments"},{"location":"reference/kerod/layers/transformer/#call-returns_1","text":"Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model]","title":"Call returns"},{"location":"reference/kerod/layers/transformer/#ancestors-in-mro_1","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/transformer/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/layers/transformer/#call_1","text":"def call ( self , src , pos_emb , key_padding_mask = None , training = None ) Forward of the EncoderLayer Parameters: Name Description src A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: Type Description tf.Tensor A 3-D Tensor of float32 and shape [batch_size, M, d_model] View Source def call ( self , src , pos_emb , key_padding_mask = None , training = None ) : \"\"\"Forward of the EncoderLayer Args: src: A 3-D Tensor of float32 and shape [batch_size, M, dim] the sequence to the encoder layer pos_embed: A 3-D Tensor of float32 and shape [batch_size, M, dim] positional encoding of the encoder key_padding_mask: [Optional] A 2-D bool Tensor of shape [batch_size, seq_len_enc] where False means padding and True means pixel from the original image. Returns: tf.Tensor: A 3-D Tensor of float32 and shape [batch_size, M, d_model] \"\"\" x_pos_emb = src + pos_emb attn_output = self . mha ( src , x_pos_emb , x_pos_emb , key_padding_mask = key_padding_mask , training = training ) # ( batch_size , input_seq_len , d_model ) attn_output = self . dropout1 ( attn_output , training = training ) out1 = self . layernorm1 ( src + attn_output ) # ( batch_size , input_seq_len , d_model ) ffn_output = self . ffn ( out1 ) # ( batch_size , input_seq_len , d_model ) ffn_output = self . dropout2 ( ffn_output , training = training ) out2 = self . layernorm2 ( out1 + ffn_output ) # ( batch_size , input_seq_len , d_model ) return out2","title":"call"},{"location":"reference/kerod/layers/transformer/#transformer","text":"class Transformer ( num_layers = 6 , d_model = 256 , num_heads = 8 , dim_feedforward = 2048 , dropout_rate = 0.1 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention .","title":"Transformer"},{"location":"reference/kerod/layers/transformer/#arguments_2","text":"Name Description num_layers the number of sub-layers in the decoder and the encoder. d_model The number of expected features in the encoder/decoder inputs num_heads The number of heads in the multiheadattention models. dim_feedforward The dim of the feedforward neuralnetworks in the EncoderLayer and DecoderLayer dropout_rate Float between 0 and 1. Fraction of the input units to drop. The same rate is shared in all the layers using dropout in the transformer.","title":"Arguments"},{"location":"reference/kerod/layers/transformer/#call-arguments_2","text":"Name Description flatten_tensor A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn)","title":"Call arguments"},{"location":"reference/kerod/layers/transformer/#call-returns_2","text":"Type Description Tuple - decoder_output : 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - encoder_output : 3-D float32 Tensor of shape [batch_size, batch_size, d_model]","title":"Call returns"},{"location":"reference/kerod/layers/transformer/#ancestors-in-mro_2","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/transformer/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/layers/transformer/#call_2","text":"def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) Parameters: Name Description flatten_tensor A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Type Description Tuple - decoder_output : 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - encoder_output : 3-D float32 Tensor of shape [batch_size, batch_size, d_model] View Source def call ( self , flatten_tensor , pos_embed , object_queries , key_padding_mask = None , coattn_mask = None , training = None ) : \" \"\" Args: flatten_tensor: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. It represents the flatten output tensor of the backbone. pos_embed: A 3-D float32 Tensor of shape [batch_size, H * W, d_model]. Positional spatial positional encoding matching the flatten_tensor. object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. key_padding_mask: A 2-D bool Tensor of shape [batch_size, H * W] where False means padding and True means pixels from the original image. coattn_mask: A 4-D float tensor of shape [batch_size, num_heads, H*W, num_object_queries, seq_len]. If provided, it will be added to the attention weight at the coattention step (memory x self_attn) Returns: Tuple: - `decoder_output`: 3-D float32 Tensor of shape [batch_size, h, d_model] where h is num_object_queries * num_layers if training is true and num_queries if training is set to False. - `encoder_output`: 3-D float32 Tensor of shape [batch_size, batch_size, d_model] \"\" \" memory = flatten_tensor for enc in self . enc_layers : # (batch_size, seq_len, d_model) memory = enc ( memory , pos_embed , key_padding_mask = key_padding_mask , training = training ) # At the beginning we set target to 0 # In the first decoder layer Q and K will be equal # to dec_out + object_queries=object_queries dec_out = tf . zeros_like ( object_queries ) layers_output = [] for layer in self . dec_layers : dec_out = layer ( dec_out , memory , pos_embed , object_queries , key_padding_mask = key_padding_mask , coattn_mask = coattn_mask , training = training ) dec_out = self . layer_norm ( dec_out ) if training : layers_output . append ( dec_out ) if training : return tf . concat ( layers_output , axis = 1 ), memory return dec_out , memory","title":"call"},{"location":"reference/kerod/layers/detection/","text":"Module kerod.layers.detection None None Sub-modules kerod.layers.detection.abstract_detection_head kerod.layers.detection.fast_rcnn kerod.layers.detection.pooling_ops kerod.layers.detection.rpn","title":"Index"},{"location":"reference/kerod/layers/detection/#module-kerodlayersdetection","text":"None None","title":"Module kerod.layers.detection"},{"location":"reference/kerod/layers/detection/#sub-modules","text":"kerod.layers.detection.abstract_detection_head kerod.layers.detection.fast_rcnn kerod.layers.detection.pooling_ops kerod.layers.detection.rpn","title":"Sub-modules"},{"location":"reference/kerod/layers/detection/abstract_detection_head/","text":"Module kerod.layers.detection.abstract_detection_head None None View Source from typing import Dict import tensorflow as tf import tensorflow.keras.layers as KL from kerod.core.standard_fields import BoxField from kerod.utils.documentation import remove_unwanted_doc from tensorflow.keras import initializers __pdoc__ = {} class AbstractDetectionHead ( KL . Layer ): \"\"\"Abstract object detector. It encapsulates the main functions of an object detector. Arguments: num_classes: Number of classes of the classification head (e.g: Your n classes + the background class) classification_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. localization_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. segmentation_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. classification_loss_weight: A float 32 representing the weight of the loss in the total loss. localization_loss_weight: A float 32 representing the weight of the loss in the total loss. segmentation_loss_weight: A float 32 representing the weight of the loss in the total loss. multiples: How many time will you replicate the output of the head. For a rpn multiples can be the number of anchors. For a fast_rcnn multiples is 1 we just want the number of classes kernel_initializer_classification_head: Initializer for the `kernel` weights matrix of the classification head (see [initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)). kernel_initializer_box_prediction_head: Initializer for the `kernel` weights matrix of the box prediction head (see [initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)). kernel_regularizer: Regularizer function applied to the kernel weights matrix ([see keras.regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)). use_mask: Boolean define if the segmentation_head will be used. \"\"\" def __init__ ( self , num_classes , classification_loss , localization_loss , segmentation_loss = None , classification_loss_weight = 1.0 , localization_loss_weight = 1.0 , segmentation_loss_weight = 1.0 , multiples = 1 , kernel_initializer_classification_head = None , kernel_initializer_box_prediction_head = None , kernel_regularizer = None , use_mask = False , ** kwargs ): super () . __init__ ( ** kwargs ) self . _num_classes = num_classes self . _classification_loss = classification_loss self . _localization_loss = localization_loss self . _classification_loss_weight = classification_loss_weight self . _localization_loss_weight = localization_loss_weight self . _multiples = multiples self . _kernel_initializer_classification_head = kernel_initializer_classification_head self . _kernel_initializer_box_prediction_head = kernel_initializer_box_prediction_head self . _kernel_regularizer = kernel_regularizer self . _use_mask = use_mask if self . _use_mask : self . _segmentation_loss_weight = segmentation_loss_weight self . _segmentation_loss = segmentation_loss def build ( self , input_shape ): self . _conv_classification_head = KL . Conv2D ( self . _multiples * self . _num_classes , ( 1 , 1 ), padding = 'valid' , activation = None , kernel_initializer = self . _kernel_initializer_classification_head , kernel_regularizer = self . _kernel_regularizer , name = f ' { self . name } classification_head' ) self . _conv_box_prediction_head = KL . Conv2D ( ( self . _num_classes - 1 ) * self . _multiples * 4 , ( 1 , 1 ), padding = 'valid' , activation = None , kernel_initializer = self . _kernel_initializer_box_prediction_head , kernel_regularizer = self . _kernel_regularizer , name = f ' { self . name } box_prediction_head' ) if self . _use_mask : self . _segmentation_layers = [ KL . Conv2D ( 256 , ( 3 , 3 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ), KL . Conv2DTranspose ( 256 , ( 2 , 2 ), strides = ( 2 , 2 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ), KL . Conv2D ( self . _num_classes , ( 3 , 3 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ) ] super () . build ( input_shape ) def build_segmentation_head ( self , inputs ): \"\"\"Build the detection head Arguments: inputs: A tensor of float and shape [N, H, W, C] Returns: tf.Tensor: A tensor and shape [N, H*2, W*2, num_classes - 1] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x def build_detection_head ( self , inputs ): \"\"\" Build a detection head composed of a classification and box_detection. Arguments: inputs: A tensor of shape [batch_size, H, W, C] Returns: Tuple: classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head def compute_losses ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], weights : Dict [ str , tf . Tensor ]) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ): losses = loss ( tf . cast ( y_true [ target ], tf . float32 ), tf . cast ( y_pred [ target ], tf . float32 ), sample_weight = tf . cast ( weights [ target ], tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField . LABELS ], axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f ' { self . name } _classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f ' { self . name } _localization_loss' , aggregation = 'mean' ) self . add_loss ([ classification_loss , localization_loss ]) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f ' { self . name } _segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss } def get_config ( self ): base_config = super () . get_config () base_config [ 'num_classes' ] = self . _num_classes base_config [ 'classification_loss_weight' ] = self . _classification_loss_weight base_config [ 'localization_loss_weight' ] = self . _localization_loss_weight base_config [ 'multiples' ] = self . _multiples base_config [ 'use_mask' ] = self . _use_mask if self . _use_mask : base_config [ 'segmentation_loss_weight' ] = self . _segmentation_loss_weight return base_config remove_unwanted_doc ( AbstractDetectionHead , __pdoc__ ) Classes AbstractDetectionHead class AbstractDetectionHead ( num_classes , classification_loss , localization_loss , segmentation_loss = None , classification_loss_weight = 1.0 , localization_loss_weight = 1.0 , segmentation_loss_weight = 1.0 , multiples = 1 , kernel_initializer_classification_head = None , kernel_initializer_box_prediction_head = None , kernel_regularizer = None , use_mask = False , ** kwargs ) Arguments Name Description num_classes Number of classes of the classification head (e.g: Your n classes + the background class) classification_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. localization_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. segmentation_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. classification_loss_weight A float 32 representing the weight of the loss in the total loss. localization_loss_weight A float 32 representing the weight of the loss in the total loss. segmentation_loss_weight A float 32 representing the weight of the loss in the total loss. multiples How many time will you replicate the output of the head. For a rpn multiples can be the number of anchors. For a fast_rcnn multiples is 1 we just want the number of classes kernel_initializer_classification_head Initializer for the kernel weights matrix of the classification head (see initializers ). kernel_initializer_box_prediction_head Initializer for the kernel weights matrix of the box prediction head (see initializers ). kernel_regularizer Regularizer function applied to the kernel weights matrix ( see keras.regularizers ). use_mask Boolean define if the segmentation_head will be used. Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods build_detection_head def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head build_segmentation_head def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x call def call ( self , inputs , ** kwargs ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Description inputs Input tensor, or list/tuple of input tensors. **kwargs Additional keyword arguments. Currently unused. Returns: Type Description None A tensor or list/tuple of tensors. View Source @doc_controls.for_subclass_implementers def call ( self , inputs , ** kwargs ) : # pylint: disable=unused-argument \" \"\" This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. \"\" \" return inputs compute_losses def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"Abstract Detection Head"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#module-kerodlayersdetectionabstract_detection_head","text":"None None View Source from typing import Dict import tensorflow as tf import tensorflow.keras.layers as KL from kerod.core.standard_fields import BoxField from kerod.utils.documentation import remove_unwanted_doc from tensorflow.keras import initializers __pdoc__ = {} class AbstractDetectionHead ( KL . Layer ): \"\"\"Abstract object detector. It encapsulates the main functions of an object detector. Arguments: num_classes: Number of classes of the classification head (e.g: Your n classes + the background class) classification_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. localization_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. segmentation_loss: An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. classification_loss_weight: A float 32 representing the weight of the loss in the total loss. localization_loss_weight: A float 32 representing the weight of the loss in the total loss. segmentation_loss_weight: A float 32 representing the weight of the loss in the total loss. multiples: How many time will you replicate the output of the head. For a rpn multiples can be the number of anchors. For a fast_rcnn multiples is 1 we just want the number of classes kernel_initializer_classification_head: Initializer for the `kernel` weights matrix of the classification head (see [initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)). kernel_initializer_box_prediction_head: Initializer for the `kernel` weights matrix of the box prediction head (see [initializers](https://www.tensorflow.org/api_docs/python/tf/keras/initializers)). kernel_regularizer: Regularizer function applied to the kernel weights matrix ([see keras.regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers)). use_mask: Boolean define if the segmentation_head will be used. \"\"\" def __init__ ( self , num_classes , classification_loss , localization_loss , segmentation_loss = None , classification_loss_weight = 1.0 , localization_loss_weight = 1.0 , segmentation_loss_weight = 1.0 , multiples = 1 , kernel_initializer_classification_head = None , kernel_initializer_box_prediction_head = None , kernel_regularizer = None , use_mask = False , ** kwargs ): super () . __init__ ( ** kwargs ) self . _num_classes = num_classes self . _classification_loss = classification_loss self . _localization_loss = localization_loss self . _classification_loss_weight = classification_loss_weight self . _localization_loss_weight = localization_loss_weight self . _multiples = multiples self . _kernel_initializer_classification_head = kernel_initializer_classification_head self . _kernel_initializer_box_prediction_head = kernel_initializer_box_prediction_head self . _kernel_regularizer = kernel_regularizer self . _use_mask = use_mask if self . _use_mask : self . _segmentation_loss_weight = segmentation_loss_weight self . _segmentation_loss = segmentation_loss def build ( self , input_shape ): self . _conv_classification_head = KL . Conv2D ( self . _multiples * self . _num_classes , ( 1 , 1 ), padding = 'valid' , activation = None , kernel_initializer = self . _kernel_initializer_classification_head , kernel_regularizer = self . _kernel_regularizer , name = f ' { self . name } classification_head' ) self . _conv_box_prediction_head = KL . Conv2D ( ( self . _num_classes - 1 ) * self . _multiples * 4 , ( 1 , 1 ), padding = 'valid' , activation = None , kernel_initializer = self . _kernel_initializer_box_prediction_head , kernel_regularizer = self . _kernel_regularizer , name = f ' { self . name } box_prediction_head' ) if self . _use_mask : self . _segmentation_layers = [ KL . Conv2D ( 256 , ( 3 , 3 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ), KL . Conv2DTranspose ( 256 , ( 2 , 2 ), strides = ( 2 , 2 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ), KL . Conv2D ( self . _num_classes , ( 3 , 3 ), padding = 'valid' , activation = 'relu' , kernel_initializer = initializers . VarianceScaling ( scale = 2. , mode = 'fan_out' ), kernel_regularizer = self . _kernel_regularizer ) ] super () . build ( input_shape ) def build_segmentation_head ( self , inputs ): \"\"\"Build the detection head Arguments: inputs: A tensor of float and shape [N, H, W, C] Returns: tf.Tensor: A tensor and shape [N, H*2, W*2, num_classes - 1] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x def build_detection_head ( self , inputs ): \"\"\" Build a detection head composed of a classification and box_detection. Arguments: inputs: A tensor of shape [batch_size, H, W, C] Returns: Tuple: classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head def compute_losses ( self , y_true : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], weights : Dict [ str , tf . Tensor ]) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ): losses = loss ( tf . cast ( y_true [ target ], tf . float32 ), tf . cast ( y_pred [ target ], tf . float32 ), sample_weight = tf . cast ( weights [ target ], tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField . LABELS ], axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f ' { self . name } _classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f ' { self . name } _localization_loss' , aggregation = 'mean' ) self . add_loss ([ classification_loss , localization_loss ]) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f ' { self . name } _segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss } def get_config ( self ): base_config = super () . get_config () base_config [ 'num_classes' ] = self . _num_classes base_config [ 'classification_loss_weight' ] = self . _classification_loss_weight base_config [ 'localization_loss_weight' ] = self . _localization_loss_weight base_config [ 'multiples' ] = self . _multiples base_config [ 'use_mask' ] = self . _use_mask if self . _use_mask : base_config [ 'segmentation_loss_weight' ] = self . _segmentation_loss_weight return base_config remove_unwanted_doc ( AbstractDetectionHead , __pdoc__ )","title":"Module kerod.layers.detection.abstract_detection_head"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#abstractdetectionhead","text":"class AbstractDetectionHead ( num_classes , classification_loss , localization_loss , segmentation_loss = None , classification_loss_weight = 1.0 , localization_loss_weight = 1.0 , segmentation_loss_weight = 1.0 , multiples = 1 , kernel_initializer_classification_head = None , kernel_initializer_box_prediction_head = None , kernel_regularizer = None , use_mask = False , ** kwargs )","title":"AbstractDetectionHead"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#arguments","text":"Name Description num_classes Number of classes of the classification head (e.g: Your n classes + the background class) classification_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. localization_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. segmentation_loss An object tf.keras.losses usually CategoricalCrossentropy. This object should have a reduction value to None and the parameter from_y_pred to True. classification_loss_weight A float 32 representing the weight of the loss in the total loss. localization_loss_weight A float 32 representing the weight of the loss in the total loss. segmentation_loss_weight A float 32 representing the weight of the loss in the total loss. multiples How many time will you replicate the output of the head. For a rpn multiples can be the number of anchors. For a fast_rcnn multiples is 1 we just want the number of classes kernel_initializer_classification_head Initializer for the kernel weights matrix of the classification head (see initializers ). kernel_initializer_box_prediction_head Initializer for the kernel weights matrix of the box prediction head (see initializers ). kernel_regularizer Regularizer function applied to the kernel weights matrix ( see keras.regularizers ). use_mask Boolean define if the segmentation_head will be used.","title":"Arguments"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#ancestors-in-mro","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#build_detection_head","text":"def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head","title":"build_detection_head"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#build_segmentation_head","text":"def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x","title":"build_segmentation_head"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#call","text":"def call ( self , inputs , ** kwargs ) This is where the layer's logic lives. Note here that call() method in tf.keras is little bit different from keras API. In keras API, you can pass support masking for layers as additional arguments. Whereas tf.keras has compute_mask() method to support masking. Parameters: Name Description inputs Input tensor, or list/tuple of input tensors. **kwargs Additional keyword arguments. Currently unused. Returns: Type Description None A tensor or list/tuple of tensors. View Source @doc_controls.for_subclass_implementers def call ( self , inputs , ** kwargs ) : # pylint: disable=unused-argument \" \"\" This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. \"\" \" return inputs","title":"call"},{"location":"reference/kerod/layers/detection/abstract_detection_head/#compute_losses","text":"def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"compute_losses"},{"location":"reference/kerod/layers/detection/fast_rcnn/","text":"Module kerod.layers.detection.fast_rcnn None None View Source import functools from typing import Dict import tensorflow as tf import tensorflow . keras . layers as KL from tensorflow . keras import initializers from tensorflow . keras . losses import SparseCategoricalCrossentropy from kerod . core . box_coder import encode_boxes_faster_rcnn from kerod . core . losses import L1Loss from kerod . core . matcher import Matcher from kerod . core . sampling_ops import batch_sample_balanced_positive_negative from kerod . core . similarity import IoUSimilarity from kerod . core . standard_fields import BoxField from kerod . core . target_assigner import TargetAssigner from kerod . layers . detection . abstract_detection_head import AbstractDetectionHead from kerod . layers . detection . pooling_ops import multilevel_roi_align from kerod . utils . documentation import remove_unwanted_doc __ pdoc__ = {} class FastRCNN ( AbstractDetectionHead ) : \"\"\"Build the Fast-RCNN on top of the FPN. The parameters used are from [Feature Pyramidal Networks for Object Detection](https://arxiv.org/abs/1612.03144). Arguments: num_classes: The number of classes that predict the classification head (N+1) where N is the number of classes of your dataset and 1 is the background. Call arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Call returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" def __ init__ ( self , num_classes , **kwargs ) : super (). __ init__ ( num_classes , SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ), L1Loss ( reduction = tf . keras . losses . Reduction . NONE ), # like in tensorpack kernel_initializer_classification_head = initializers . RandomNormal ( stddev = 0.01 ), kernel_initializer_box_prediction_head = initializers . RandomNormal ( stddev = 0.001 ), **kwargs ) matcher = Matcher ([ 0.5 ], [ 0 , 1 ]) # The same scale_factors is used in decoding as well encode = functools . partial ( encode_boxes_faster_rcnn , scale_factors= ( 10.0 , 10.0 , 5.0 , 5.0 )) self . target_assigner = TargetAssigner ( IoUSimilarity (), matcher , encode , dtype = self . _ compute_dtype ) def build ( self , input_shape ) : self . denses = [ KL . Dense ( 1024 , kernel_initializer = initializers . VarianceScaling (), kernel_regularizer = self . _ kernel_regularizer , activation='relu' ) for _ in range ( 2 ) ] super (). build ( input_shape ) def call ( self , inputs ) : \"\"\"Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: ```python from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes(classification_pred, localization_pred, anchors, images_information, num_classes) ``` where `images_information` is provided as input of your model and `num_classes` includes the background. Arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" # Remove P6 pyramid = inputs [ 0 ][:- 1 ] anchors = inputs [ 1 ] # We can compute the original image shape regarding # TODO compute it more automatically without knowing that the last layer is stride 32 image_shape = tf . cast ( tf . shape ( pyramid [ - 1 ])[ 1 : 3 ] * 32 , dtype = self . _ compute_dtype ) boxe_tensors = multilevel_roi_align ( pyramid , anchors , image_shape , crop_size = 7 ) l = KL . Flatten ()( boxe_tensors ) for dense in self . denses : l = dense ( l ) classification_pred , localization_pred = self . build_detection_head ( tf . reshape ( l , ( - 1 , 1 , 1 , 1024 ))) batch_size = tf . shape ( anchors )[ 0 ] classification_pred = tf . reshape ( classification_pred , ( batch_size , - 1 , self . _ num_classes )) localization_pred = tf . reshape ( localization_pred , ( batch_size , - 1 , ( self . _ num_classes - 1 ) * 4 )) return classification_pred , localization_pred def sample_boxes ( self , anchors : tf . Tensor , ground_truths: Dict [ str , tf . Tensor ], sampling_size: int = 512 , sampling_positive_ratio: float = 0.25 ) : \"\"\"Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Arguments: anchors: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size: Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio: Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Tuple: 1. y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - `BoxField.LABELS`: A 2-D tensor of shape [batch_size, num_anchors], - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] Raises: ValueError: If the batch_size is None. ValueError: If the batch_size between your ground_truths and the anchors does not match. \"\"\" ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField . LABELS ] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } y_true , weights = self . target_assigner . assign ({ BoxField . BOXES : anchors }, ground_truths ) labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ], sampling_size , labels , positive_fraction = sampling_positive_ratio , dtype = self . _ compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] selected_boxes_idx = tf . where ( sample_idx == 1 ) batch_size = tf . shape ( sample_idx )[ 0 ] # Extract the selected anchors corresponding anchors # tf . gather_nd collaps the batch_together so we reshape with the proper batch_size anchors = tf . reshape ( tf . gather_nd ( anchors , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField . BOXES ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField . BOXES ], selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField . LABELS ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField . LABELS ], selected_boxes_idx ), ( batch_size , - 1 )) for key in y_true . keys () : weights [ key ] = tf . reshape ( tf . gather_nd ( weights [ key ], selected_boxes_idx ), ( batch_size , - 1 )) weights [ key ] = tf . stop_gradient ( weights [ key ]) y_true [ key ] = tf . stop_gradient ( y_true [ key ]) return y_true , weights , anchors def compute_loss ( self , y_true: dict , weights : dict , classification_pred: tf . Tensor , localization_pred: tf . Tensor ) : \"\"\"Compute the loss of the FastRCNN Arguments: y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, 4] weights: A dict with: - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] classification_pred: A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred: A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Tuple: - `classification_loss`: A scalar - `localization_loss`: A scalar \"\"\" y_true_classification = tf . cast ( y_true [ BoxField . LABELS ], tf . int32 ) accuracy , fg_accuracy , false_negative = compute_fast_rcnn_metrics ( y_true_classification , classification_pred ) self . add_metric ( accuracy , name='accuracy' , aggregation='mean' ) self . add_metric ( fg_accuracy , name='fg_accuracy' , aggregation='mean' ) self . add_metric ( false_negative , name='false_negative' , aggregation='mean' ) # y_true [ BoxField . LABELS ] is just 1 and 0 we are using it as mask to extract # the corresponding target anchors batch_size = tf . shape ( classification_pred )[ 0 ] # We create a boolean mask to extract the desired localization prediction to compute # the loss one_hot_targets = tf . one_hot ( y_true_classification , self . _ num_classes , dtype = tf . int8 ) one_hot_targets = tf . reshape ( one_hot_targets , [ - 1 ]) # We need to insert a fake background classes at the position 0 localization_pred = tf . pad ( localization_pred , [[ 0 , 0 ], [ 0 , 0 ], [ 4 , 0 ]]) localization_pred = tf . reshape ( localization_pred , [ - 1 , 4 ]) extracted_localization_pred = tf . boolean_mask ( localization_pred , one_hot_targets > 0 ) extracted_localization_pred = tf . reshape ( extracted_localization_pred , ( batch_size , - 1 , 4 )) y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : extracted_localization_pred } return self . compute_losses ( y_true , y_pred , weights ) def compute_fast_rcnn_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training of the fast rcnn head. `Warning`: This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred: A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Tuple: 1. `accuracy`: A scalar tensor representing the accuracy with the background classes included 2. `fg_accuracy`: A scalar tensor representing the accuracy without the background classes included 3. `false_negative`: A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. \"\"\" # compute usefull metrics # Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis=- 1 , name='label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # The accuracy allows to determine if the models perform well ( background included ) accuracy = tf . reduce_mean ( correct , name='accuracy' ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) num_fg = tf . shape ( fg_inds )[ 0 ] fg_label_pred = tf . argmax ( tf . gather_nd ( y_pred , fg_inds ), axis=- 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int32 ), name='num_zero' ) # Number of example predicted as background instead of one of our classes false_negative = tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 , name='false_negative' ) fg_accuracy = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name='fg_accuracy' ) return accuracy , fg_accuracy , false_negative remove_unwanted_doc ( FastRCNN , __ pdoc__ ) Functions compute_fast_rcnn_metrics def compute_fast_rcnn_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training of the fast rcnn head. Warning : This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Parameters: Name Description y_true A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Type Description Tuple 1. accuracy : A scalar tensor representing the accuracy with the background classes included 2. fg_accuracy : A scalar tensor representing the accuracy without the background classes included 3. false_negative : A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. View Source def compute_fast_rcnn_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor ) : \" \"\" Useful metrics that allows to track how behave the training of the fast rcnn head. `Warning`: This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred: A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Tuple: 1. `accuracy`: A scalar tensor representing the accuracy with the background classes included 2. `fg_accuracy`: A scalar tensor representing the accuracy without the background classes included 3. `false_negative`: A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. \"\" \" # compute usefull metrics #Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # The accuracy allows to determine if the models perform well (background included) accuracy = tf . reduce_mean ( correct , name = 'accuracy' ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) num_fg = tf . shape ( fg_inds ) [ 0 ] fg_label_pred = tf . argmax ( tf . gather_nd ( y_pred , fg_inds ), axis =- 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int32 ), name = 'num_zero' ) # Number of example predicted as background instead of one of our classes false_negative = tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 , name = 'false_negative' ) fg_accuracy = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = 'fg_accuracy' ) return accuracy , fg_accuracy , false_negative Classes FastRCNN class FastRCNN ( num_classes , ** kwargs ) are from Feature Pyramidal Networks for Object Detection . Arguments Name Description num_classes The number of classes that predict the classification head (N+1) where N is the number of classes of your dataset and 1 is the background. Call arguments Name Description inputs A Tuple 1. pyramid : A List of tensors the output of the pyramid 2. anchors : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Call returns Type Description Tuple classification_pred : A logit Tensor of shape [batch_size, num_boxes, num_classes] localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] anchors : A Tensor of shape [batch_size, num_boxes, 4] Ancestors (in MRO) kerod.layers.detection.abstract_detection_head.AbstractDetectionHead tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods build_detection_head def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head build_segmentation_head def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x call def call ( self , inputs ) Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes ( classification_pred , localization_pred , anchors , images_information , num_classes ) where images_information is provided as input of your model and num_classes includes the background. Parameters: Name Description inputs A Tuple 1. pyramid : A List of tensors the output of the pyramid 2. anchors : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Type Description Tuple classification_pred : A logit Tensor of shape [batch_size, num_boxes, num_classes] localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs ) : \"\"\"Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: ```python from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes(classification_pred, localization_pred, anchors, images_information, num_classes) ``` where `images_information` is provided as input of your model and `num_classes` includes the background. Arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" # Remove P6 pyramid = inputs [ 0 ][:- 1 ] anchors = inputs [ 1 ] # We can compute the original image shape regarding # TODO compute it more automatically without knowing that the last layer is stride 32 image_shape = tf . cast ( tf . shape ( pyramid [ - 1 ])[ 1 : 3 ] * 32 , dtype = self . _ compute_dtype ) boxe_tensors = multilevel_roi_align ( pyramid , anchors , image_shape , crop_size = 7 ) l = KL . Flatten ()( boxe_tensors ) for dense in self . denses : l = dense ( l ) classification_pred , localization_pred = self . build_detection_head ( tf . reshape ( l , ( - 1 , 1 , 1 , 1024 ))) batch_size = tf . shape ( anchors )[ 0 ] classification_pred = tf . reshape ( classification_pred , ( batch_size , - 1 , self . _ num_classes )) localization_pred = tf . reshape ( localization_pred , ( batch_size , - 1 , ( self . _ num_classes - 1 ) * 4 )) return classification_pred , localization_pred compute_loss def compute_loss ( self , y_true : dict , weights : dict , classification_pred : tensorflow . python . framework . ops . Tensor , localization_pred : tensorflow . python . framework . ops . Tensor ) Compute the loss of the FastRCNN Parameters: Name Description y_true A dict with : - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes] - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_anchors, 4] weights A dict with: - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes] - BoxField.BOXES : A 2-D tensor of shape [batch_size, num_anchors] classification_pred A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Type Description Tuple - classification_loss : A scalar - localization_loss : A scalar View Source def compute_loss ( self , y_true : dict , weights : dict , classification_pred : tf . Tensor , localization_pred : tf . Tensor ) : \" \"\" Compute the loss of the FastRCNN Arguments: y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, 4] weights: A dict with: - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] classification_pred: A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred: A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Tuple: - `classification_loss`: A scalar - `localization_loss`: A scalar \"\" \" y_true_classification = tf . cast ( y_true [ BoxField . LABELS ] , tf . int32 ) accuracy , fg_accuracy , false_negative = compute_fast_rcnn_metrics ( y_true_classification , classification_pred ) self . add_metric ( accuracy , name = 'accuracy' , aggregation = 'mean' ) self . add_metric ( fg_accuracy , name = 'fg_accuracy' , aggregation = 'mean' ) self . add_metric ( false_negative , name = 'false_negative' , aggregation = 'mean' ) # y_true[BoxField.LABELS] is just 1 and 0 we are using it as mask to extract # the corresponding target anchors batch_size = tf . shape ( classification_pred ) [ 0 ] # We create a boolean mask to extract the desired localization prediction to compute # the loss one_hot_targets = tf . one_hot ( y_true_classification , self . _num_classes , dtype = tf . int8 ) one_hot_targets = tf . reshape ( one_hot_targets , [ - 1 ] ) # We need to insert a fake background classes at the position 0 localization_pred = tf . pad ( localization_pred , [[ 0 , 0 ] , [ 0 , 0 ] , [ 4 , 0 ]] ) localization_pred = tf . reshape ( localization_pred , [ - 1 , 4 ] ) extracted_localization_pred = tf . boolean_mask ( localization_pred , one_hot_targets > 0 ) extracted_localization_pred = tf . reshape ( extracted_localization_pred , ( batch_size , - 1 , 4 )) y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : extracted_localization_pred } return self . compute_losses ( y_true , y_pred , weights ) compute_losses def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss } sample_boxes def sample_boxes ( self , anchors : tensorflow . python . framework . ops . Tensor , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], sampling_size : int = 512 , sampling_positive_ratio : float = 0.25 ) Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Parameters: Name Description anchors A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Type Description Tuple 1. y_true: A dict with : - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - BoxField.LABELS : A 2-D tensor of shape [batch_size, num_anchors], - BoxField.BOXES : A 2-D tensor of shape [batch_size, num_anchors] Raises: Type Description ValueError If the batch_size is None. ValueError If the batch_size between your ground_truths and the anchors does not match. View Source def sample_boxes ( self , anchors : tf . Tensor , ground_truths : Dict [ str, tf.Tensor ] , sampling_size : int = 512 , sampling_positive_ratio : float = 0.25 ) : \"\"\"Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Arguments: anchors: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size: Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio: Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Tuple: 1. y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - `BoxField.LABELS`: A 2-D tensor of shape [batch_size, num_anchors], - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] Raises: ValueError: If the batch_size is None. ValueError: If the batch_size between your ground_truths and the anchors does not match. \"\"\" ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField.BOXES ] , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } y_true , weights = self . target_assigner . assign ( { BoxField . BOXES : anchors } , ground_truths ) labels = y_true [ BoxField.LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField.LABELS ] , sampling_size , labels , positive_fraction = sampling_positive_ratio , dtype = self . _compute_dtype ) weights [ BoxField.LABELS ] = sample_idx * weights [ BoxField.LABELS ] weights [ BoxField.BOXES ] = sample_idx * weights [ BoxField.BOXES ] selected_boxes_idx = tf . where ( sample_idx == 1 ) batch_size = tf . shape ( sample_idx ) [ 0 ] # Extract the selected anchors corresponding anchors # tf . gather_nd collaps the batch_together so we reshape with the proper batch_size anchors = tf . reshape ( tf . gather_nd ( anchors , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField.BOXES ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField.BOXES ] , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField.LABELS ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField.LABELS ] , selected_boxes_idx ), ( batch_size , - 1 )) for key in y_true . keys () : weights [ key ] = tf . reshape ( tf . gather_nd ( weights [ key ] , selected_boxes_idx ), ( batch_size , - 1 )) weights [ key ] = tf . stop_gradient ( weights [ key ] ) y_true [ key ] = tf . stop_gradient ( y_true [ key ] ) return y_true , weights , anchors","title":"Fast Rcnn"},{"location":"reference/kerod/layers/detection/fast_rcnn/#module-kerodlayersdetectionfast_rcnn","text":"None None View Source import functools from typing import Dict import tensorflow as tf import tensorflow . keras . layers as KL from tensorflow . keras import initializers from tensorflow . keras . losses import SparseCategoricalCrossentropy from kerod . core . box_coder import encode_boxes_faster_rcnn from kerod . core . losses import L1Loss from kerod . core . matcher import Matcher from kerod . core . sampling_ops import batch_sample_balanced_positive_negative from kerod . core . similarity import IoUSimilarity from kerod . core . standard_fields import BoxField from kerod . core . target_assigner import TargetAssigner from kerod . layers . detection . abstract_detection_head import AbstractDetectionHead from kerod . layers . detection . pooling_ops import multilevel_roi_align from kerod . utils . documentation import remove_unwanted_doc __ pdoc__ = {} class FastRCNN ( AbstractDetectionHead ) : \"\"\"Build the Fast-RCNN on top of the FPN. The parameters used are from [Feature Pyramidal Networks for Object Detection](https://arxiv.org/abs/1612.03144). Arguments: num_classes: The number of classes that predict the classification head (N+1) where N is the number of classes of your dataset and 1 is the background. Call arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Call returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" def __ init__ ( self , num_classes , **kwargs ) : super (). __ init__ ( num_classes , SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ), L1Loss ( reduction = tf . keras . losses . Reduction . NONE ), # like in tensorpack kernel_initializer_classification_head = initializers . RandomNormal ( stddev = 0.01 ), kernel_initializer_box_prediction_head = initializers . RandomNormal ( stddev = 0.001 ), **kwargs ) matcher = Matcher ([ 0.5 ], [ 0 , 1 ]) # The same scale_factors is used in decoding as well encode = functools . partial ( encode_boxes_faster_rcnn , scale_factors= ( 10.0 , 10.0 , 5.0 , 5.0 )) self . target_assigner = TargetAssigner ( IoUSimilarity (), matcher , encode , dtype = self . _ compute_dtype ) def build ( self , input_shape ) : self . denses = [ KL . Dense ( 1024 , kernel_initializer = initializers . VarianceScaling (), kernel_regularizer = self . _ kernel_regularizer , activation='relu' ) for _ in range ( 2 ) ] super (). build ( input_shape ) def call ( self , inputs ) : \"\"\"Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: ```python from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes(classification_pred, localization_pred, anchors, images_information, num_classes) ``` where `images_information` is provided as input of your model and `num_classes` includes the background. Arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" # Remove P6 pyramid = inputs [ 0 ][:- 1 ] anchors = inputs [ 1 ] # We can compute the original image shape regarding # TODO compute it more automatically without knowing that the last layer is stride 32 image_shape = tf . cast ( tf . shape ( pyramid [ - 1 ])[ 1 : 3 ] * 32 , dtype = self . _ compute_dtype ) boxe_tensors = multilevel_roi_align ( pyramid , anchors , image_shape , crop_size = 7 ) l = KL . Flatten ()( boxe_tensors ) for dense in self . denses : l = dense ( l ) classification_pred , localization_pred = self . build_detection_head ( tf . reshape ( l , ( - 1 , 1 , 1 , 1024 ))) batch_size = tf . shape ( anchors )[ 0 ] classification_pred = tf . reshape ( classification_pred , ( batch_size , - 1 , self . _ num_classes )) localization_pred = tf . reshape ( localization_pred , ( batch_size , - 1 , ( self . _ num_classes - 1 ) * 4 )) return classification_pred , localization_pred def sample_boxes ( self , anchors : tf . Tensor , ground_truths: Dict [ str , tf . Tensor ], sampling_size: int = 512 , sampling_positive_ratio: float = 0.25 ) : \"\"\"Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Arguments: anchors: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size: Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio: Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Tuple: 1. y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - `BoxField.LABELS`: A 2-D tensor of shape [batch_size, num_anchors], - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] Raises: ValueError: If the batch_size is None. ValueError: If the batch_size between your ground_truths and the anchors does not match. \"\"\" ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField . LABELS ] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } y_true , weights = self . target_assigner . assign ({ BoxField . BOXES : anchors }, ground_truths ) labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ], sampling_size , labels , positive_fraction = sampling_positive_ratio , dtype = self . _ compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] selected_boxes_idx = tf . where ( sample_idx == 1 ) batch_size = tf . shape ( sample_idx )[ 0 ] # Extract the selected anchors corresponding anchors # tf . gather_nd collaps the batch_together so we reshape with the proper batch_size anchors = tf . reshape ( tf . gather_nd ( anchors , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField . BOXES ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField . BOXES ], selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField . LABELS ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField . LABELS ], selected_boxes_idx ), ( batch_size , - 1 )) for key in y_true . keys () : weights [ key ] = tf . reshape ( tf . gather_nd ( weights [ key ], selected_boxes_idx ), ( batch_size , - 1 )) weights [ key ] = tf . stop_gradient ( weights [ key ]) y_true [ key ] = tf . stop_gradient ( y_true [ key ]) return y_true , weights , anchors def compute_loss ( self , y_true: dict , weights : dict , classification_pred: tf . Tensor , localization_pred: tf . Tensor ) : \"\"\"Compute the loss of the FastRCNN Arguments: y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, 4] weights: A dict with: - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] classification_pred: A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred: A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Tuple: - `classification_loss`: A scalar - `localization_loss`: A scalar \"\"\" y_true_classification = tf . cast ( y_true [ BoxField . LABELS ], tf . int32 ) accuracy , fg_accuracy , false_negative = compute_fast_rcnn_metrics ( y_true_classification , classification_pred ) self . add_metric ( accuracy , name='accuracy' , aggregation='mean' ) self . add_metric ( fg_accuracy , name='fg_accuracy' , aggregation='mean' ) self . add_metric ( false_negative , name='false_negative' , aggregation='mean' ) # y_true [ BoxField . LABELS ] is just 1 and 0 we are using it as mask to extract # the corresponding target anchors batch_size = tf . shape ( classification_pred )[ 0 ] # We create a boolean mask to extract the desired localization prediction to compute # the loss one_hot_targets = tf . one_hot ( y_true_classification , self . _ num_classes , dtype = tf . int8 ) one_hot_targets = tf . reshape ( one_hot_targets , [ - 1 ]) # We need to insert a fake background classes at the position 0 localization_pred = tf . pad ( localization_pred , [[ 0 , 0 ], [ 0 , 0 ], [ 4 , 0 ]]) localization_pred = tf . reshape ( localization_pred , [ - 1 , 4 ]) extracted_localization_pred = tf . boolean_mask ( localization_pred , one_hot_targets > 0 ) extracted_localization_pred = tf . reshape ( extracted_localization_pred , ( batch_size , - 1 , 4 )) y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : extracted_localization_pred } return self . compute_losses ( y_true , y_pred , weights ) def compute_fast_rcnn_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training of the fast rcnn head. `Warning`: This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred: A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Tuple: 1. `accuracy`: A scalar tensor representing the accuracy with the background classes included 2. `fg_accuracy`: A scalar tensor representing the accuracy without the background classes included 3. `false_negative`: A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. \"\"\" # compute usefull metrics # Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis=- 1 , name='label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # The accuracy allows to determine if the models perform well ( background included ) accuracy = tf . reduce_mean ( correct , name='accuracy' ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) num_fg = tf . shape ( fg_inds )[ 0 ] fg_label_pred = tf . argmax ( tf . gather_nd ( y_pred , fg_inds ), axis=- 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int32 ), name='num_zero' ) # Number of example predicted as background instead of one of our classes false_negative = tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 , name='false_negative' ) fg_accuracy = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name='fg_accuracy' ) return accuracy , fg_accuracy , false_negative remove_unwanted_doc ( FastRCNN , __ pdoc__ )","title":"Module kerod.layers.detection.fast_rcnn"},{"location":"reference/kerod/layers/detection/fast_rcnn/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/detection/fast_rcnn/#compute_fast_rcnn_metrics","text":"def compute_fast_rcnn_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training of the fast rcnn head. Warning : This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Parameters: Name Description y_true A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Type Description Tuple 1. accuracy : A scalar tensor representing the accuracy with the background classes included 2. fg_accuracy : A scalar tensor representing the accuracy without the background classes included 3. false_negative : A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. View Source def compute_fast_rcnn_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor ) : \" \"\" Useful metrics that allows to track how behave the training of the fast rcnn head. `Warning`: This function should be used if the ground_truths have been added to the RoIs. It won't work if the there are no foreground ground_truths in the sample_boxes which isn't possible if they have been added. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_sample_anchors, num_classes] y_pred: A tensor with shape [batch_size, num_sample_anchors, num_classes], representing the classification logits. Returns: Tuple: 1. `accuracy`: A scalar tensor representing the accuracy with the background classes included 2. `fg_accuracy`: A scalar tensor representing the accuracy without the background classes included 3. `false_negative`: A scalar tensor representing the ratio of boxes predicted as background instead of their respective class among the foreground example to predict. \"\" \" # compute usefull metrics #Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # The accuracy allows to determine if the models perform well (background included) accuracy = tf . reduce_mean ( correct , name = 'accuracy' ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) num_fg = tf . shape ( fg_inds ) [ 0 ] fg_label_pred = tf . argmax ( tf . gather_nd ( y_pred , fg_inds ), axis =- 1 ) num_zero = tf . reduce_sum ( tf . cast ( tf . equal ( fg_label_pred , 0 ), tf . int32 ), name = 'num_zero' ) # Number of example predicted as background instead of one of our classes false_negative = tf . cast ( tf . truediv ( num_zero , num_fg ), tf . float32 , name = 'false_negative' ) fg_accuracy = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = 'fg_accuracy' ) return accuracy , fg_accuracy , false_negative","title":"compute_fast_rcnn_metrics"},{"location":"reference/kerod/layers/detection/fast_rcnn/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/detection/fast_rcnn/#fastrcnn","text":"class FastRCNN ( num_classes , ** kwargs ) are from Feature Pyramidal Networks for Object Detection .","title":"FastRCNN"},{"location":"reference/kerod/layers/detection/fast_rcnn/#arguments","text":"Name Description num_classes The number of classes that predict the classification head (N+1) where N is the number of classes of your dataset and 1 is the background.","title":"Arguments"},{"location":"reference/kerod/layers/detection/fast_rcnn/#call-arguments","text":"Name Description inputs A Tuple 1. pyramid : A List of tensors the output of the pyramid 2. anchors : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)]","title":"Call arguments"},{"location":"reference/kerod/layers/detection/fast_rcnn/#call-returns","text":"Type Description Tuple classification_pred : A logit Tensor of shape [batch_size, num_boxes, num_classes] localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] anchors : A Tensor of shape [batch_size, num_boxes, 4]","title":"Call returns"},{"location":"reference/kerod/layers/detection/fast_rcnn/#ancestors-in-mro","text":"kerod.layers.detection.abstract_detection_head.AbstractDetectionHead tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/detection/fast_rcnn/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/detection/fast_rcnn/#build_detection_head","text":"def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head","title":"build_detection_head"},{"location":"reference/kerod/layers/detection/fast_rcnn/#build_segmentation_head","text":"def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x","title":"build_segmentation_head"},{"location":"reference/kerod/layers/detection/fast_rcnn/#call","text":"def call ( self , inputs ) Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes ( classification_pred , localization_pred , anchors , images_information , num_classes ) where images_information is provided as input of your model and num_classes includes the background. Parameters: Name Description inputs A Tuple 1. pyramid : A List of tensors the output of the pyramid 2. anchors : A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Type Description Tuple classification_pred : A logit Tensor of shape [batch_size, num_boxes, num_classes] localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs ) : \"\"\"Build the computational graph of the fast RCNN HEAD. It performs a raw prediction of the FastRCNN head you can post_process them using: ```python from kerod.layers.post_processing import post_process_fast_rcnn_boxes outputs = post_process_fast_rcnn_boxes(classification_pred, localization_pred, anchors, images_information, num_classes) ``` where `images_information` is provided as input of your model and `num_classes` includes the background. Arguments: inputs: A Tuple 1. `pyramid`: A List of tensors the output of the pyramid 2. `anchors`: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] Returns: Tuple: `classification_pred`: A logit Tensor of shape [batch_size, num_boxes, num_classes] `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" # Remove P6 pyramid = inputs [ 0 ][:- 1 ] anchors = inputs [ 1 ] # We can compute the original image shape regarding # TODO compute it more automatically without knowing that the last layer is stride 32 image_shape = tf . cast ( tf . shape ( pyramid [ - 1 ])[ 1 : 3 ] * 32 , dtype = self . _ compute_dtype ) boxe_tensors = multilevel_roi_align ( pyramid , anchors , image_shape , crop_size = 7 ) l = KL . Flatten ()( boxe_tensors ) for dense in self . denses : l = dense ( l ) classification_pred , localization_pred = self . build_detection_head ( tf . reshape ( l , ( - 1 , 1 , 1 , 1024 ))) batch_size = tf . shape ( anchors )[ 0 ] classification_pred = tf . reshape ( classification_pred , ( batch_size , - 1 , self . _ num_classes )) localization_pred = tf . reshape ( localization_pred , ( batch_size , - 1 , ( self . _ num_classes - 1 ) * 4 )) return classification_pred , localization_pred","title":"call"},{"location":"reference/kerod/layers/detection/fast_rcnn/#compute_loss","text":"def compute_loss ( self , y_true : dict , weights : dict , classification_pred : tensorflow . python . framework . ops . Tensor , localization_pred : tensorflow . python . framework . ops . Tensor ) Compute the loss of the FastRCNN Parameters: Name Description y_true A dict with : - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes] - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_anchors, 4] weights A dict with: - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes] - BoxField.BOXES : A 2-D tensor of shape [batch_size, num_anchors] classification_pred A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Type Description Tuple - classification_loss : A scalar - localization_loss : A scalar View Source def compute_loss ( self , y_true : dict , weights : dict , classification_pred : tf . Tensor , localization_pred : tf . Tensor ) : \" \"\" Compute the loss of the FastRCNN Arguments: y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, 4] weights: A dict with: - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes] - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] classification_pred: A 3-D tensor of float and shape [batch_size, num_anchors, num_classes] localization_pred: A 3-D tensor of float and shape [batch_size, num_anchors, (num_classes - 1) * 4] Returns: Tuple: - `classification_loss`: A scalar - `localization_loss`: A scalar \"\" \" y_true_classification = tf . cast ( y_true [ BoxField . LABELS ] , tf . int32 ) accuracy , fg_accuracy , false_negative = compute_fast_rcnn_metrics ( y_true_classification , classification_pred ) self . add_metric ( accuracy , name = 'accuracy' , aggregation = 'mean' ) self . add_metric ( fg_accuracy , name = 'fg_accuracy' , aggregation = 'mean' ) self . add_metric ( false_negative , name = 'false_negative' , aggregation = 'mean' ) # y_true[BoxField.LABELS] is just 1 and 0 we are using it as mask to extract # the corresponding target anchors batch_size = tf . shape ( classification_pred ) [ 0 ] # We create a boolean mask to extract the desired localization prediction to compute # the loss one_hot_targets = tf . one_hot ( y_true_classification , self . _num_classes , dtype = tf . int8 ) one_hot_targets = tf . reshape ( one_hot_targets , [ - 1 ] ) # We need to insert a fake background classes at the position 0 localization_pred = tf . pad ( localization_pred , [[ 0 , 0 ] , [ 0 , 0 ] , [ 4 , 0 ]] ) localization_pred = tf . reshape ( localization_pred , [ - 1 , 4 ] ) extracted_localization_pred = tf . boolean_mask ( localization_pred , one_hot_targets > 0 ) extracted_localization_pred = tf . reshape ( extracted_localization_pred , ( batch_size , - 1 , 4 )) y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : extracted_localization_pred } return self . compute_losses ( y_true , y_pred , weights )","title":"compute_loss"},{"location":"reference/kerod/layers/detection/fast_rcnn/#compute_losses","text":"def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"compute_losses"},{"location":"reference/kerod/layers/detection/fast_rcnn/#sample_boxes","text":"def sample_boxes ( self , anchors : tensorflow . python . framework . ops . Tensor , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], sampling_size : int = 512 , sampling_positive_ratio : float = 0.25 ) Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Parameters: Name Description anchors A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Type Description Tuple 1. y_true: A dict with : - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_anchors, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - BoxField.LABELS : A 2-D tensor of shape [batch_size, num_anchors], - BoxField.BOXES : A 2-D tensor of shape [batch_size, num_anchors] Raises: Type Description ValueError If the batch_size is None. ValueError If the batch_size between your ground_truths and the anchors does not match. View Source def sample_boxes ( self , anchors : tf . Tensor , ground_truths : Dict [ str, tf.Tensor ] , sampling_size : int = 512 , sampling_positive_ratio : float = 0.25 ) : \"\"\"Perform the sampling of the target anchors. During the training a set of RoIs is detected by the RPN. However, you do not want to analyse all the set. You only want to analyse the anchors that you sampled with this method. Arguments: anchors: A tensor of shape [batch_size, num_boxes, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 sampling_size: Desired sampling size. If None, keeps all positive samples and randomly selects negative samples so that the positive sample fraction matches positive_fraction. sampling_positive_ratio: Desired fraction of positive examples (scalar in [0,1]) in the batch. Returns: Tuple: 1. y_true: A dict with : - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_anchors, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_anchors, box_code_dimension] 2. weights: A dict with: - `BoxField.LABELS`: A 2-D tensor of shape [batch_size, num_anchors], - `BoxField.BOXES`: A 2-D tensor of shape [batch_size, num_anchors] Raises: ValueError: If the batch_size is None. ValueError: If the batch_size between your ground_truths and the anchors does not match. \"\"\" ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField.BOXES ] , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } y_true , weights = self . target_assigner . assign ( { BoxField . BOXES : anchors } , ground_truths ) labels = y_true [ BoxField.LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField.LABELS ] , sampling_size , labels , positive_fraction = sampling_positive_ratio , dtype = self . _compute_dtype ) weights [ BoxField.LABELS ] = sample_idx * weights [ BoxField.LABELS ] weights [ BoxField.BOXES ] = sample_idx * weights [ BoxField.BOXES ] selected_boxes_idx = tf . where ( sample_idx == 1 ) batch_size = tf . shape ( sample_idx ) [ 0 ] # Extract the selected anchors corresponding anchors # tf . gather_nd collaps the batch_together so we reshape with the proper batch_size anchors = tf . reshape ( tf . gather_nd ( anchors , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField.BOXES ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField.BOXES ] , selected_boxes_idx ), ( batch_size , - 1 , 4 )) y_true [ BoxField.LABELS ] = tf . reshape ( tf . gather_nd ( y_true [ BoxField.LABELS ] , selected_boxes_idx ), ( batch_size , - 1 )) for key in y_true . keys () : weights [ key ] = tf . reshape ( tf . gather_nd ( weights [ key ] , selected_boxes_idx ), ( batch_size , - 1 )) weights [ key ] = tf . stop_gradient ( weights [ key ] ) y_true [ key ] = tf . stop_gradient ( y_true [ key ] ) return y_true , weights , anchors","title":"sample_boxes"},{"location":"reference/kerod/layers/detection/pooling_ops/","text":"Module kerod.layers.detection.pooling_ops None None View Source import tensorflow as tf import tensorflow.keras.layers as KL from tensorflow.keras import backend as K from kerod.core.box_ops import ( compute_area , normalize_box_coordinates , transform_fpcoor_for_tf ) def _crop_and_resize ( tensor , boxes , box_indices , crop_size : int ): \"\"\"Taken from tensorpack (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Arguments: tensor: A 4-D tensor of shape [batch, image_height, image_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" boxes = tf . stop_gradient ( boxes ) # TF's crop_and_resize produces zeros on border. The symetric padding # allows to have a better interpolation for the boxes on the border of the image. tensor = tf . pad ( tensor , [[ 0 , 0 ], [ 1 , 1 ], [ 1 , 1 ], [ 0 , 0 ]], mode = 'SYMMETRIC' ) boxes = boxes + 1 # Height, width extraction tensor_shape = tf . shape ( tensor )[ 1 : 3 ] # The boxes should be at the size of the input tensor boxes = transform_fpcoor_for_tf ( boxes , tensor_shape , [ crop_size , crop_size ]) ret = tf . image . crop_and_resize ( tensor , tf . cast ( boxes , tf . float32 ), # crop and resize needs float32 tf . cast ( box_indices , tf . int32 ), crop_size = [ crop_size , crop_size ]) return ret def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ): \"\"\"RoI align like operation from the paper Mask-RCNN. Arguments: inputs: A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape: A tuple with the height and the width of the original image input image crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" normalized_boxes = normalize_box_coordinates ( boxes , image_shape [ 0 ], image_shape [ 1 ]) # Normalized the boxes to the input tensor_shape # TODO rewrite this normalization unnomarlization isn't pretty at all tensor_shape = tf . shape ( inputs )[ 1 : 3 ] normalized_boxes *= tf . cast ( tf . tile ( tensor_shape [ None ], [ 1 , 2 ]), normalized_boxes . dtype ) ret = _crop_and_resize ( inputs , normalized_boxes , box_indices , crop_size * 2 ) return KL . AveragePooling2D ( padding = 'same' )( ret ) def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ): \"\"\"Perform a batch multilevel roi_align on the inputs Arguments: - *inputs*: A list of tensors of shape [batch_size, width, height, channel] representing the pyramid. - *boxes*: A tensor and shape [batch_size, num_boxes, (y1, x1, y2, x2)] - *image_shape*: A tuple with the height and the width of the original image input image Returns: A tensor and shape [batch_size * num_boxes, 7, 7, channel] \"\"\" boxes_per_level , box_indices_per_level , pos_per_level = match_boxes_to_their_pyramid_level ( boxes , len ( inputs )) tensors_per_level = [] for tensor , target_boxes , box_indices in zip ( inputs , boxes_per_level , box_indices_per_level ): tensors_per_level . append ( roi_align ( tensor , target_boxes , box_indices , image_shape , crop_size )) tensors = tf . concat ( values = tensors_per_level , axis = 0 ) original_pos = tf . concat ( values = pos_per_level , axis = 0 ) # Reorder the tensor per batch indices_to_reorder_boxes = tf . math . invert_permutation ( original_pos ) tensors = tf . gather ( tensors , indices_to_reorder_boxes ) return tensors def match_boxes_to_their_pyramid_level ( boxes , num_level ): \"\"\"Match the boxes to the proper level based on their area Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, 4] - *num_level*: Number of level of the target pyramid Returns: - *boxes_per_level*: A list of 2-D tensor and shape [N, 4] - *box_indices_per_level*: A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. - *original_pos_per_level* A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. \"\"\" def _select_level ( tensors , levels ): return [ tf . squeeze ( tf . gather ( tensors , selected_level ), 1 ) for selected_level in levels ] batch_size = tf . shape ( boxes )[ 0 ] num_boxes = tf . shape ( boxes )[ 1 ] boxes = tf . reshape ( boxes , ( - 1 , 4 )) box_levels = assign_pyramid_level_to_boxes ( boxes , num_level ) box_indices = tf . tile ( tf . expand_dims ( tf . range ( 0 , batch_size ), 1 ), [ 1 , num_boxes ]) box_indices = tf . reshape ( box_indices , ( - 1 ,)) box_original_pos = tf . range ( batch_size * num_boxes ) levels = [ tf . where ( tf . equal ( box_levels , i )) for i in range ( num_level )] boxes_per_level = _select_level ( boxes , levels ) box_indices_per_level = _select_level ( box_indices , levels ) original_pos_per_level = _select_level ( box_original_pos , levels ) return boxes_per_level , box_indices_per_level , original_pos_per_level def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ): \"\"\"Compute the pyramid level of an RoI Arguments: - *boxes*: A tensor of shape [nb_batches * nb_boxes, 4] - *num_level*: Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. \"\"\" denominator = tf . constant ( 224 , dtype = boxes . dtype ) area = compute_area ( boxes ) k = level_target + tf . math . log ( tf . sqrt ( area ) / denominator + K . epsilon ()) * tf . cast ( 1. / tf . math . log ( 2.0 ), dtype = boxes . dtype ) k = tf . cast ( k , tf . int32 ) k = tf . clip_by_value ( k , 0 , num_level - 1 ) k = tf . reshape ( k , [ - 1 ]) return k Functions assign_pyramid_level_to_boxes def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ) Compute the pyramid level of an RoI Arguments: boxes : A tensor of shape [nb_batches * nb_boxes, 4] num_level : Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. View Source def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ): \"\"\"Compute the pyramid level of an RoI Arguments: - *boxes*: A tensor of shape [nb_batches * nb_boxes, 4] - *num_level*: Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. \"\"\" denominator = tf . constant ( 224 , dtype = boxes . dtype ) area = compute_area ( boxes ) k = level_target + tf . math . log ( tf . sqrt ( area ) / denominator + K . epsilon ()) * tf . cast ( 1. / tf . math . log ( 2.0 ), dtype = boxes . dtype ) k = tf . cast ( k , tf . int32 ) k = tf . clip_by_value ( k , 0 , num_level - 1 ) k = tf . reshape ( k , [ - 1 ]) return k match_boxes_to_their_pyramid_level def match_boxes_to_their_pyramid_level ( boxes , num_level ) Match the boxes to the proper level based on their area Arguments: boxes : A tensor of shape [batch_size, num_boxes, 4] num_level : Number of level of the target pyramid Returns: boxes_per_level : A list of 2-D tensor and shape [N, 4] box_indices_per_level : A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. original_pos_per_level A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. View Source def match_boxes_to_their_pyramid_level ( boxes , num_level ) : \"\"\"Match the boxes to the proper level based on their area Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, 4] - *num_level*: Number of level of the target pyramid Returns: - *boxes_per_level*: A list of 2-D tensor and shape [N, 4] - *box_indices_per_level*: A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. - *original_pos_per_level* A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. \"\"\" def _select_level ( tensors , levels ) : return [ tf.squeeze(tf.gather(tensors, selected_level), 1) for selected_level in levels ] batch_size = tf . shape ( boxes ) [ 0 ] num_boxes = tf . shape ( boxes ) [ 1 ] boxes = tf . reshape ( boxes , ( - 1 , 4 )) box_levels = assign_pyramid_level_to_boxes ( boxes , num_level ) box_indices = tf . tile ( tf . expand_dims ( tf . range ( 0 , batch_size ), 1 ), [ 1, num_boxes ] ) box_indices = tf . reshape ( box_indices , ( - 1 ,)) box_original_pos = tf . range ( batch_size * num_boxes ) levels = [ tf.where(tf.equal(box_levels, i)) for i in range(num_level) ] boxes_per_level = _select_level ( boxes , levels ) box_indices_per_level = _select_level ( box_indices , levels ) original_pos_per_level = _select_level ( box_original_pos , levels ) return boxes_per_level , box_indices_per_level , original_pos_per_level multilevel_roi_align def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ) Perform a batch multilevel roi_align on the inputs Arguments: inputs : A list of tensors of shape [batch_size, width, height, channel] representing the pyramid. boxes : A tensor and shape [batch_size, num_boxes, (y1, x1, y2, x2)] image_shape : A tuple with the height and the width of the original image input image Returns: A tensor and shape [batch_size * num_boxes, 7, 7, channel] View Source def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ) : \"\"\" Perform a batch multilevel roi_align on the inputs Arguments : - * inputs * : A list of tensors of shape [ batch_size , width , height , channel ] representing the pyramid . - * boxes * : A tensor and shape [ batch_size , num_boxes , ( y1 , x1 , y2 , x2 ) ] - * image_shape * : A tuple with the height and the width of the original image input image Returns : A tensor and shape [ batch_size * num_boxes , 7 , 7 , channel ] \"\"\" boxes_per_level , box_indices_per_level , pos_per_level = match_boxes_to_their_pyramid_level ( boxes , len ( inputs )) tensors_per_level = [] for tensor , target_boxes , box_indices in zip ( inputs , boxes_per_level , box_indices_per_level ) : tensors_per_level . append ( roi_align ( tensor , target_boxes , box_indices , image_shape , crop_size )) tensors = tf . concat ( values = tensors_per_level , axis = 0 ) original_pos = tf . concat ( values = pos_per_level , axis = 0 ) # Reorder the tensor per batch indices_to_reorder_boxes = tf . math . invert_permutation ( original_pos ) tensors = tf . gather ( tensors , indices_to_reorder_boxes ) return tensors roi_align def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ) RoI align like operation from the paper Mask-RCNN. Parameters: Name Description inputs A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape A tuple with the height and the width of the original image input image crop_size An int representing the ouput size of the crop. Returns: Type Description None A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. View Source def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ) : \"\"\"RoI align like operation from the paper Mask-RCNN. Arguments: inputs: A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape: A tuple with the height and the width of the original image input image crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" normalized_boxes = normalize_box_coordinates ( boxes , image_shape [ 0 ] , image_shape [ 1 ] ) # Normalized the boxes to the input tensor_shape # TODO rewrite this normalization unnomarlization isn 't pretty at all tensor_shape = tf.shape(inputs)[1:3] normalized_boxes *= tf.cast(tf.tile(tensor_shape[None], [1, 2]), normalized_boxes.dtype) ret = _crop_and_resize(inputs, normalized_boxes, box_indices, crop_size * 2) return KL.AveragePooling2D(padding=' same ' )( ret )","title":"Pooling Ops"},{"location":"reference/kerod/layers/detection/pooling_ops/#module-kerodlayersdetectionpooling_ops","text":"None None View Source import tensorflow as tf import tensorflow.keras.layers as KL from tensorflow.keras import backend as K from kerod.core.box_ops import ( compute_area , normalize_box_coordinates , transform_fpcoor_for_tf ) def _crop_and_resize ( tensor , boxes , box_indices , crop_size : int ): \"\"\"Taken from tensorpack (https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/modeling/model_box.py) Aligned version of tf.image.crop_and_resize, following our definition of floating point boxes. Arguments: tensor: A 4-D tensor of shape [batch, image_height, image_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" boxes = tf . stop_gradient ( boxes ) # TF's crop_and_resize produces zeros on border. The symetric padding # allows to have a better interpolation for the boxes on the border of the image. tensor = tf . pad ( tensor , [[ 0 , 0 ], [ 1 , 1 ], [ 1 , 1 ], [ 0 , 0 ]], mode = 'SYMMETRIC' ) boxes = boxes + 1 # Height, width extraction tensor_shape = tf . shape ( tensor )[ 1 : 3 ] # The boxes should be at the size of the input tensor boxes = transform_fpcoor_for_tf ( boxes , tensor_shape , [ crop_size , crop_size ]) ret = tf . image . crop_and_resize ( tensor , tf . cast ( boxes , tf . float32 ), # crop and resize needs float32 tf . cast ( box_indices , tf . int32 ), crop_size = [ crop_size , crop_size ]) return ret def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ): \"\"\"RoI align like operation from the paper Mask-RCNN. Arguments: inputs: A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape: A tuple with the height and the width of the original image input image crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" normalized_boxes = normalize_box_coordinates ( boxes , image_shape [ 0 ], image_shape [ 1 ]) # Normalized the boxes to the input tensor_shape # TODO rewrite this normalization unnomarlization isn't pretty at all tensor_shape = tf . shape ( inputs )[ 1 : 3 ] normalized_boxes *= tf . cast ( tf . tile ( tensor_shape [ None ], [ 1 , 2 ]), normalized_boxes . dtype ) ret = _crop_and_resize ( inputs , normalized_boxes , box_indices , crop_size * 2 ) return KL . AveragePooling2D ( padding = 'same' )( ret ) def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ): \"\"\"Perform a batch multilevel roi_align on the inputs Arguments: - *inputs*: A list of tensors of shape [batch_size, width, height, channel] representing the pyramid. - *boxes*: A tensor and shape [batch_size, num_boxes, (y1, x1, y2, x2)] - *image_shape*: A tuple with the height and the width of the original image input image Returns: A tensor and shape [batch_size * num_boxes, 7, 7, channel] \"\"\" boxes_per_level , box_indices_per_level , pos_per_level = match_boxes_to_their_pyramid_level ( boxes , len ( inputs )) tensors_per_level = [] for tensor , target_boxes , box_indices in zip ( inputs , boxes_per_level , box_indices_per_level ): tensors_per_level . append ( roi_align ( tensor , target_boxes , box_indices , image_shape , crop_size )) tensors = tf . concat ( values = tensors_per_level , axis = 0 ) original_pos = tf . concat ( values = pos_per_level , axis = 0 ) # Reorder the tensor per batch indices_to_reorder_boxes = tf . math . invert_permutation ( original_pos ) tensors = tf . gather ( tensors , indices_to_reorder_boxes ) return tensors def match_boxes_to_their_pyramid_level ( boxes , num_level ): \"\"\"Match the boxes to the proper level based on their area Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, 4] - *num_level*: Number of level of the target pyramid Returns: - *boxes_per_level*: A list of 2-D tensor and shape [N, 4] - *box_indices_per_level*: A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. - *original_pos_per_level* A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. \"\"\" def _select_level ( tensors , levels ): return [ tf . squeeze ( tf . gather ( tensors , selected_level ), 1 ) for selected_level in levels ] batch_size = tf . shape ( boxes )[ 0 ] num_boxes = tf . shape ( boxes )[ 1 ] boxes = tf . reshape ( boxes , ( - 1 , 4 )) box_levels = assign_pyramid_level_to_boxes ( boxes , num_level ) box_indices = tf . tile ( tf . expand_dims ( tf . range ( 0 , batch_size ), 1 ), [ 1 , num_boxes ]) box_indices = tf . reshape ( box_indices , ( - 1 ,)) box_original_pos = tf . range ( batch_size * num_boxes ) levels = [ tf . where ( tf . equal ( box_levels , i )) for i in range ( num_level )] boxes_per_level = _select_level ( boxes , levels ) box_indices_per_level = _select_level ( box_indices , levels ) original_pos_per_level = _select_level ( box_original_pos , levels ) return boxes_per_level , box_indices_per_level , original_pos_per_level def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ): \"\"\"Compute the pyramid level of an RoI Arguments: - *boxes*: A tensor of shape [nb_batches * nb_boxes, 4] - *num_level*: Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. \"\"\" denominator = tf . constant ( 224 , dtype = boxes . dtype ) area = compute_area ( boxes ) k = level_target + tf . math . log ( tf . sqrt ( area ) / denominator + K . epsilon ()) * tf . cast ( 1. / tf . math . log ( 2.0 ), dtype = boxes . dtype ) k = tf . cast ( k , tf . int32 ) k = tf . clip_by_value ( k , 0 , num_level - 1 ) k = tf . reshape ( k , [ - 1 ]) return k","title":"Module kerod.layers.detection.pooling_ops"},{"location":"reference/kerod/layers/detection/pooling_ops/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/detection/pooling_ops/#assign_pyramid_level_to_boxes","text":"def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ) Compute the pyramid level of an RoI Arguments: boxes : A tensor of shape [nb_batches * nb_boxes, 4] num_level : Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. View Source def assign_pyramid_level_to_boxes ( boxes , num_level , level_target = 2 ): \"\"\"Compute the pyramid level of an RoI Arguments: - *boxes*: A tensor of shape [nb_batches * nb_boxes, 4] - *num_level*: Assign all the boxes mapped to a superior level to the num_level. level_target: Will affect all the boxes of area 224^2 to the level_target of the pyramid. Returns: A 2-D tensor of type int32 and shape [nb_batches * nb_boxes] corresponding to the target level of the pyramid. \"\"\" denominator = tf . constant ( 224 , dtype = boxes . dtype ) area = compute_area ( boxes ) k = level_target + tf . math . log ( tf . sqrt ( area ) / denominator + K . epsilon ()) * tf . cast ( 1. / tf . math . log ( 2.0 ), dtype = boxes . dtype ) k = tf . cast ( k , tf . int32 ) k = tf . clip_by_value ( k , 0 , num_level - 1 ) k = tf . reshape ( k , [ - 1 ]) return k","title":"assign_pyramid_level_to_boxes"},{"location":"reference/kerod/layers/detection/pooling_ops/#match_boxes_to_their_pyramid_level","text":"def match_boxes_to_their_pyramid_level ( boxes , num_level ) Match the boxes to the proper level based on their area Arguments: boxes : A tensor of shape [batch_size, num_boxes, 4] num_level : Number of level of the target pyramid Returns: boxes_per_level : A list of 2-D tensor and shape [N, 4] box_indices_per_level : A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. original_pos_per_level A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. View Source def match_boxes_to_their_pyramid_level ( boxes , num_level ) : \"\"\"Match the boxes to the proper level based on their area Arguments: - *boxes*: A tensor of shape [batch_size, num_boxes, 4] - *num_level*: Number of level of the target pyramid Returns: - *boxes_per_level*: A list of 2-D tensor and shape [N, 4] - *box_indices_per_level*: A list of 1-D tensor with int32 values in [0, batch). The value of a box_indices_per_level[lvl][i] specifies the image that the i-th box refers to. - *original_pos_per_level* A list of 1-D tensor with int32 values in [0, num_boxes * batch_size) It will be useful to reorder our boxes after the roi_align operation. \"\"\" def _select_level ( tensors , levels ) : return [ tf.squeeze(tf.gather(tensors, selected_level), 1) for selected_level in levels ] batch_size = tf . shape ( boxes ) [ 0 ] num_boxes = tf . shape ( boxes ) [ 1 ] boxes = tf . reshape ( boxes , ( - 1 , 4 )) box_levels = assign_pyramid_level_to_boxes ( boxes , num_level ) box_indices = tf . tile ( tf . expand_dims ( tf . range ( 0 , batch_size ), 1 ), [ 1, num_boxes ] ) box_indices = tf . reshape ( box_indices , ( - 1 ,)) box_original_pos = tf . range ( batch_size * num_boxes ) levels = [ tf.where(tf.equal(box_levels, i)) for i in range(num_level) ] boxes_per_level = _select_level ( boxes , levels ) box_indices_per_level = _select_level ( box_indices , levels ) original_pos_per_level = _select_level ( box_original_pos , levels ) return boxes_per_level , box_indices_per_level , original_pos_per_level","title":"match_boxes_to_their_pyramid_level"},{"location":"reference/kerod/layers/detection/pooling_ops/#multilevel_roi_align","text":"def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ) Perform a batch multilevel roi_align on the inputs Arguments: inputs : A list of tensors of shape [batch_size, width, height, channel] representing the pyramid. boxes : A tensor and shape [batch_size, num_boxes, (y1, x1, y2, x2)] image_shape : A tuple with the height and the width of the original image input image Returns: A tensor and shape [batch_size * num_boxes, 7, 7, channel] View Source def multilevel_roi_align ( inputs , boxes , image_shape , crop_size : int = 7 ) : \"\"\" Perform a batch multilevel roi_align on the inputs Arguments : - * inputs * : A list of tensors of shape [ batch_size , width , height , channel ] representing the pyramid . - * boxes * : A tensor and shape [ batch_size , num_boxes , ( y1 , x1 , y2 , x2 ) ] - * image_shape * : A tuple with the height and the width of the original image input image Returns : A tensor and shape [ batch_size * num_boxes , 7 , 7 , channel ] \"\"\" boxes_per_level , box_indices_per_level , pos_per_level = match_boxes_to_their_pyramid_level ( boxes , len ( inputs )) tensors_per_level = [] for tensor , target_boxes , box_indices in zip ( inputs , boxes_per_level , box_indices_per_level ) : tensors_per_level . append ( roi_align ( tensor , target_boxes , box_indices , image_shape , crop_size )) tensors = tf . concat ( values = tensors_per_level , axis = 0 ) original_pos = tf . concat ( values = pos_per_level , axis = 0 ) # Reorder the tensor per batch indices_to_reorder_boxes = tf . math . invert_permutation ( original_pos ) tensors = tf . gather ( tensors , indices_to_reorder_boxes ) return tensors","title":"multilevel_roi_align"},{"location":"reference/kerod/layers/detection/pooling_ops/#roi_align","text":"def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ) RoI align like operation from the paper Mask-RCNN. Parameters: Name Description inputs A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape A tuple with the height and the width of the original image input image crop_size An int representing the ouput size of the crop. Returns: Type Description None A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. View Source def roi_align ( inputs , boxes , box_indices , image_shape , crop_size : int ) : \"\"\"RoI align like operation from the paper Mask-RCNN. Arguments: inputs: A 4-D tensor of shape [batch, height, tensor_width, depth]. Both image_height and image_width need to be positive. boxes: A 2-D tensor of shape [num_boxes, 4]. The i-th row of the tensor specifies the coordinates of a box in the box_ind[i] image and is specified in normalized coordinates [y1, x1, y2, x2]. A normalized coordinate value of y is mapped to the image coordinate at y * (image_height - 1), so as the [0, 1] interval of normalized image height is mapped to [0, image_height - 1] in image height coordinates. We do allow y1 > y2, in which case the sampled crop is an up-down flipped version of the original image. The width dimension is treated similarly. Normalized coordinates outside the [0, 1] range are allowed, in which case we use extrapolation_value to extrapolate the input image values. box_indices: A 1-D tensor of shape [num_boxes] with int32 values in [0, batch). The value of box_ind[i] specifies the image that the i-th box refers to. image_shape: A tuple with the height and the width of the original image input image crop_size: An int representing the ouput size of the crop. Returns: A 4-D tensor of shape [num_boxes, crop_height, crop_width, depth]. \"\"\" normalized_boxes = normalize_box_coordinates ( boxes , image_shape [ 0 ] , image_shape [ 1 ] ) # Normalized the boxes to the input tensor_shape # TODO rewrite this normalization unnomarlization isn 't pretty at all tensor_shape = tf.shape(inputs)[1:3] normalized_boxes *= tf.cast(tf.tile(tensor_shape[None], [1, 2]), normalized_boxes.dtype) ret = _crop_and_resize(inputs, normalized_boxes, box_indices, crop_size * 2) return KL.AveragePooling2D(padding=' same ' )( ret )","title":"roi_align"},{"location":"reference/kerod/layers/detection/rpn/","text":"Module kerod.layers.detection.rpn None None View Source from typing import List import tensorflow as tf import tensorflow.keras.layers as KL from tensorflow.keras import initializers from tensorflow.keras.losses import SparseCategoricalCrossentropy from kerod.core.box_coder import encode_boxes_faster_rcnn from kerod.core.losses import L1Loss from kerod.core.matcher import Matcher from kerod.core.sampling_ops import batch_sample_balanced_positive_negative from kerod.core.similarity import IoUSimilarity from kerod.core.standard_fields import BoxField from kerod.core.target_assigner import TargetAssigner from kerod.layers import Anchors from kerod.layers.detection.abstract_detection_head import \\ AbstractDetectionHead from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} SAMPLING_SIZE = 256 SAMPLING_POSITIVE_RATIO = 0.5 class RegionProposalNetwork ( AbstractDetectionHead ): \"\"\"It has been introduced in the [Faster R-CNN paper](https://arxiv.org/abs/1506.01497) and use the parameters from [Feature Pyramidal Networks for Object Detection](https://arxiv.org/abs/1612.03144). Arguments: anchor_ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) Call arguments: inputs: A List of tensors the output of the pyramid Call returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\"\" def __init__ ( self , anchor_ratios = ( 0.5 , 1 , 2 ), ** kwargs ): super () . __init__ ( 2 , SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ), L1Loss ( reduction = tf . keras . losses . Reduction . NONE ), multiples = len ( anchor_ratios ), kernel_initializer_classification_head = initializers . RandomNormal ( stddev = 0.01 ), kernel_initializer_box_prediction_head = initializers . RandomNormal ( stddev = 0.01 ), ** kwargs ) #Force each ground_truths to match to at least one anchor matcher = Matcher ([ 0.3 , 0.7 ], [ 0 , - 1 , 1 ], allow_low_quality_matches = True ) self . target_assigner = TargetAssigner ( IoUSimilarity (), matcher , encode_boxes_faster_rcnn , dtype = self . _compute_dtype ) anchor_strides = ( 4 , 8 , 16 , 32 , 64 ) anchor_zises = ( 32 , 64 , 128 , 256 , 512 ) self . _anchor_ratios = anchor_ratios # Precompute a deterministic grid of anchors for each layer of the pyramid. # We will extract a subpart of the anchors according to self . _anchors = [ Anchors ( stride , size , self . _anchor_ratios ) for stride , size in zip ( anchor_strides , anchor_zises ) ] def build ( self , input_shape ): self . rpn_conv2d = KL . Conv2D ( 512 , ( 3 , 3 ), padding = 'same' , kernel_initializer = self . _kernel_initializer_classification_head , kernel_regularizer = self . _kernel_regularizer ) super () . build ( input_shape ) def build_rpn_head ( self , inputs ): \"\"\"Predictions for the classification and the regression Arguments: inputs: A tensor of shape [batch_size, width, height, channel] Returns: Tuple: classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] \"\"\" batch_size = tf . shape ( inputs )[ 0 ] rpn_conv2d = self . rpn_conv2d ( inputs ) classification_head , localization_head = self . build_detection_head ( rpn_conv2d ) classification_head = tf . reshape ( classification_head , ( batch_size , - 1 , 2 )) localization_head = tf . reshape ( localization_head , ( batch_size , - 1 , 4 )) return classification_head , localization_head def call ( self , inputs : List [ tf . Tensor ]): \"\"\"Create the computation graph for the rpn inference Arguments: inputs: A List of tensors the output of the pyramid Returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\"\" anchors = [ anchors ( tensor ) for tensor , anchors in zip ( inputs , self . _anchors )] rpn_predictions = [ self . build_rpn_head ( tensor ) for tensor in inputs ] localization_pred = [ prediction [ 1 ] for prediction in rpn_predictions ] classification_pred = [ prediction [ 0 ] for prediction in rpn_predictions ] return localization_pred , classification_pred , anchors def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ): \"\"\"Compute the loss Arguments: localization_pred: A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred: A list of tensors of shape [batch_size, num_anchors, 2] anchors: A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Tuple: - `classification_loss`: A scalar in tf.float32 - `localization_loss`: A scalar in tf.float32 \"\"\" localization_pred = tf . concat ( localization_pred , 1 ) classification_pred = tf . concat ( classification_pred , 1 ) anchors = tf . concat ( anchors , 0 ) ground_truths = { # We add one because the background is not counted in ground_truths[BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } # anchors are deterministic duplicate them to create a batch anchors = tf . tile ( anchors [ None ], ( tf . shape ( ground_truths [ BoxField . BOXES ])[ 0 ], 1 , 1 )) y_true , weights = self . target_assigner . assign ({ BoxField . BOXES : anchors }, ground_truths ) y_true [ BoxField . LABELS ] = tf . minimum ( y_true [ BoxField . LABELS ], 1 ) ## Compute metrics recall = compute_rpn_metrics ( y_true [ BoxField . LABELS ], classification_pred , weights [ BoxField . LABELS ]) self . add_metric ( recall , name = 'rpn_recall' , aggregation = 'mean' ) # All the boxes which are not -1 can be sampled labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ], SAMPLING_SIZE , labels , positive_fraction = SAMPLING_POSITIVE_RATIO , dtype = self . _compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : localization_pred } return self . compute_losses ( y_true , y_pred , weights ) def get_config ( self ): base_config = super () . get_config () base_config [ 'anchor_ratios' ] = self . _anchor_ratios return base_config def compute_rpn_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor , weights : tf . Tensor ): \"\"\"Useful metrics that allows to track how behave the training of the rpn head. Arguments: y_true: A tensor vector with shape [batch_size, num_anchors] where 0 = background and 1 = foreground. y_pred: A tensor of shape [batch_size, num_anchors, 2], representing the classification logits. weights: A tensor of shape [batch_size, num_anchors] where weights should Returns: tf.Tensor: Recall, among all the boxes that we had to find how much did we found. \"\"\" # Force the cast to avoid type issue when the mixed precision is activated y_true , y_pred , weights = tf . cast ( y_true , tf . float32 ), tf . cast ( y_pred , tf . float32 ), tf . cast ( weights , tf . float32 ) # Sometimes the weights have decimal value we do not want that weights = tf . clip_by_value ( tf . math . ceil ( weights ), 0 , 1 ) masked_y_true = y_true * weights prediction = tf . cast ( tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' ), tf . float32 ) * weights # 0 or 1 correct = tf . cast ( tf . equal ( prediction , masked_y_true ), tf . float32 ) fg_inds = tf . where ( masked_y_true == 1 ) num_valid_anchor = tf . math . count_nonzero ( masked_y_true ) num_pos_foreground_prediction = tf . math . count_nonzero ( tf . gather_nd ( correct , fg_inds )) recall = tf . truediv ( num_pos_foreground_prediction , num_valid_anchor , name = 'recall' ) return recall remove_unwanted_doc ( RegionProposalNetwork , __pdoc__ ) Variables SAMPLING_POSITIVE_RATIO SAMPLING_SIZE Functions compute_rpn_metrics def compute_rpn_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor , weights : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training of the rpn head. Parameters: Name Description y_true A tensor vector with shape [batch_size, num_anchors] where 0 = background and 1 = foreground. y_pred A tensor of shape [batch_size, num_anchors, 2], representing the classification logits. weights A tensor of shape [batch_size, num_anchors] where weights should Returns: Type Description tf.Tensor Recall, among all the boxes that we had to find how much did we found. View Source def compute_rpn_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor , weights: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training of the rpn head. Arguments: y_true: A tensor vector with shape [ batch_size , num_anchors ] where 0 = background and 1 = foreground . y_pred: A tensor of shape [ batch_size , num_anchors , 2 ], representing the classification logits . weights: A tensor of shape [ batch_size , num_anchors ] where weights should Returns: tf . Tensor: Recall , among all the boxes that we had to find how much did we found . \"\"\" # Force the cast to avoid type issue when the mixed precision is activated y_true , y_pred , weights = tf . cast ( y_true , tf . float32 ), tf . cast ( y_pred , tf . float32 ), tf . cast ( weights , tf . float32 ) # Sometimes the weights have decimal value we do not want that weights = tf . clip_by_value ( tf . math . ceil ( weights ), 0 , 1 ) masked_y_true = y_true * weights prediction = tf . cast ( tf . argmax ( y_pred , axis =- 1 , name = ' label_prediction '), tf . float32 ) * weights # 0 or 1 correct = tf . cast ( tf . equal ( prediction , masked_y_true ), tf . float32 ) fg_inds = tf . where ( masked_y_true == 1 ) num_valid_anchor = tf . math . count_nonzero ( masked_y_true ) num_pos_foreground_prediction = tf . math . count_nonzero ( tf . gather_nd ( correct , fg_inds )) recall = tf . truediv ( num_pos_foreground_prediction , num_valid_anchor , name = ' recall ') return recall Classes RegionProposalNetwork class RegionProposalNetwork ( anchor_ratios = ( 0.5 , 1 , 2 ), ** kwargs ) use the parameters from Feature Pyramidal Networks for Object Detection . Arguments Name Description anchor_ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) Call arguments Name Description inputs A List of tensors the output of the pyramid Call returns Type Description Tuple - localization_pred : A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - classification_pred : A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - anchors : A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] Ancestors (in MRO) kerod.layers.detection.abstract_detection_head.AbstractDetectionHead tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods build_detection_head def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head build_rpn_head def build_rpn_head ( self , inputs ) Predictions for the classification and the regression Parameters: Name Description inputs A tensor of shape [batch_size, width, height, channel] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_rpn_head ( self , inputs ) : \"\"\"Predictions for the classification and the regression Arguments: inputs: A tensor of shape [ batch_size , width , height , channel ] Returns: Tuple: classification_head: a tensor of shape [ batch_size , num_anchors , 2 ] localization_head: a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" batch_size = tf . shape ( inputs )[ 0 ] rpn_conv2d = self . rpn_conv2d ( inputs ) classification_head , localization_head = self . build_detection_head ( rpn_conv2d ) classification_head = tf . reshape ( classification_head , ( batch_size , - 1 , 2 )) localization_head = tf . reshape ( localization_head , ( batch_size , - 1 , 4 )) return classification_head , localization_head build_segmentation_head def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x call def call ( self , inputs : List [ tensorflow . python . framework . ops . Tensor ] ) Create the computation graph for the rpn inference Parameters: Name Description inputs A List of tensors the output of the pyramid Returns: Type Description Tuple - localization_pred : A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - classification_pred : A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - anchors : A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] View Source def call ( self , inputs : List [ tf . Tensor ] ) : \" \"\" Create the computation graph for the rpn inference Arguments: inputs: A List of tensors the output of the pyramid Returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\" \" anchors = [ anchors ( tensor ) for tensor , anchors in zip ( inputs , self . _anchors ) ] rpn_predictions = [ self . build_rpn_head ( tensor ) for tensor in inputs ] localization_pred = [ prediction [ 1 ] for prediction in rpn_predictions ] classification_pred = [ prediction [ 0 ] for prediction in rpn_predictions ] return localization_pred , classification_pred , anchors compute_loss def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ) Compute the loss Parameters: Name Description localization_pred A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred A list of tensors of shape [batch_size, num_anchors, 2] anchors A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Type Description Tuple - classification_loss : A scalar in tf.float32 - localization_loss : A scalar in tf.float32 View Source def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ) : \" \"\" Compute the loss Arguments: localization_pred: A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred: A list of tensors of shape [batch_size, num_anchors, 2] anchors: A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Tuple: - `classification_loss`: A scalar in tf.float32 - `localization_loss`: A scalar in tf.float32 \"\" \" localization_pred = tf . concat ( localization_pred , 1 ) classification_pred = tf . concat ( classification_pred , 1 ) anchors = tf . concat ( anchors , 0 ) ground_truths = { # We add one because the background is not counted in ground_truths[BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ] , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } # anchors are deterministic duplicate them to create a batch anchors = tf . tile ( anchors [ None ] , ( tf . shape ( ground_truths [ BoxField . BOXES ] ) [ 0 ] , 1 , 1 )) y_true , weights = self . target_assigner . assign ( { BoxField . BOXES : anchors } , ground_truths ) y_true [ BoxField . LABELS ] = tf . minimum ( y_true [ BoxField . LABELS ] , 1 ) ## Compute metrics recall = compute_rpn_metrics ( y_true [ BoxField . LABELS ] , classification_pred , weights [ BoxField . LABELS ] ) self . add_metric ( recall , name = 'rpn_recall' , aggregation = 'mean' ) # All the boxes which are not -1 can be sampled labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ] , SAMPLING_SIZE , labels , positive_fraction = SAMPLING_POSITIVE_RATIO , dtype = self . _compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : localization_pred } return self . compute_losses ( y_true , y_pred , weights ) compute_losses def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"Rpn"},{"location":"reference/kerod/layers/detection/rpn/#module-kerodlayersdetectionrpn","text":"None None View Source from typing import List import tensorflow as tf import tensorflow.keras.layers as KL from tensorflow.keras import initializers from tensorflow.keras.losses import SparseCategoricalCrossentropy from kerod.core.box_coder import encode_boxes_faster_rcnn from kerod.core.losses import L1Loss from kerod.core.matcher import Matcher from kerod.core.sampling_ops import batch_sample_balanced_positive_negative from kerod.core.similarity import IoUSimilarity from kerod.core.standard_fields import BoxField from kerod.core.target_assigner import TargetAssigner from kerod.layers import Anchors from kerod.layers.detection.abstract_detection_head import \\ AbstractDetectionHead from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} SAMPLING_SIZE = 256 SAMPLING_POSITIVE_RATIO = 0.5 class RegionProposalNetwork ( AbstractDetectionHead ): \"\"\"It has been introduced in the [Faster R-CNN paper](https://arxiv.org/abs/1506.01497) and use the parameters from [Feature Pyramidal Networks for Object Detection](https://arxiv.org/abs/1612.03144). Arguments: anchor_ratios: The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2) Call arguments: inputs: A List of tensors the output of the pyramid Call returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\"\" def __init__ ( self , anchor_ratios = ( 0.5 , 1 , 2 ), ** kwargs ): super () . __init__ ( 2 , SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ), L1Loss ( reduction = tf . keras . losses . Reduction . NONE ), multiples = len ( anchor_ratios ), kernel_initializer_classification_head = initializers . RandomNormal ( stddev = 0.01 ), kernel_initializer_box_prediction_head = initializers . RandomNormal ( stddev = 0.01 ), ** kwargs ) #Force each ground_truths to match to at least one anchor matcher = Matcher ([ 0.3 , 0.7 ], [ 0 , - 1 , 1 ], allow_low_quality_matches = True ) self . target_assigner = TargetAssigner ( IoUSimilarity (), matcher , encode_boxes_faster_rcnn , dtype = self . _compute_dtype ) anchor_strides = ( 4 , 8 , 16 , 32 , 64 ) anchor_zises = ( 32 , 64 , 128 , 256 , 512 ) self . _anchor_ratios = anchor_ratios # Precompute a deterministic grid of anchors for each layer of the pyramid. # We will extract a subpart of the anchors according to self . _anchors = [ Anchors ( stride , size , self . _anchor_ratios ) for stride , size in zip ( anchor_strides , anchor_zises ) ] def build ( self , input_shape ): self . rpn_conv2d = KL . Conv2D ( 512 , ( 3 , 3 ), padding = 'same' , kernel_initializer = self . _kernel_initializer_classification_head , kernel_regularizer = self . _kernel_regularizer ) super () . build ( input_shape ) def build_rpn_head ( self , inputs ): \"\"\"Predictions for the classification and the regression Arguments: inputs: A tensor of shape [batch_size, width, height, channel] Returns: Tuple: classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] \"\"\" batch_size = tf . shape ( inputs )[ 0 ] rpn_conv2d = self . rpn_conv2d ( inputs ) classification_head , localization_head = self . build_detection_head ( rpn_conv2d ) classification_head = tf . reshape ( classification_head , ( batch_size , - 1 , 2 )) localization_head = tf . reshape ( localization_head , ( batch_size , - 1 , 4 )) return classification_head , localization_head def call ( self , inputs : List [ tf . Tensor ]): \"\"\"Create the computation graph for the rpn inference Arguments: inputs: A List of tensors the output of the pyramid Returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\"\" anchors = [ anchors ( tensor ) for tensor , anchors in zip ( inputs , self . _anchors )] rpn_predictions = [ self . build_rpn_head ( tensor ) for tensor in inputs ] localization_pred = [ prediction [ 1 ] for prediction in rpn_predictions ] classification_pred = [ prediction [ 0 ] for prediction in rpn_predictions ] return localization_pred , classification_pred , anchors def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ): \"\"\"Compute the loss Arguments: localization_pred: A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred: A list of tensors of shape [batch_size, num_anchors, 2] anchors: A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Tuple: - `classification_loss`: A scalar in tf.float32 - `localization_loss`: A scalar in tf.float32 \"\"\" localization_pred = tf . concat ( localization_pred , 1 ) classification_pred = tf . concat ( classification_pred , 1 ) anchors = tf . concat ( anchors , 0 ) ground_truths = { # We add one because the background is not counted in ground_truths[BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ], BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } # anchors are deterministic duplicate them to create a batch anchors = tf . tile ( anchors [ None ], ( tf . shape ( ground_truths [ BoxField . BOXES ])[ 0 ], 1 , 1 )) y_true , weights = self . target_assigner . assign ({ BoxField . BOXES : anchors }, ground_truths ) y_true [ BoxField . LABELS ] = tf . minimum ( y_true [ BoxField . LABELS ], 1 ) ## Compute metrics recall = compute_rpn_metrics ( y_true [ BoxField . LABELS ], classification_pred , weights [ BoxField . LABELS ]) self . add_metric ( recall , name = 'rpn_recall' , aggregation = 'mean' ) # All the boxes which are not -1 can be sampled labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ], SAMPLING_SIZE , labels , positive_fraction = SAMPLING_POSITIVE_RATIO , dtype = self . _compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : localization_pred } return self . compute_losses ( y_true , y_pred , weights ) def get_config ( self ): base_config = super () . get_config () base_config [ 'anchor_ratios' ] = self . _anchor_ratios return base_config def compute_rpn_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor , weights : tf . Tensor ): \"\"\"Useful metrics that allows to track how behave the training of the rpn head. Arguments: y_true: A tensor vector with shape [batch_size, num_anchors] where 0 = background and 1 = foreground. y_pred: A tensor of shape [batch_size, num_anchors, 2], representing the classification logits. weights: A tensor of shape [batch_size, num_anchors] where weights should Returns: tf.Tensor: Recall, among all the boxes that we had to find how much did we found. \"\"\" # Force the cast to avoid type issue when the mixed precision is activated y_true , y_pred , weights = tf . cast ( y_true , tf . float32 ), tf . cast ( y_pred , tf . float32 ), tf . cast ( weights , tf . float32 ) # Sometimes the weights have decimal value we do not want that weights = tf . clip_by_value ( tf . math . ceil ( weights ), 0 , 1 ) masked_y_true = y_true * weights prediction = tf . cast ( tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' ), tf . float32 ) * weights # 0 or 1 correct = tf . cast ( tf . equal ( prediction , masked_y_true ), tf . float32 ) fg_inds = tf . where ( masked_y_true == 1 ) num_valid_anchor = tf . math . count_nonzero ( masked_y_true ) num_pos_foreground_prediction = tf . math . count_nonzero ( tf . gather_nd ( correct , fg_inds )) recall = tf . truediv ( num_pos_foreground_prediction , num_valid_anchor , name = 'recall' ) return recall remove_unwanted_doc ( RegionProposalNetwork , __pdoc__ )","title":"Module kerod.layers.detection.rpn"},{"location":"reference/kerod/layers/detection/rpn/#variables","text":"SAMPLING_POSITIVE_RATIO SAMPLING_SIZE","title":"Variables"},{"location":"reference/kerod/layers/detection/rpn/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/detection/rpn/#compute_rpn_metrics","text":"def compute_rpn_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor , weights : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training of the rpn head. Parameters: Name Description y_true A tensor vector with shape [batch_size, num_anchors] where 0 = background and 1 = foreground. y_pred A tensor of shape [batch_size, num_anchors, 2], representing the classification logits. weights A tensor of shape [batch_size, num_anchors] where weights should Returns: Type Description tf.Tensor Recall, among all the boxes that we had to find how much did we found. View Source def compute_rpn_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor , weights: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training of the rpn head. Arguments: y_true: A tensor vector with shape [ batch_size , num_anchors ] where 0 = background and 1 = foreground . y_pred: A tensor of shape [ batch_size , num_anchors , 2 ], representing the classification logits . weights: A tensor of shape [ batch_size , num_anchors ] where weights should Returns: tf . Tensor: Recall , among all the boxes that we had to find how much did we found . \"\"\" # Force the cast to avoid type issue when the mixed precision is activated y_true , y_pred , weights = tf . cast ( y_true , tf . float32 ), tf . cast ( y_pred , tf . float32 ), tf . cast ( weights , tf . float32 ) # Sometimes the weights have decimal value we do not want that weights = tf . clip_by_value ( tf . math . ceil ( weights ), 0 , 1 ) masked_y_true = y_true * weights prediction = tf . cast ( tf . argmax ( y_pred , axis =- 1 , name = ' label_prediction '), tf . float32 ) * weights # 0 or 1 correct = tf . cast ( tf . equal ( prediction , masked_y_true ), tf . float32 ) fg_inds = tf . where ( masked_y_true == 1 ) num_valid_anchor = tf . math . count_nonzero ( masked_y_true ) num_pos_foreground_prediction = tf . math . count_nonzero ( tf . gather_nd ( correct , fg_inds )) recall = tf . truediv ( num_pos_foreground_prediction , num_valid_anchor , name = ' recall ') return recall","title":"compute_rpn_metrics"},{"location":"reference/kerod/layers/detection/rpn/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/detection/rpn/#regionproposalnetwork","text":"class RegionProposalNetwork ( anchor_ratios = ( 0.5 , 1 , 2 ), ** kwargs ) use the parameters from Feature Pyramidal Networks for Object Detection .","title":"RegionProposalNetwork"},{"location":"reference/kerod/layers/detection/rpn/#arguments","text":"Name Description anchor_ratios The ratios are the different shapes that you want to apply on your anchors. e.g: (0.5, 1, 2)","title":"Arguments"},{"location":"reference/kerod/layers/detection/rpn/#call-arguments","text":"Name Description inputs A List of tensors the output of the pyramid","title":"Call arguments"},{"location":"reference/kerod/layers/detection/rpn/#call-returns","text":"Type Description Tuple - localization_pred : A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - classification_pred : A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - anchors : A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)]","title":"Call returns"},{"location":"reference/kerod/layers/detection/rpn/#ancestors-in-mro","text":"kerod.layers.detection.abstract_detection_head.AbstractDetectionHead tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/detection/rpn/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/detection/rpn/#build_detection_head","text":"def build_detection_head ( self , inputs ) Build a detection head composed of a classification and box_detection. Parameters: Name Description inputs A tensor of shape [batch_size, H, W, C] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_detection_head ( self , inputs ) : \"\"\" Build a detection head composed of a classification and box_detection. Arguments : inputs : A tensor of shape [ batch_size , H , W , C ] Returns : Tuple : classification_head : a tensor of shape [ batch_size , num_anchors , 2 ] localization_head : a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" classification_head = self . _conv_classification_head ( inputs ) box_prediction_head = self . _conv_box_prediction_head ( inputs ) return classification_head , box_prediction_head","title":"build_detection_head"},{"location":"reference/kerod/layers/detection/rpn/#build_rpn_head","text":"def build_rpn_head ( self , inputs ) Predictions for the classification and the regression Parameters: Name Description inputs A tensor of shape [batch_size, width, height, channel] Returns: Type Description Tuple classification_head: a tensor of shape [batch_size, num_anchors, 2] localization_head: a tensor of shape [batch_size, num_anchors, 4] View Source def build_rpn_head ( self , inputs ) : \"\"\"Predictions for the classification and the regression Arguments: inputs: A tensor of shape [ batch_size , width , height , channel ] Returns: Tuple: classification_head: a tensor of shape [ batch_size , num_anchors , 2 ] localization_head: a tensor of shape [ batch_size , num_anchors , 4 ] \"\"\" batch_size = tf . shape ( inputs )[ 0 ] rpn_conv2d = self . rpn_conv2d ( inputs ) classification_head , localization_head = self . build_detection_head ( rpn_conv2d ) classification_head = tf . reshape ( classification_head , ( batch_size , - 1 , 2 )) localization_head = tf . reshape ( localization_head , ( batch_size , - 1 , 4 )) return classification_head , localization_head","title":"build_rpn_head"},{"location":"reference/kerod/layers/detection/rpn/#build_segmentation_head","text":"def build_segmentation_head ( self , inputs ) Build the detection head Parameters: Name Description inputs A tensor of float and shape [N, H, W, C] Returns: Type Description tf.Tensor A tensor and shape [N, H 2, W 2, num_classes - 1] View Source def build_segmentation_head ( self , inputs ) : \"\"\" Build the detection head Arguments : inputs : A tensor of float and shape [ N , H , W , C ] Returns : tf . Tensor : A tensor and shape [ N , H * 2 , W * 2 , num_classes - 1 ] \"\"\" x = inputs for layer in self . _segmentation_layers : x = layer ( x ) return x","title":"build_segmentation_head"},{"location":"reference/kerod/layers/detection/rpn/#call","text":"def call ( self , inputs : List [ tensorflow . python . framework . ops . Tensor ] ) Create the computation graph for the rpn inference Parameters: Name Description inputs A List of tensors the output of the pyramid Returns: Type Description Tuple - localization_pred : A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - classification_pred : A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - anchors : A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] View Source def call ( self , inputs : List [ tf . Tensor ] ) : \" \"\" Create the computation graph for the rpn inference Arguments: inputs: A List of tensors the output of the pyramid Returns: Tuple: - `localization_pred`: A list of logits 3-D tensor of shape [batch_size, num_anchors, 4] - `classification_pred`: A lost of logits 3-D tensor of shape [batch_size, num_anchors, 2] - `anchors`: A list of tensors of shape [batch_size, num_anchors, (y_min, x_min, y_max, x_max)] \"\" \" anchors = [ anchors ( tensor ) for tensor , anchors in zip ( inputs , self . _anchors ) ] rpn_predictions = [ self . build_rpn_head ( tensor ) for tensor in inputs ] localization_pred = [ prediction [ 1 ] for prediction in rpn_predictions ] classification_pred = [ prediction [ 0 ] for prediction in rpn_predictions ] return localization_pred , classification_pred , anchors","title":"call"},{"location":"reference/kerod/layers/detection/rpn/#compute_loss","text":"def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ) Compute the loss Parameters: Name Description localization_pred A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred A list of tensors of shape [batch_size, num_anchors, 2] anchors A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Type Description Tuple - classification_loss : A scalar in tf.float32 - localization_loss : A scalar in tf.float32 View Source def compute_loss ( self , localization_pred , classification_pred , anchors , ground_truths ) : \" \"\" Compute the loss Arguments: localization_pred: A list of tensors of shape [batch_size, num_anchors, 4]. classification_pred: A list of tensors of shape [batch_size, num_anchors, 2] anchors: A list of tensors of shape [num_anchors, (y_min, x_min, y_max, x_max)] ground_truths: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 Returns: Tuple: - `classification_loss`: A scalar in tf.float32 - `localization_loss`: A scalar in tf.float32 \"\" \" localization_pred = tf . concat ( localization_pred , 1 ) classification_pred = tf . concat ( classification_pred , 1 ) anchors = tf . concat ( anchors , 0 ) ground_truths = { # We add one because the background is not counted in ground_truths[BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : ground_truths [ BoxField . BOXES ] , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } # anchors are deterministic duplicate them to create a batch anchors = tf . tile ( anchors [ None ] , ( tf . shape ( ground_truths [ BoxField . BOXES ] ) [ 0 ] , 1 , 1 )) y_true , weights = self . target_assigner . assign ( { BoxField . BOXES : anchors } , ground_truths ) y_true [ BoxField . LABELS ] = tf . minimum ( y_true [ BoxField . LABELS ] , 1 ) ## Compute metrics recall = compute_rpn_metrics ( y_true [ BoxField . LABELS ] , classification_pred , weights [ BoxField . LABELS ] ) self . add_metric ( recall , name = 'rpn_recall' , aggregation = 'mean' ) # All the boxes which are not -1 can be sampled labels = y_true [ BoxField . LABELS ] > 0 sample_idx = batch_sample_balanced_positive_negative ( weights [ BoxField . LABELS ] , SAMPLING_SIZE , labels , positive_fraction = SAMPLING_POSITIVE_RATIO , dtype = self . _compute_dtype ) weights [ BoxField . LABELS ] = sample_idx * weights [ BoxField . LABELS ] weights [ BoxField . BOXES ] = sample_idx * weights [ BoxField . BOXES ] y_pred = { BoxField . LABELS : classification_pred , BoxField . BOXES : localization_pred } return self . compute_losses ( y_true , y_pred , weights )","title":"compute_loss"},{"location":"reference/kerod/layers/detection/rpn/#compute_losses","text":"def compute_losses ( self , y_true : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], weights : Dict [ str , tensorflow . python . framework . ops . Tensor ] ) -> dict Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Parameters: Name Description y_pred A dict of tensors of shape [N, nb_boxes, num_output]. y_true A dict of tensors of shape [N, nb_boxes, num_output]. weights A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: Type Description None dict : A dict of different losses View Source def compute_losses ( self , y_true : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , weights : Dict [ str, tf.Tensor ] ) -> dict : \"\"\"Compute the losses of the object detection head. Each dictionary is composed of the same keys (classification, localization, segmentation) Arguments: y_pred: A dict of tensors of shape [N, nb_boxes, num_output]. y_true: A dict of tensors of shape [N, nb_boxes, num_output]. weights: A dict of tensors ofshape [N, nb_boxes, num_output]. This tensor is composed of one hot vectors. Returns: dict : A dict of different losses \"\"\" def _compute_loss ( loss , loss_weight , target ) : losses = loss ( tf . cast ( y_true [ target ] , tf . float32 ), tf . cast ( y_pred [ target ] , tf . float32 ), sample_weight = tf . cast ( weights [ target ] , tf . float32 )) return loss_weight * tf . reduce_mean ( tf . reduce_sum ( losses , axis = 1 ) / normalizer ) normalizer = tf . maximum ( tf . reduce_sum ( weights [ BoxField.LABELS ] , axis = 1 ), 1.0 ) normalizer = tf . cast ( normalizer , tf . float32 ) classification_loss = _compute_loss ( self . _classification_loss , self . _classification_loss_weight , BoxField . LABELS ) self . add_metric ( classification_loss , name = f '{self.name}_classification_loss' , aggregation = 'mean' ) localization_loss = _compute_loss ( self . _localization_loss , self . _localization_loss_weight , BoxField . BOXES ) self . add_metric ( localization_loss , name = f '{self.name}_localization_loss' , aggregation = 'mean' ) self . add_loss ( [ classification_loss, localization_loss ] ) if self . _use_mask : segmentation_loss = _compute_loss ( self . _segmentation_loss , self . _segmentation_loss_weight , BoxField . MASKS ) self . add_metric ( segmentation_loss , name = f '{self.name}_segmentation_loss' , aggregation = 'mean' ) self . add_loss ( segmentation_loss ) return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss , BoxField . MASKS : segmentation_loss } return { BoxField . LABELS : classification_loss , BoxField . BOXES : localization_loss }","title":"compute_losses"},{"location":"reference/kerod/layers/post_processing/","text":"Module kerod.layers.post_processing None None View Source from kerod.layers.post_processing.non_maximum_suppression import ( post_process_fast_rcnn_boxes , post_process_rpn ) Sub-modules kerod.layers.post_processing.non_maximum_suppression kerod.layers.post_processing.post_processing_detr","title":"Index"},{"location":"reference/kerod/layers/post_processing/#module-kerodlayerspost_processing","text":"None None View Source from kerod.layers.post_processing.non_maximum_suppression import ( post_process_fast_rcnn_boxes , post_process_rpn )","title":"Module kerod.layers.post_processing"},{"location":"reference/kerod/layers/post_processing/#sub-modules","text":"kerod.layers.post_processing.non_maximum_suppression kerod.layers.post_processing.post_processing_detr","title":"Sub-modules"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/","text":"Module kerod.layers.post_processing.non_maximum_suppression Methods using non maximum_suppression to handle overlaps between boxes. None View Source \"\"\"Methods using non maximum_suppression to handle overlaps between boxes. \"\"\" from typing import List import tensorflow as tf from kerod.core.box_coder import decode_boxes_faster_rcnn from kerod.core.box_ops import clip_boxes from kerod.core.standard_fields import BoxField from kerod.utils.ops import get_full_indices def post_process_rpn ( cls_pred_per_lvl : List [ tf . Tensor ], loc_pred_per_lvl : List [ tf . Tensor ], anchors_per_lvl : List [ tf . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0.7 ): \"\"\"Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Arguments: - *cls_pred_per_lvl*: A list of Tensor of shape [batch_size, num_boxes, 2]. One item per level of the pyramid. - *loc_pred_per_lvl*: A list of Tensor of shape [batch_size, num_boxes, 4 * (num_anchors)]. One item per level of the pyramid. - *anchors_per_lvl*: A list of Tensor of shape [batch_size, num_boxes * num_anchors, 4] One item per level of the pyramid. - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *pre_nms_topk_per_lvl*: Will extract at each level this amount of boxes - post_nms_topk: Number of boxes selected after the nms Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. \"\"\" topk_boxes_per_lvl = [] topk_scores_per_lvl = [] for cls_pred , loc_pred , anchors in zip ( cls_pred_per_lvl , loc_pred_per_lvl , anchors_per_lvl ): batch_size = tf . shape ( cls_pred )[ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors ) boxes = clip_boxes ( boxes , image_information ) # Remove the background classes scores = cls_pred [:, :, 1 ] topk = tf . minimum ( pre_nms_topk_per_lvl , tf . size ( scores [ 0 ])) topk_scores , topk_indices = tf . math . top_k ( scores , k = topk , sorted = False ) topk_indices = get_full_indices ( topk_indices ) topk_boxes_per_lvl . append ( tf . cast ( tf . gather_nd ( boxes , topk_indices ), tf . float32 )) topk_scores_per_lvl . append ( tf . cast ( topk_scores , tf . float32 )) topk_boxes = tf . concat ( topk_boxes_per_lvl , 1 ) topk_scores = tf . concat ( topk_scores_per_lvl , 1 ) if post_nms_topk is None : post_nms_topk = pre_nms_topk_per_lvl nmsed_boxes , _ , _ , _ = tf . image . combined_non_max_suppression ( topk_boxes [:, :, None ], topk_scores [ ... , None ], post_nms_topk , post_nms_topk , iou_threshold = iou_threshold , score_threshold = 0 , clip_boxes = False ) return tf . stop_gradient ( nmsed_boxes ) def post_process_fast_rcnn_boxes ( cls_pred : tf . Tensor , loc_pred : tf . Tensor , anchors : tf . Tensor , image_information : tf . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ): \"\"\"This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: - *cls_pred*: A Tensor of shape [batch_size, num_boxes, num_classes] - *loc_pred*: A Tensor of shape [batch_size, num_boxes, 4 * num_classes] - *anchors*: A Tensor of shape [batch_size, num_boxes, 4] - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *num_classes*: The number of classes (background is not included). - *max_output_size_per_class*: A scalar integer `Tensor` representing the maximum number of boxes to be selected by non max suppression per class - *max_total_size*: A scalar representing maximum number of boxes retained over all classes. - *iou_threshold*: A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. - *score_threshold*: A float representing the threshold for deciding when to remove boxes based on score. `O.05` is used like in Detectron or Tensorpack. Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. - *nmsed_classes*: A Tensor of shape [batch_size, max_detections] containing the class for boxes. - *valid_detections*: A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. \"\"\" batch_size = tf . shape ( cls_pred )[ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) anchors = tf . reshape ( tf . tile ( anchors , [ 1 , 1 , num_classes ]), ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors , scale_factors = ( 10.0 , 10.0 , 5.0 , 5.0 )) boxes = clip_boxes ( boxes , image_information ) boxes = tf . reshape ( boxes , ( batch_size , - 1 , num_classes , 4 )) nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections = tf . image . combined_non_max_suppression ( tf . cast ( boxes , tf . float32 ), tf . cast ( cls_pred , tf . float32 ), max_output_size_per_class , max_total_size , iou_threshold = iou_threshold , score_threshold = score_threshold , clip_boxes = False ) normalizer_boxes = tf . tile ( image_information [:, None ], [ 1 , 1 , 2 ]) nmsed_boxes = tf . math . divide ( nmsed_boxes , normalizer_boxes , name = BoxField . BOXES ) nmsed_scores = tf . identity ( nmsed_scores , name = BoxField . SCORES ) nmsed_labels = tf . identity ( nmsed_labels , name = BoxField . LABELS ) valid_detections = tf . identity ( valid_detections , name = BoxField . NUM_BOXES ) return nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections Functions post_process_fast_rcnn_boxes def post_process_fast_rcnn_boxes ( cls_pred : tensorflow . python . framework . ops . Tensor , loc_pred : tensorflow . python . framework . ops . Tensor , anchors : tensorflow . python . framework . ops . Tensor , image_information : tensorflow . python . framework . ops . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ) This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: cls_pred : A Tensor of shape [batch_size, num_boxes, num_classes] loc_pred : A Tensor of shape [batch_size, num_boxes, 4 * num_classes] anchors : A Tensor of shape [batch_size, num_boxes, 4] image_informations : A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. num_classes : The number of classes (background is not included). max_output_size_per_class : A scalar integer Tensor representing the maximum number of boxes to be selected by non max suppression per class max_total_size : A scalar representing maximum number of boxes retained over all classes. iou_threshold : A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. score_threshold : A float representing the threshold for deciding when to remove boxes based on score. O.05 is used like in Detectron or Tensorpack. Returns: nmsed_boxes : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. nmsed_scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. nmsed_classes : A Tensor of shape [batch_size, max_detections] containing the class for boxes. valid_detections : A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. View Source def post_process_fast_rcnn_boxes ( cls_pred : tf . Tensor , loc_pred : tf . Tensor , anchors : tf . Tensor , image_information : tf . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ) : \"\"\"This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: - *cls_pred*: A Tensor of shape [batch_size, num_boxes, num_classes] - *loc_pred*: A Tensor of shape [batch_size, num_boxes, 4 * num_classes] - *anchors*: A Tensor of shape [batch_size, num_boxes, 4] - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *num_classes*: The number of classes (background is not included). - *max_output_size_per_class*: A scalar integer `Tensor` representing the maximum number of boxes to be selected by non max suppression per class - *max_total_size*: A scalar representing maximum number of boxes retained over all classes. - *iou_threshold*: A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. - *score_threshold*: A float representing the threshold for deciding when to remove boxes based on score. `O.05` is used like in Detectron or Tensorpack. Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. - *nmsed_classes*: A Tensor of shape [batch_size, max_detections] containing the class for boxes. - *valid_detections*: A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. \"\"\" batch_size = tf . shape ( cls_pred ) [ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) anchors = tf . reshape ( tf . tile ( anchors , [ 1, 1, num_classes ] ), ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors , scale_factors = ( 10.0 , 10.0 , 5.0 , 5.0 )) boxes = clip_boxes ( boxes , image_information ) boxes = tf . reshape ( boxes , ( batch_size , - 1 , num_classes , 4 )) nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections = tf . image . combined_non_max_suppression ( tf . cast ( boxes , tf . float32 ), tf . cast ( cls_pred , tf . float32 ), max_output_size_per_class , max_total_size , iou_threshold = iou_threshold , score_threshold = score_threshold , clip_boxes = False ) normalizer_boxes = tf . tile ( image_information [ :, None ] , [ 1, 1, 2 ] ) nmsed_boxes = tf . math . divide ( nmsed_boxes , normalizer_boxes , name = BoxField . BOXES ) nmsed_scores = tf . identity ( nmsed_scores , name = BoxField . SCORES ) nmsed_labels = tf . identity ( nmsed_labels , name = BoxField . LABELS ) valid_detections = tf . identity ( valid_detections , name = BoxField . NUM_BOXES ) return nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections post_process_rpn def post_process_rpn ( cls_pred_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], loc_pred_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], anchors_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0.7 ) Sample RPN proposals by the following steps: Pick top k1 by scores NMS them Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Arguments: cls_pred_per_lvl : A list of Tensor of shape [batch_size, num_boxes, 2]. One item per level of the pyramid. loc_pred_per_lvl : A list of Tensor of shape [batch_size, num_boxes, 4 * (num_anchors)]. One item per level of the pyramid. anchors_per_lvl : A list of Tensor of shape [batch_size, num_boxes * num_anchors, 4] One item per level of the pyramid. image_informations : A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. pre_nms_topk_per_lvl : Will extract at each level this amount of boxes post_nms_topk: Number of boxes selected after the nms Returns: nmsed_boxes : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. nmsed_scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. View Source def post_process_rpn ( cls_pred_per_lvl : List [ tf . Tensor ], loc_pred_per_lvl : List [ tf . Tensor ], anchors_per_lvl : List [ tf . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0 . 7 ) : \"\"\" Sample RPN proposals by the following steps: 1 . Pick top k1 by scores 2 . NMS them 3 . Pick top k2 by scores . Default k2 == k1 , i . e . does not filter the NMS output . Arguments : - * cls_pred_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes , 2 ]. One item per level of the pyramid . - * loc_pred_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes , 4 * ( num_anchors ) ]. One item per level of the pyramid . - * anchors_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes * num_anchors , 4 ] One item per level of the pyramid . - * image_informations * : A Tensor of shape [ batch_size , ( height , width ) ] The height and the width are without the padding . - * pre_nms_topk_per_lvl * : Will extract at each level this amount of boxes - post_nms_topk : Number of boxes selected after the nms Returns : - * nmsed_boxes * : A Tensor of shape [ batch_size , max_detections , 4 ] containing the non - max suppressed boxes . - * nmsed_scores * : A Tensor of shape [ batch_size , max_detections ] containing the scores for the boxes . \"\"\" topk_boxes_per_lvl = [] topk_scores_per_lvl = [] for cls_pred , loc_pred , anchors in zip ( cls_pred_per_lvl , loc_pred_per_lvl , anchors_per_lvl ) : batch_size = tf . shape ( cls_pred ) [ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors ) boxes = clip_boxes ( boxes , image_information ) # Remove the background classes scores = cls_pred [:, :, 1 ] topk = tf . minimum ( pre_nms_topk_per_lvl , tf . size ( scores [ 0 ] )) topk_scores , topk_indices = tf . math . top_k ( scores , k = topk , sorted = False ) topk_indices = get_full_indices ( topk_indices ) topk_boxes_per_lvl . append ( tf . cast ( tf . gather_nd ( boxes , topk_indices ) , tf . float32 )) topk_scores_per_lvl . append ( tf . cast ( topk_scores , tf . float32 )) topk_boxes = tf . concat ( topk_boxes_per_lvl , 1 ) topk_scores = tf . concat ( topk_scores_per_lvl , 1 ) if post_nms_topk is None : post_nms_topk = pre_nms_topk_per_lvl nmsed_boxes , _ , _ , _ = tf . image . combined_non_max_suppression ( topk_boxes [:, :, None ], topk_scores [..., None ], post_nms_topk , post_nms_topk , iou_threshold = iou_threshold , score_threshold = 0 , clip_boxes = False ) return tf . stop_gradient ( nmsed_boxes )","title":"Non Maximum Suppression"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/#module-kerodlayerspost_processingnon_maximum_suppression","text":"Methods using non maximum_suppression to handle overlaps between boxes. None View Source \"\"\"Methods using non maximum_suppression to handle overlaps between boxes. \"\"\" from typing import List import tensorflow as tf from kerod.core.box_coder import decode_boxes_faster_rcnn from kerod.core.box_ops import clip_boxes from kerod.core.standard_fields import BoxField from kerod.utils.ops import get_full_indices def post_process_rpn ( cls_pred_per_lvl : List [ tf . Tensor ], loc_pred_per_lvl : List [ tf . Tensor ], anchors_per_lvl : List [ tf . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0.7 ): \"\"\"Sample RPN proposals by the following steps: 1. Pick top k1 by scores 2. NMS them 3. Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Arguments: - *cls_pred_per_lvl*: A list of Tensor of shape [batch_size, num_boxes, 2]. One item per level of the pyramid. - *loc_pred_per_lvl*: A list of Tensor of shape [batch_size, num_boxes, 4 * (num_anchors)]. One item per level of the pyramid. - *anchors_per_lvl*: A list of Tensor of shape [batch_size, num_boxes * num_anchors, 4] One item per level of the pyramid. - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *pre_nms_topk_per_lvl*: Will extract at each level this amount of boxes - post_nms_topk: Number of boxes selected after the nms Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. \"\"\" topk_boxes_per_lvl = [] topk_scores_per_lvl = [] for cls_pred , loc_pred , anchors in zip ( cls_pred_per_lvl , loc_pred_per_lvl , anchors_per_lvl ): batch_size = tf . shape ( cls_pred )[ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors ) boxes = clip_boxes ( boxes , image_information ) # Remove the background classes scores = cls_pred [:, :, 1 ] topk = tf . minimum ( pre_nms_topk_per_lvl , tf . size ( scores [ 0 ])) topk_scores , topk_indices = tf . math . top_k ( scores , k = topk , sorted = False ) topk_indices = get_full_indices ( topk_indices ) topk_boxes_per_lvl . append ( tf . cast ( tf . gather_nd ( boxes , topk_indices ), tf . float32 )) topk_scores_per_lvl . append ( tf . cast ( topk_scores , tf . float32 )) topk_boxes = tf . concat ( topk_boxes_per_lvl , 1 ) topk_scores = tf . concat ( topk_scores_per_lvl , 1 ) if post_nms_topk is None : post_nms_topk = pre_nms_topk_per_lvl nmsed_boxes , _ , _ , _ = tf . image . combined_non_max_suppression ( topk_boxes [:, :, None ], topk_scores [ ... , None ], post_nms_topk , post_nms_topk , iou_threshold = iou_threshold , score_threshold = 0 , clip_boxes = False ) return tf . stop_gradient ( nmsed_boxes ) def post_process_fast_rcnn_boxes ( cls_pred : tf . Tensor , loc_pred : tf . Tensor , anchors : tf . Tensor , image_information : tf . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ): \"\"\"This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: - *cls_pred*: A Tensor of shape [batch_size, num_boxes, num_classes] - *loc_pred*: A Tensor of shape [batch_size, num_boxes, 4 * num_classes] - *anchors*: A Tensor of shape [batch_size, num_boxes, 4] - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *num_classes*: The number of classes (background is not included). - *max_output_size_per_class*: A scalar integer `Tensor` representing the maximum number of boxes to be selected by non max suppression per class - *max_total_size*: A scalar representing maximum number of boxes retained over all classes. - *iou_threshold*: A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. - *score_threshold*: A float representing the threshold for deciding when to remove boxes based on score. `O.05` is used like in Detectron or Tensorpack. Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. - *nmsed_classes*: A Tensor of shape [batch_size, max_detections] containing the class for boxes. - *valid_detections*: A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. \"\"\" batch_size = tf . shape ( cls_pred )[ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) anchors = tf . reshape ( tf . tile ( anchors , [ 1 , 1 , num_classes ]), ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors , scale_factors = ( 10.0 , 10.0 , 5.0 , 5.0 )) boxes = clip_boxes ( boxes , image_information ) boxes = tf . reshape ( boxes , ( batch_size , - 1 , num_classes , 4 )) nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections = tf . image . combined_non_max_suppression ( tf . cast ( boxes , tf . float32 ), tf . cast ( cls_pred , tf . float32 ), max_output_size_per_class , max_total_size , iou_threshold = iou_threshold , score_threshold = score_threshold , clip_boxes = False ) normalizer_boxes = tf . tile ( image_information [:, None ], [ 1 , 1 , 2 ]) nmsed_boxes = tf . math . divide ( nmsed_boxes , normalizer_boxes , name = BoxField . BOXES ) nmsed_scores = tf . identity ( nmsed_scores , name = BoxField . SCORES ) nmsed_labels = tf . identity ( nmsed_labels , name = BoxField . LABELS ) valid_detections = tf . identity ( valid_detections , name = BoxField . NUM_BOXES ) return nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections","title":"Module kerod.layers.post_processing.non_maximum_suppression"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/#post_process_fast_rcnn_boxes","text":"def post_process_fast_rcnn_boxes ( cls_pred : tensorflow . python . framework . ops . Tensor , loc_pred : tensorflow . python . framework . ops . Tensor , anchors : tensorflow . python . framework . ops . Tensor , image_information : tensorflow . python . framework . ops . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ) This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: cls_pred : A Tensor of shape [batch_size, num_boxes, num_classes] loc_pred : A Tensor of shape [batch_size, num_boxes, 4 * num_classes] anchors : A Tensor of shape [batch_size, num_boxes, 4] image_informations : A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. num_classes : The number of classes (background is not included). max_output_size_per_class : A scalar integer Tensor representing the maximum number of boxes to be selected by non max suppression per class max_total_size : A scalar representing maximum number of boxes retained over all classes. iou_threshold : A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. score_threshold : A float representing the threshold for deciding when to remove boxes based on score. O.05 is used like in Detectron or Tensorpack. Returns: nmsed_boxes : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. nmsed_scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. nmsed_classes : A Tensor of shape [batch_size, max_detections] containing the class for boxes. valid_detections : A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. View Source def post_process_fast_rcnn_boxes ( cls_pred : tf . Tensor , loc_pred : tf . Tensor , anchors : tf . Tensor , image_information : tf . Tensor , num_classes , max_output_size_per_class : int = 100 , max_total_size : int = 100 , iou_threshold : float = 0.5 , score_threshold : float = 0.05 ) : \"\"\"This is the classical post_processing for the Faster RCNN paper. This operation performs non_max_suppression on the inputs per batch, across all classes. Prunes away boxes that have high intersection-over-union (IOU) overlap with previously selected boxes. Bounding boxes are supplied as [y1, x1, y2, x2], where (y1, x1) and (y2, x2) are the coordinates of any diagonal pair of box corners and the coordinates can be provided as normalized (i.e., lying in the interval [0, 1]) or absolute. Note that this algorithm is agnostic to where the origin is in the coordinate system. Also note that this algorithm is invariant to orthogonal transformations and translations of the coordinate system; thus translating or reflections of the coordinate system result in the same boxes being selected by the algorithm. The output of this operation is the final boxes, scores and classes tensor returned after performing non_max_suppression. Arguments: - *cls_pred*: A Tensor of shape [batch_size, num_boxes, num_classes] - *loc_pred*: A Tensor of shape [batch_size, num_boxes, 4 * num_classes] - *anchors*: A Tensor of shape [batch_size, num_boxes, 4] - *image_informations*: A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. - *num_classes*: The number of classes (background is not included). - *max_output_size_per_class*: A scalar integer `Tensor` representing the maximum number of boxes to be selected by non max suppression per class - *max_total_size*: A scalar representing maximum number of boxes retained over all classes. - *iou_threshold*: A float representing the threshold for deciding whether boxes overlap too much with respect to IOU. - *score_threshold*: A float representing the threshold for deciding when to remove boxes based on score. `O.05` is used like in Detectron or Tensorpack. Returns: - *nmsed_boxes*: A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. The coordinates returned are between 0 and 1. - *nmsed_scores*: A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. - *nmsed_classes*: A Tensor of shape [batch_size, max_detections] containing the class for boxes. - *valid_detections*: A [batch_size] int32 tensor indicating the number of valid detections per batch item. Only the top valid_detections[i] entries in nms_boxes[i], nms_scores[i] and nms_class[i] are valid. The rest of the entries are zero paddings. \"\"\" batch_size = tf . shape ( cls_pred ) [ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) anchors = tf . reshape ( tf . tile ( anchors , [ 1, 1, num_classes ] ), ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors , scale_factors = ( 10.0 , 10.0 , 5.0 , 5.0 )) boxes = clip_boxes ( boxes , image_information ) boxes = tf . reshape ( boxes , ( batch_size , - 1 , num_classes , 4 )) nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections = tf . image . combined_non_max_suppression ( tf . cast ( boxes , tf . float32 ), tf . cast ( cls_pred , tf . float32 ), max_output_size_per_class , max_total_size , iou_threshold = iou_threshold , score_threshold = score_threshold , clip_boxes = False ) normalizer_boxes = tf . tile ( image_information [ :, None ] , [ 1, 1, 2 ] ) nmsed_boxes = tf . math . divide ( nmsed_boxes , normalizer_boxes , name = BoxField . BOXES ) nmsed_scores = tf . identity ( nmsed_scores , name = BoxField . SCORES ) nmsed_labels = tf . identity ( nmsed_labels , name = BoxField . LABELS ) valid_detections = tf . identity ( valid_detections , name = BoxField . NUM_BOXES ) return nmsed_boxes , nmsed_scores , nmsed_labels , valid_detections","title":"post_process_fast_rcnn_boxes"},{"location":"reference/kerod/layers/post_processing/non_maximum_suppression/#post_process_rpn","text":"def post_process_rpn ( cls_pred_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], loc_pred_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], anchors_per_lvl : List [ tensorflow . python . framework . ops . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0.7 ) Sample RPN proposals by the following steps: Pick top k1 by scores NMS them Pick top k2 by scores. Default k2 == k1, i.e. does not filter the NMS output. Arguments: cls_pred_per_lvl : A list of Tensor of shape [batch_size, num_boxes, 2]. One item per level of the pyramid. loc_pred_per_lvl : A list of Tensor of shape [batch_size, num_boxes, 4 * (num_anchors)]. One item per level of the pyramid. anchors_per_lvl : A list of Tensor of shape [batch_size, num_boxes * num_anchors, 4] One item per level of the pyramid. image_informations : A Tensor of shape [batch_size, (height, width)] The height and the width are without the padding. pre_nms_topk_per_lvl : Will extract at each level this amount of boxes post_nms_topk: Number of boxes selected after the nms Returns: nmsed_boxes : A Tensor of shape [batch_size, max_detections, 4] containing the non-max suppressed boxes. nmsed_scores : A Tensor of shape [batch_size, max_detections] containing the scores for the boxes. View Source def post_process_rpn ( cls_pred_per_lvl : List [ tf . Tensor ], loc_pred_per_lvl : List [ tf . Tensor ], anchors_per_lvl : List [ tf . Tensor ], image_information , pre_nms_topk_per_lvl , post_nms_topk = None , iou_threshold : float = 0 . 7 ) : \"\"\" Sample RPN proposals by the following steps: 1 . Pick top k1 by scores 2 . NMS them 3 . Pick top k2 by scores . Default k2 == k1 , i . e . does not filter the NMS output . Arguments : - * cls_pred_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes , 2 ]. One item per level of the pyramid . - * loc_pred_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes , 4 * ( num_anchors ) ]. One item per level of the pyramid . - * anchors_per_lvl * : A list of Tensor of shape [ batch_size , num_boxes * num_anchors , 4 ] One item per level of the pyramid . - * image_informations * : A Tensor of shape [ batch_size , ( height , width ) ] The height and the width are without the padding . - * pre_nms_topk_per_lvl * : Will extract at each level this amount of boxes - post_nms_topk : Number of boxes selected after the nms Returns : - * nmsed_boxes * : A Tensor of shape [ batch_size , max_detections , 4 ] containing the non - max suppressed boxes . - * nmsed_scores * : A Tensor of shape [ batch_size , max_detections ] containing the scores for the boxes . \"\"\" topk_boxes_per_lvl = [] topk_scores_per_lvl = [] for cls_pred , loc_pred , anchors in zip ( cls_pred_per_lvl , loc_pred_per_lvl , anchors_per_lvl ) : batch_size = tf . shape ( cls_pred ) [ 0 ] loc_pred = tf . reshape ( loc_pred , ( batch_size , - 1 , 4 )) boxes = decode_boxes_faster_rcnn ( loc_pred , anchors ) boxes = clip_boxes ( boxes , image_information ) # Remove the background classes scores = cls_pred [:, :, 1 ] topk = tf . minimum ( pre_nms_topk_per_lvl , tf . size ( scores [ 0 ] )) topk_scores , topk_indices = tf . math . top_k ( scores , k = topk , sorted = False ) topk_indices = get_full_indices ( topk_indices ) topk_boxes_per_lvl . append ( tf . cast ( tf . gather_nd ( boxes , topk_indices ) , tf . float32 )) topk_scores_per_lvl . append ( tf . cast ( topk_scores , tf . float32 )) topk_boxes = tf . concat ( topk_boxes_per_lvl , 1 ) topk_scores = tf . concat ( topk_scores_per_lvl , 1 ) if post_nms_topk is None : post_nms_topk = pre_nms_topk_per_lvl nmsed_boxes , _ , _ , _ = tf . image . combined_non_max_suppression ( topk_boxes [:, :, None ], topk_scores [..., None ], post_nms_topk , post_nms_topk , iou_threshold = iou_threshold , score_threshold = 0 , clip_boxes = False ) return tf . stop_gradient ( nmsed_boxes )","title":"post_process_rpn"},{"location":"reference/kerod/layers/post_processing/post_processing_detr/","text":"Module kerod.layers.post_processing.post_processing_detr None None View Source import tensorflow as tf from kerod.core.standard_fields import BoxField from kerod.utils.ops import get_full_indices from kerod.core.box_ops import convert_to_xyxy_coordinates def post_processing ( boxes : tf . Tensor , logits : tf . Tensor , image_information : tf . Tensor , image_padded_information : tf . Tensor , sorted = True ): \"\"\"PostProcessing described in the paper Object Detection with transformers \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) Arguments: - *logits*: A Tensor of shape [batch_size, num_queries, num_classes + 1] representing the class probability. - *localization_pred*: A Tensor of shape [batch_size, num_queries, (y_cent, x_cent, h, w)] - *image_information*: A 2-D tensor of float32 and shape [2, (height, width)]. It contains the shape of the image without any padding. - *image_padded_information*: A 2-D tensor of float32 and shape [(height_pad, width_pad)]. It contains the shape of the image without any padding. This padding is added during the dataset step when we batch the images together (padded_batch). - *sorted*: Return all the elements sorted by scores in descending order. Returns: - *boxes*: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - *scores*: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - *classes*: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" probabilities = tf . nn . softmax ( logits , axis =- 1 ) # Remove the background at pos 0 scores = tf . reduce_max ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . SCORES ) labels = tf . argmax ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . LABELS ) # Prediction between 0 and 1 are performed with padding # Boxes (y1,x1,y2,x2) * Padded_image_(h,w,h,w) /unpadded_image_(h,w,h,w) # where padded_image and unpadded_image are in the image space image_padded_information = tf . cast ( image_padded_information , boxes . dtype ) image_information = tf . cast ( image_information , boxes . dtype ) # [batch_size, (y1_coeff, x1_coeff, y2_coeff, x2_coeff)] coeffs = tf . tile ( image_padded_information , [ 2 ]) / tf . tile ( image_information , [ 1 , 2 ]) boxes = convert_to_xyxy_coordinates ( boxes ) boxes_without_padding = boxes * coeffs [:, None ] boxes_without_padding = tf . clip_by_value ( boxes_without_padding , 0 , 1 , name = BoxField . BOXES ) if not sorted : return boxes_without_padding , scores , labels sorted_scores , indices = tf . math . top_k ( scores , k = tf . shape ( scores )[ - 1 ], sorted = True ) indices = get_full_indices ( indices ) sorted_labels = tf . gather_nd ( labels , indices ) sorted_boxes_without_padding = tf . gather_nd ( boxes_without_padding , indices ) return sorted_boxes_without_padding , sorted_scores , sorted_labels Functions post_processing def post_processing ( boxes : tensorflow . python . framework . ops . Tensor , logits : tensorflow . python . framework . ops . Tensor , image_information : tensorflow . python . framework . ops . Tensor , image_padded_information : tensorflow . python . framework . ops . Tensor , sorted = True ) PostProcessing described in the paper Object Detection with transformers \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) Arguments: logits : A Tensor of shape [batch_size, num_queries, num_classes + 1] representing the class probability. localization_pred : A Tensor of shape [batch_size, num_queries, (y_cent, x_cent, h, w)] image_information : A 2-D tensor of float32 and shape [2, (height, width)]. It contains the shape of the image without any padding. image_padded_information : A 2-D tensor of float32 and shape [(height_pad, width_pad)]. It contains the shape of the image without any padding. This padding is added during the dataset step when we batch the images together (padded_batch). sorted : Return all the elements sorted by scores in descending order. Returns: boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def post_processing ( boxes : tf . Tensor , logits : tf . Tensor , image_information : tf . Tensor , image_padded_information : tf . Tensor , sorted = True ) : \"\"\" PostProcessing described in the paper Object Detection with transformers \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) Arguments : - * logits * : A Tensor of shape [ batch_size , num_queries , num_classes + 1 ] representing the class probability . - * localization_pred * : A Tensor of shape [ batch_size , num_queries , ( y_cent , x_cent , h , w ) ] - * image_information * : A 2 - D tensor of float32 and shape [ 2 , ( height , width ) ]. It contains the shape of the image without any padding . - * image_padded_information * : A 2 - D tensor of float32 and shape [ ( height_pad , width_pad ) ]. It contains the shape of the image without any padding . This padding is added during the dataset step when we batch the images together ( padded_batch ) . - * sorted * : Return all the elements sorted by scores in descending order . Returns : - * boxes * : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . - * scores * : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . - * classes * : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" probabilities = tf . nn . softmax ( logits , axis =- 1 ) # Remove the background at pos 0 scores = tf . reduce_max ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . SCORES ) labels = tf . argmax ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . LABELS ) # Prediction between 0 and 1 are performed with padding # Boxes ( y1 , x1 , y2 , x2 ) * Padded_image_ ( h , w , h , w ) / unpadded_image_ ( h , w , h , w ) # where padded_image and unpadded_image are in the image space image_padded_information = tf . cast ( image_padded_information , boxes . dtype ) image_information = tf . cast ( image_information , boxes . dtype ) # [ batch_size , ( y1_coeff , x1_coeff , y2_coeff , x2_coeff ) ] coeffs = tf . tile ( image_padded_information , [ 2 ] ) / tf . tile ( image_information , [ 1 , 2 ] ) boxes = convert_to_xyxy_coordinates ( boxes ) boxes_without_padding = boxes * coeffs [:, None ] boxes_without_padding = tf . clip_by_value ( boxes_without_padding , 0 , 1 , name = BoxField . BOXES ) if not sorted : return boxes_without_padding , scores , labels sorted_scores , indices = tf . math . top_k ( scores , k = tf . shape ( scores ) [ - 1 ], sorted = True ) indices = get_full_indices ( indices ) sorted_labels = tf . gather_nd ( labels , indices ) sorted_boxes_without_padding = tf . gather_nd ( boxes_without_padding , indices ) return sorted_boxes_without_padding , sorted_scores , sorted_labels","title":"Post Processing Detr"},{"location":"reference/kerod/layers/post_processing/post_processing_detr/#module-kerodlayerspost_processingpost_processing_detr","text":"None None View Source import tensorflow as tf from kerod.core.standard_fields import BoxField from kerod.utils.ops import get_full_indices from kerod.core.box_ops import convert_to_xyxy_coordinates def post_processing ( boxes : tf . Tensor , logits : tf . Tensor , image_information : tf . Tensor , image_padded_information : tf . Tensor , sorted = True ): \"\"\"PostProcessing described in the paper Object Detection with transformers \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) Arguments: - *logits*: A Tensor of shape [batch_size, num_queries, num_classes + 1] representing the class probability. - *localization_pred*: A Tensor of shape [batch_size, num_queries, (y_cent, x_cent, h, w)] - *image_information*: A 2-D tensor of float32 and shape [2, (height, width)]. It contains the shape of the image without any padding. - *image_padded_information*: A 2-D tensor of float32 and shape [(height_pad, width_pad)]. It contains the shape of the image without any padding. This padding is added during the dataset step when we batch the images together (padded_batch). - *sorted*: Return all the elements sorted by scores in descending order. Returns: - *boxes*: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - *scores*: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - *classes*: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" probabilities = tf . nn . softmax ( logits , axis =- 1 ) # Remove the background at pos 0 scores = tf . reduce_max ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . SCORES ) labels = tf . argmax ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . LABELS ) # Prediction between 0 and 1 are performed with padding # Boxes (y1,x1,y2,x2) * Padded_image_(h,w,h,w) /unpadded_image_(h,w,h,w) # where padded_image and unpadded_image are in the image space image_padded_information = tf . cast ( image_padded_information , boxes . dtype ) image_information = tf . cast ( image_information , boxes . dtype ) # [batch_size, (y1_coeff, x1_coeff, y2_coeff, x2_coeff)] coeffs = tf . tile ( image_padded_information , [ 2 ]) / tf . tile ( image_information , [ 1 , 2 ]) boxes = convert_to_xyxy_coordinates ( boxes ) boxes_without_padding = boxes * coeffs [:, None ] boxes_without_padding = tf . clip_by_value ( boxes_without_padding , 0 , 1 , name = BoxField . BOXES ) if not sorted : return boxes_without_padding , scores , labels sorted_scores , indices = tf . math . top_k ( scores , k = tf . shape ( scores )[ - 1 ], sorted = True ) indices = get_full_indices ( indices ) sorted_labels = tf . gather_nd ( labels , indices ) sorted_boxes_without_padding = tf . gather_nd ( boxes_without_padding , indices ) return sorted_boxes_without_padding , sorted_scores , sorted_labels","title":"Module kerod.layers.post_processing.post_processing_detr"},{"location":"reference/kerod/layers/post_processing/post_processing_detr/#functions","text":"","title":"Functions"},{"location":"reference/kerod/layers/post_processing/post_processing_detr/#post_processing","text":"def post_processing ( boxes : tensorflow . python . framework . ops . Tensor , logits : tensorflow . python . framework . ops . Tensor , image_information : tensorflow . python . framework . ops . Tensor , image_padded_information : tensorflow . python . framework . ops . Tensor , sorted = True ) PostProcessing described in the paper Object Detection with transformers \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) Arguments: logits : A Tensor of shape [batch_size, num_queries, num_classes + 1] representing the class probability. localization_pred : A Tensor of shape [batch_size, num_queries, (y_cent, x_cent, h, w)] image_information : A 2-D tensor of float32 and shape [2, (height, width)]. It contains the shape of the image without any padding. image_padded_information : A 2-D tensor of float32 and shape [(height_pad, width_pad)]. It contains the shape of the image without any padding. This padding is added during the dataset step when we batch the images together (padded_batch). sorted : Return all the elements sorted by scores in descending order. Returns: boxes : A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores : A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes : A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). View Source def post_processing ( boxes : tf . Tensor , logits : tf . Tensor , image_information : tf . Tensor , image_padded_information : tf . Tensor , sorted = True ) : \"\"\" PostProcessing described in the paper Object Detection with transformers \" To optimize for AP, we override the prediction of these slots with the second highest scoring class , using the corresponding confidence \" Part 4 . Example : background + 3 classes [ 0 . 54 , 0 . 40 , 0 . 03 , 0 . 03 ] => score = 0 . 40 , label = 0 ( 1 - 1 ) Arguments : - * logits * : A Tensor of shape [ batch_size , num_queries , num_classes + 1 ] representing the class probability . - * localization_pred * : A Tensor of shape [ batch_size , num_queries , ( y_cent , x_cent , h , w ) ] - * image_information * : A 2 - D tensor of float32 and shape [ 2 , ( height , width ) ]. It contains the shape of the image without any padding . - * image_padded_information * : A 2 - D tensor of float32 and shape [ ( height_pad , width_pad ) ]. It contains the shape of the image without any padding . This padding is added during the dataset step when we batch the images together ( padded_batch ) . - * sorted * : Return all the elements sorted by scores in descending order . Returns : - * boxes * : A Tensor of shape [ batch_size , self . num_queries , ( y1 , x1 , y2 , x2 ) ] containing the boxes with the coordinates between 0 and 1 . - * scores * : A Tensor of shape [ batch_size , self . num_queries ] containing the score of the boxes . - * classes * : A Tensor of shape [ batch_size , self . num_queries ] containing the class of the boxes [ 0 , num_classes ) . \"\"\" probabilities = tf . nn . softmax ( logits , axis =- 1 ) # Remove the background at pos 0 scores = tf . reduce_max ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . SCORES ) labels = tf . argmax ( probabilities [:, :, 1 :], axis =- 1 , name = BoxField . LABELS ) # Prediction between 0 and 1 are performed with padding # Boxes ( y1 , x1 , y2 , x2 ) * Padded_image_ ( h , w , h , w ) / unpadded_image_ ( h , w , h , w ) # where padded_image and unpadded_image are in the image space image_padded_information = tf . cast ( image_padded_information , boxes . dtype ) image_information = tf . cast ( image_information , boxes . dtype ) # [ batch_size , ( y1_coeff , x1_coeff , y2_coeff , x2_coeff ) ] coeffs = tf . tile ( image_padded_information , [ 2 ] ) / tf . tile ( image_information , [ 1 , 2 ] ) boxes = convert_to_xyxy_coordinates ( boxes ) boxes_without_padding = boxes * coeffs [:, None ] boxes_without_padding = tf . clip_by_value ( boxes_without_padding , 0 , 1 , name = BoxField . BOXES ) if not sorted : return boxes_without_padding , scores , labels sorted_scores , indices = tf . math . top_k ( scores , k = tf . shape ( scores ) [ - 1 ], sorted = True ) indices = get_full_indices ( indices ) sorted_labels = tf . gather_nd ( labels , indices ) sorted_boxes_without_padding = tf . gather_nd ( boxes_without_padding , indices ) return sorted_boxes_without_padding , sorted_scores , sorted_labels","title":"post_processing"},{"location":"reference/kerod/layers/smca/","text":"Module kerod.layers.smca None None Sub-modules kerod.layers.smca.reference_points kerod.layers.smca.weight_map","title":"Index"},{"location":"reference/kerod/layers/smca/#module-kerodlayerssmca","text":"None None","title":"Module kerod.layers.smca"},{"location":"reference/kerod/layers/smca/#sub-modules","text":"kerod.layers.smca.reference_points kerod.layers.smca.weight_map","title":"Sub-modules"},{"location":"reference/kerod/layers/smca/reference_points/","text":"Module kerod.layers.smca.reference_points None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class SMCAReferencePoints ( tf . keras . layers . Layer ): \"\"\"Multi head reference points from the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Based on the object queries will create a set of reference points which will allow to create a [spatial dynamical weight maps](./weight_map.py) in order to modulate the co-attention inside the transformer Arguments: hidden_dim: Positive integer, dimensionality of the hidden space. num_heads: Positive integer, each head starts from a head-shared center and then predicts a head-specific center offset and head specific scales. Call arguments: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\"\" def __init__ ( self , hidden_dim : int , num_heads : int , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . hidden_dim = hidden_dim self . xy_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 2 ) # (y_cent, x_cent) ]) # Each head will have its proper focus weight and width self . yx_offset_hw_embed = tf . keras . layers . Dense ( 4 * num_heads ) def call ( self , object_queries ): \"\"\" Args: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\"\" yx_pre_sigmoid = self . xy_embed ( object_queries ) # [bs, num_queries, 2] yx = tf . nn . sigmoid ( yx_pre_sigmoid ) # Where y and x are offsets predictions for 'yx' (above) # h and w are the scales predictions yx_offset_hw = self . yx_offset_hw_embed ( object_queries ) # [bs, num_queries, head * 4] batch_size = tf . shape ( object_queries )[ 0 ] num_queries = tf . shape ( object_queries )[ 1 ] # Add offset coordinates to yx and concatenate with scale predictions # yx => [bs, num_queries, head, 2] yx = tf . tile ( yx [:, :, None ], ( 1 , 1 , self . num_heads , 1 )) # yx_offset_hw => [bs, num_queries, head, 4] yx_offset_hw = tf . reshape ( yx_offset_hw , ( batch_size , - 1 , self . num_heads , 4 )) yxhw = tf . concat ([ yx , tf . zeros (( batch_size , num_queries , self . num_heads , 2 ))], axis =- 1 ) return yxhw + yx_offset_hw , yx_pre_sigmoid def get_config ( self ): config = super () . get_config () config [ 'num_heads' ] = self . num_heads config [ 'hidden_dim' ] = self . hidden_dim return config remove_unwanted_doc ( SMCAReferencePoints , __pdoc__ ) Classes SMCAReferencePoints class SMCAReferencePoints ( hidden_dim : int , num_heads : int , ** kwargs ) Based on the object queries will create a set of reference points which will allow to create a spatial dynamical weight maps in order to modulate the co-attention inside the transformer Arguments Name Description hidden_dim Positive integer, dimensionality of the hidden space. num_heads Positive integer, each head starts from a head-shared center and then predicts a head-specific center offset and head specific scales. Call arguments Name Description object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns Type Description Tuple - reference_points : A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - embedding_reference_points : A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , object_queries ) Parameters: Name Description object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. View Source def call ( self , object_queries ) : \" \"\" Args: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\" \" yx_pre_sigmoid = self . xy_embed ( object_queries ) # [bs, num_queries, 2] yx = tf . nn . sigmoid ( yx_pre_sigmoid ) # Where y and x are offsets predictions for 'yx' (above) # h and w are the scales predictions yx_offset_hw = self . yx_offset_hw_embed ( object_queries ) # [bs, num_queries, head * 4] batch_size = tf . shape ( object_queries ) [ 0 ] num_queries = tf . shape ( object_queries ) [ 1 ] # Add offset coordinates to yx and concatenate with scale predictions # yx => [bs, num_queries, head, 2] yx = tf . tile ( yx [ : , : , None ] , ( 1 , 1 , self . num_heads , 1 )) # yx_offset_hw => [bs, num_queries, head, 4] yx_offset_hw = tf . reshape ( yx_offset_hw , ( batch_size , - 1 , self . num_heads , 4 )) yxhw = tf . concat ( [ yx , tf . zeros (( batch_size , num_queries , self . num_heads , 2 )) ] , axis =- 1 ) return yxhw + yx_offset_hw , yx_pre_sigmoid","title":"Reference Points"},{"location":"reference/kerod/layers/smca/reference_points/#module-kerodlayerssmcareference_points","text":"None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class SMCAReferencePoints ( tf . keras . layers . Layer ): \"\"\"Multi head reference points from the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Based on the object queries will create a set of reference points which will allow to create a [spatial dynamical weight maps](./weight_map.py) in order to modulate the co-attention inside the transformer Arguments: hidden_dim: Positive integer, dimensionality of the hidden space. num_heads: Positive integer, each head starts from a head-shared center and then predicts a head-specific center offset and head specific scales. Call arguments: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\"\" def __init__ ( self , hidden_dim : int , num_heads : int , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_heads = num_heads self . hidden_dim = hidden_dim self . xy_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 2 ) # (y_cent, x_cent) ]) # Each head will have its proper focus weight and width self . yx_offset_hw_embed = tf . keras . layers . Dense ( 4 * num_heads ) def call ( self , object_queries ): \"\"\" Args: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\"\" yx_pre_sigmoid = self . xy_embed ( object_queries ) # [bs, num_queries, 2] yx = tf . nn . sigmoid ( yx_pre_sigmoid ) # Where y and x are offsets predictions for 'yx' (above) # h and w are the scales predictions yx_offset_hw = self . yx_offset_hw_embed ( object_queries ) # [bs, num_queries, head * 4] batch_size = tf . shape ( object_queries )[ 0 ] num_queries = tf . shape ( object_queries )[ 1 ] # Add offset coordinates to yx and concatenate with scale predictions # yx => [bs, num_queries, head, 2] yx = tf . tile ( yx [:, :, None ], ( 1 , 1 , self . num_heads , 1 )) # yx_offset_hw => [bs, num_queries, head, 4] yx_offset_hw = tf . reshape ( yx_offset_hw , ( batch_size , - 1 , self . num_heads , 4 )) yxhw = tf . concat ([ yx , tf . zeros (( batch_size , num_queries , self . num_heads , 2 ))], axis =- 1 ) return yxhw + yx_offset_hw , yx_pre_sigmoid def get_config ( self ): config = super () . get_config () config [ 'num_heads' ] = self . num_heads config [ 'hidden_dim' ] = self . hidden_dim return config remove_unwanted_doc ( SMCAReferencePoints , __pdoc__ )","title":"Module kerod.layers.smca.reference_points"},{"location":"reference/kerod/layers/smca/reference_points/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/smca/reference_points/#smcareferencepoints","text":"class SMCAReferencePoints ( hidden_dim : int , num_heads : int , ** kwargs ) Based on the object queries will create a set of reference points which will allow to create a spatial dynamical weight maps in order to modulate the co-attention inside the transformer","title":"SMCAReferencePoints"},{"location":"reference/kerod/layers/smca/reference_points/#arguments","text":"Name Description hidden_dim Positive integer, dimensionality of the hidden space. num_heads Positive integer, each head starts from a head-shared center and then predicts a head-specific center offset and head specific scales.","title":"Arguments"},{"location":"reference/kerod/layers/smca/reference_points/#call-arguments","text":"Name Description object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder.","title":"Call arguments"},{"location":"reference/kerod/layers/smca/reference_points/#call-returns","text":"Type Description Tuple - reference_points : A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - embedding_reference_points : A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied.","title":"Call returns"},{"location":"reference/kerod/layers/smca/reference_points/#ancestors-in-mro","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/smca/reference_points/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/smca/reference_points/#call","text":"def call ( self , object_queries ) Parameters: Name Description object_queries A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. View Source def call ( self , object_queries ) : \" \"\" Args: object_queries: A 3-D float32 Tensor of shape [batch_size, num_object_queries, d_model] small fixed number of learned positional embeddings input of the decoder. Call returns: Tuple: - `reference_points`: A float tensor of shape [batch_size, num_object_queries, num_heads, (y, x, w, h)]. - `embedding_reference_points`: A tensor of shape [batch_size, num_object_queries, num_heads, 2]. The embedding of y and x without the sigmoid applied. \"\" \" yx_pre_sigmoid = self . xy_embed ( object_queries ) # [bs, num_queries, 2] yx = tf . nn . sigmoid ( yx_pre_sigmoid ) # Where y and x are offsets predictions for 'yx' (above) # h and w are the scales predictions yx_offset_hw = self . yx_offset_hw_embed ( object_queries ) # [bs, num_queries, head * 4] batch_size = tf . shape ( object_queries ) [ 0 ] num_queries = tf . shape ( object_queries ) [ 1 ] # Add offset coordinates to yx and concatenate with scale predictions # yx => [bs, num_queries, head, 2] yx = tf . tile ( yx [ : , : , None ] , ( 1 , 1 , self . num_heads , 1 )) # yx_offset_hw => [bs, num_queries, head, 4] yx_offset_hw = tf . reshape ( yx_offset_hw , ( batch_size , - 1 , self . num_heads , 4 )) yxhw = tf . concat ( [ yx , tf . zeros (( batch_size , num_queries , self . num_heads , 2 )) ] , axis =- 1 ) return yxhw + yx_offset_hw , yx_pre_sigmoid","title":"call"},{"location":"reference/kerod/layers/smca/weight_map/","text":"Module kerod.layers.smca.weight_map None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class DynamicalWeightMaps ( tf . keras . layers . Layer ): \"\"\"Dynamical spatial weight maps described in [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Each object query first dynamically predicts the center and scale of its responsible object, which are then used to generate a 2-D Gaussian like spatial weight map. Example: This is an example of spatial weight maps from a 20 x 20 tensor and where we generated 25 random reference points. ![Spatial weight map](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/spatial_weight_map.png) This spatial_weight_map is then flatten to obtain a [25, 400] matrix representing the the modulated co-attention. ![Spatial weight map flatten](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/flatten_weight_map.png) Args: beta: Modulate the bandwidth of the Gaussian-like distribution. Call arguments: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Call returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. \"\"\" def __init__ ( self , beta = 8. , ** kwargs ): super () . __init__ ( ** kwargs ) self . _beta = beta def call ( self , height : int , width : int , ref_points : tf . Tensor ): \"\"\" Args: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. \"\"\" x = tf . cast ( tf . linspace ( 0 , 1 , width ), self . dtype ) y = tf . cast ( tf . linspace ( 0 , 1 , height ), self . dtype ) x , y = tf . meshgrid ( x , y ) # x and y have shape [height, width] ref_points = tf . transpose ( ref_points , ( 0 , 2 , 1 , 3 )) y_cent , x_cent , h , w = tf . split ( ref_points , 4 , axis =- 1 ) # [batch_size, heads, N, 1] => [batch_size,heads, N, 1, 1] y_cent , x_cent = y_cent [ ... , tf . newaxis ], x_cent [ ... , tf . newaxis ] h , w = h [ ... , tf . newaxis ], w [ ... , tf . newaxis ] # [height, width] => [1, 1, 1, height, width] y , x = y [ tf . newaxis , tf . newaxis , tf . newaxis ], x [ tf . newaxis , tf . newaxis , tf . newaxis ] # [batch_size, heads, N, height, width] beta = tf . cast ( tf . convert_to_tensor ( self . _beta ), self . dtype ) x_term = - ( x - x_cent ) ** 2 / ( beta * w ** 2 ) y_term = - ( y - y_cent ) ** 2 / ( beta * h ** 2 ) weight_map = tf . math . exp ( x_term + y_term ) batch_size , num_heads = tf . shape ( weight_map )[ 0 ], tf . shape ( weight_map )[ 1 ] return tf . reshape ( weight_map , ( batch_size , num_heads , - 1 , height * width )) def get_config ( self ): config = super () . get_config () config [ 'beta' ] = self . _beta return config remove_unwanted_doc ( DynamicalWeightMaps , __pdoc__ ) Classes DynamicalWeightMaps class DynamicalWeightMaps ( beta = 8.0 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . Each object query first dynamically predicts the center and scale of its responsible object, which are then used to generate a 2-D Gaussian like spatial weight map. Example: This is an example of spatial weight maps from a 20 x 20 tensor and where we generated 25 random reference points. This spatial_weight_map is then flatten to obtain a [25, 400] matrix representing the the modulated co-attention. Args: beta: Modulate the bandwidth of the Gaussian-like distribution. Call arguments: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Call returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , height : int , width : int , ref_points : tensorflow . python . framework . ops . Tensor ) Parameters: Name Description height The targeted height of the feature map width The targeted width of the feature map ref_points A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Returns: Type Description tf.Tensor The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. View Source def call ( self , height : int , width : int , ref_points : tf . Tensor ) : \"\"\" Args : height : The targeted height of the feature map width : The targeted width of the feature map ref_points : A tensor of shape [ batch_size , N , heads , ( y , x , h , w ) ] in [ 0 , 1 ] Returns : tf . Tensor : The weight_map per reference points a 4 D tensor of float 32 and shape [ batch_size , heads , N , height * width ]. \"\"\" x = tf . cast ( tf . linspace ( 0 , 1 , width ) , self . dtype ) y = tf . cast ( tf . linspace ( 0 , 1 , height ) , self . dtype ) x , y = tf . meshgrid ( x , y ) # x and y have shape [ height , width ] ref_points = tf . transpose ( ref_points , ( 0 , 2 , 1 , 3 )) y_cent , x_cent , h , w = tf . split ( ref_points , 4 , axis =- 1 ) # [ batch_size , heads , N , 1 ] => [ batch_size , heads , N , 1 , 1 ] y_cent , x_cent = y_cent [..., tf . newaxis ], x_cent [..., tf . newaxis ] h , w = h [..., tf . newaxis ], w [..., tf . newaxis ] # [ height , width ] => [ 1 , 1 , 1 , height , width ] y , x = y [ tf . newaxis , tf . newaxis , tf . newaxis ], x [ tf . newaxis , tf . newaxis , tf . newaxis ] # [ batch_size , heads , N , height , width ] beta = tf . cast ( tf . convert_to_tensor ( self . _beta ) , self . dtype ) x_term = - ( x - x_cent ) ** 2 / ( beta * w ** 2 ) y_term = - ( y - y_cent ) ** 2 / ( beta * h ** 2 ) weight_map = tf . math . exp ( x_term + y_term ) batch_size , num_heads = tf . shape ( weight_map ) [ 0 ], tf . shape ( weight_map ) [ 1 ] return tf . reshape ( weight_map , ( batch_size , num_heads , - 1 , height * width ))","title":"Weight Map"},{"location":"reference/kerod/layers/smca/weight_map/#module-kerodlayerssmcaweight_map","text":"None None View Source import tensorflow as tf from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class DynamicalWeightMaps ( tf . keras . layers . Layer ): \"\"\"Dynamical spatial weight maps described in [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). Each object query first dynamically predicts the center and scale of its responsible object, which are then used to generate a 2-D Gaussian like spatial weight map. Example: This is an example of spatial weight maps from a 20 x 20 tensor and where we generated 25 random reference points. ![Spatial weight map](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/spatial_weight_map.png) This spatial_weight_map is then flatten to obtain a [25, 400] matrix representing the the modulated co-attention. ![Spatial weight map flatten](https://raw.githubusercontent.com/EmGarr/kerod/master/ressources/flatten_weight_map.png) Args: beta: Modulate the bandwidth of the Gaussian-like distribution. Call arguments: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Call returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. \"\"\" def __init__ ( self , beta = 8. , ** kwargs ): super () . __init__ ( ** kwargs ) self . _beta = beta def call ( self , height : int , width : int , ref_points : tf . Tensor ): \"\"\" Args: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. \"\"\" x = tf . cast ( tf . linspace ( 0 , 1 , width ), self . dtype ) y = tf . cast ( tf . linspace ( 0 , 1 , height ), self . dtype ) x , y = tf . meshgrid ( x , y ) # x and y have shape [height, width] ref_points = tf . transpose ( ref_points , ( 0 , 2 , 1 , 3 )) y_cent , x_cent , h , w = tf . split ( ref_points , 4 , axis =- 1 ) # [batch_size, heads, N, 1] => [batch_size,heads, N, 1, 1] y_cent , x_cent = y_cent [ ... , tf . newaxis ], x_cent [ ... , tf . newaxis ] h , w = h [ ... , tf . newaxis ], w [ ... , tf . newaxis ] # [height, width] => [1, 1, 1, height, width] y , x = y [ tf . newaxis , tf . newaxis , tf . newaxis ], x [ tf . newaxis , tf . newaxis , tf . newaxis ] # [batch_size, heads, N, height, width] beta = tf . cast ( tf . convert_to_tensor ( self . _beta ), self . dtype ) x_term = - ( x - x_cent ) ** 2 / ( beta * w ** 2 ) y_term = - ( y - y_cent ) ** 2 / ( beta * h ** 2 ) weight_map = tf . math . exp ( x_term + y_term ) batch_size , num_heads = tf . shape ( weight_map )[ 0 ], tf . shape ( weight_map )[ 1 ] return tf . reshape ( weight_map , ( batch_size , num_heads , - 1 , height * width )) def get_config ( self ): config = super () . get_config () config [ 'beta' ] = self . _beta return config remove_unwanted_doc ( DynamicalWeightMaps , __pdoc__ )","title":"Module kerod.layers.smca.weight_map"},{"location":"reference/kerod/layers/smca/weight_map/#classes","text":"","title":"Classes"},{"location":"reference/kerod/layers/smca/weight_map/#dynamicalweightmaps","text":"class DynamicalWeightMaps ( beta = 8.0 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . Each object query first dynamically predicts the center and scale of its responsible object, which are then used to generate a 2-D Gaussian like spatial weight map. Example: This is an example of spatial weight maps from a 20 x 20 tensor and where we generated 25 random reference points. This spatial_weight_map is then flatten to obtain a [25, 400] matrix representing the the modulated co-attention. Args: beta: Modulate the bandwidth of the Gaussian-like distribution. Call arguments: height: The targeted height of the feature map width: The targeted width of the feature map ref_points: A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Call returns: tf.Tensor: The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width].","title":"DynamicalWeightMaps"},{"location":"reference/kerod/layers/smca/weight_map/#ancestors-in-mro","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/layers/smca/weight_map/#methods","text":"","title":"Methods"},{"location":"reference/kerod/layers/smca/weight_map/#call","text":"def call ( self , height : int , width : int , ref_points : tensorflow . python . framework . ops . Tensor ) Parameters: Name Description height The targeted height of the feature map width The targeted width of the feature map ref_points A tensor of shape [batch_size, N, heads, (y, x, h, w)] in [0, 1] Returns: Type Description tf.Tensor The weight_map per reference points a 4D tensor of float 32 and shape [batch_size, heads, N, height * width]. View Source def call ( self , height : int , width : int , ref_points : tf . Tensor ) : \"\"\" Args : height : The targeted height of the feature map width : The targeted width of the feature map ref_points : A tensor of shape [ batch_size , N , heads , ( y , x , h , w ) ] in [ 0 , 1 ] Returns : tf . Tensor : The weight_map per reference points a 4 D tensor of float 32 and shape [ batch_size , heads , N , height * width ]. \"\"\" x = tf . cast ( tf . linspace ( 0 , 1 , width ) , self . dtype ) y = tf . cast ( tf . linspace ( 0 , 1 , height ) , self . dtype ) x , y = tf . meshgrid ( x , y ) # x and y have shape [ height , width ] ref_points = tf . transpose ( ref_points , ( 0 , 2 , 1 , 3 )) y_cent , x_cent , h , w = tf . split ( ref_points , 4 , axis =- 1 ) # [ batch_size , heads , N , 1 ] => [ batch_size , heads , N , 1 , 1 ] y_cent , x_cent = y_cent [..., tf . newaxis ], x_cent [..., tf . newaxis ] h , w = h [..., tf . newaxis ], w [..., tf . newaxis ] # [ height , width ] => [ 1 , 1 , 1 , height , width ] y , x = y [ tf . newaxis , tf . newaxis , tf . newaxis ], x [ tf . newaxis , tf . newaxis , tf . newaxis ] # [ batch_size , heads , N , height , width ] beta = tf . cast ( tf . convert_to_tensor ( self . _beta ) , self . dtype ) x_term = - ( x - x_cent ) ** 2 / ( beta * w ** 2 ) y_term = - ( y - y_cent ) ** 2 / ( beta * h ** 2 ) weight_map = tf . math . exp ( x_term + y_term ) batch_size , num_heads = tf . shape ( weight_map ) [ 0 ], tf . shape ( weight_map ) [ 1 ] return tf . reshape ( weight_map , ( batch_size , num_heads , - 1 , height * width ))","title":"call"},{"location":"reference/kerod/model/","text":"Module kerod.model None None View Source from kerod.model.factory import KerodModel Sub-modules kerod.model.backbone kerod.model.detr kerod.model.factory kerod.model.faster_rcnn kerod.model.smca_detr","title":"Index"},{"location":"reference/kerod/model/#module-kerodmodel","text":"None None View Source from kerod.model.factory import KerodModel","title":"Module kerod.model"},{"location":"reference/kerod/model/#sub-modules","text":"kerod.model.backbone kerod.model.detr kerod.model.factory kerod.model.faster_rcnn kerod.model.smca_detr","title":"Sub-modules"},{"location":"reference/kerod/model/detr/","text":"Module kerod.model.detr None None View Source from typing import Dict import tensorflow as tf from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.python.keras.engine import data_adapter from tensorflow_addons.losses.giou_loss import GIoULoss from kerod.core.box_ops import ( convert_to_center_coordinates , convert_to_xyxy_coordinates ) from kerod.core.losses import L1Loss from kerod.core.matcher import hungarian_matching from kerod.core.similarity import DetrSimilarity from kerod.core.standard_fields import BoxField , DatasetField from kerod.core.target_assigner import TargetAssigner from kerod.layers import PositionEmbeddingSine , Transformer from kerod.layers.post_processing.post_processing_detr import \\ post_processing as detr_postprocessing from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.utils import item_assignment from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class DeTr ( tf . keras . Model ): \"\"\"Build a DeTr model according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) You can use it as follow: ```python model = DeTrResnet50Pytorch(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model.compile(optimizer=optimizer, loss=None) model.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A vision model like ResNet50. num_queries: number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Call returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" def __init__ ( self , num_classes : int , backbone , num_queries = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . num_queries = num_queries self . hidden_dim = 256 self . backbone = backbone self . input_proj = tf . keras . layers . Conv2D ( self . hidden_dim , 1 ) self . pos_embed = PositionEmbeddingSine ( output_dim = self . hidden_dim ) self . transformer_num_layers = 6 self . transformer = Transformer ( num_layers = self . transformer_num_layers , d_model = self . hidden_dim , num_heads = 8 , dim_feedforward = 2048 ) self . bbox_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 4 , activation = 'sigmoid' , dtype = tf . float32 ) # (x1, y1, x2, y2) ]) self . class_embed = tf . keras . layers . Dense ( num_classes + 1 , dtype = tf . float32 ) # Will create a learnable embedding matrix for all our queries # It is a matrix of [num_queries, self.hidden_dim] # The embedding layers self . query_embed = tf . keras . layers . Embedding ( num_queries , self . hidden_dim , embeddings_initializer = tf . keras . initializers . RandomNormal ( mean = 0. , stddev = 1. )) self . all_the_queries = tf . range ( num_queries ) # Loss computation self . weight_class , self . weight_l1 , self . weight_giou = 1 , 5 , 2 similarity_func = DetrSimilarity ( self . weight_class , self . weight_l1 , self . weight_giou ) self . target_assigner = TargetAssigner ( similarity_func , hungarian_matching , lambda gt , pred : gt , negative_class_weight = 1.0 ) # Relative classification weight applied to the no-object category # It down-weight the log-probability term of a no-object # by a factor 10 to account for class imbalance self . non_object_weight = tf . constant ( 0.1 , dtype = self . compute_dtype ) # Losses self . giou = GIoULoss ( reduction = tf . keras . losses . Reduction . NONE ) self . l1 = L1Loss ( reduction = tf . keras . losses . Reduction . NONE ) self . scc = SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ) # Metrics self . giou_metric = tf . keras . metrics . Mean ( name = \"giou_last_layer\" ) self . l1_metric = tf . keras . metrics . Mean ( name = \"l1_last_layer\" ) self . scc_metric = tf . keras . metrics . Mean ( name = \"scc_last_layer\" ) self . loss_metric = tf . keras . metrics . Mean ( name = \"loss\" ) self . precision_metric = tf . keras . metrics . SparseCategoricalAccuracy () # Object recall = foreground self . recall_metric = tf . keras . metrics . Mean ( name = \"object_recall\" ) @property def metrics ( self ): return [ self . loss_metric , self . giou_metric , self . l1_metric , self . scc_metric , self . precision_metric , self . recall_metric ] def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images )[ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images )[ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ... , None ], tf . float32 ), tf . shape ( x )[ 1 : 3 ], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ], ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } def compute_loss ( self , ground_truths : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: Scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField . BOXES ] / tf . tile ( input_shape [ None ], [ 1 , 2 ]) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField . BOXES ], self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField . SCORES ], self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [{ BoxField . BOXES : boxes , BoxField . SCORES : logits } for boxes , logits in zip ( boxes_per_lvl , logits_per_lvl )] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField . NUM_BOXES ]), tf . float32 ) loss = 0 # Compute the Giou, L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ): # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss def _compute_loss ( self , y_pred : Dict [ str , tf . Tensor ], ground_truths : Dict [ str , tf . Tensor ], num_boxes : int , compute_metrics = False , ) -> tf . Tensor : y_true , weights = self . target_assigner . assign ( y_pred , ground_truths ) # Reduce the class imbalanced by applying to the weights # self.non_object_weight for the non object (pos 0) weights [ BoxField . LABELS ] = item_assignment ( weights [ BoxField . LABELS ], y_true [ BoxField . LABELS ] == 0 , self . non_object_weight ) # Caveats GIoU is buggy and if the batch_size is 1 and the sample_weight # is provided will raise an error giou = self . giou ( convert_to_xyxy_coordinates ( y_true [ BoxField . BOXES ]), convert_to_xyxy_coordinates ( y_pred [ BoxField . BOXES ]), sample_weight = weights [ BoxField . BOXES ]) l1 = self . l1 ( y_true [ BoxField . BOXES ], y_pred [ BoxField . BOXES ], sample_weight = weights [ BoxField . BOXES ]) # SparseCategoricalCrossentropy scc = self . scc ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) giou = self . weight_giou * tf . reduce_sum ( giou ) / num_boxes l1 = self . weight_l1 * tf . reduce_sum ( l1 ) / num_boxes scc = self . weight_class * tf . reduce_sum ( scc ) / tf . reduce_sum ( weights [ BoxField . LABELS ]) if compute_metrics : self . giou_metric . update_state ( giou ) self . l1_metric . update_state ( l1 ) self . scc_metric . update_state ( scc ) self . precision_metric . update_state ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) recall = compute_detr_metrics ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ]) self . recall_metric . update_state ( recall ) return giou + l1 + scc def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): \"\"\"Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: boxes: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], ) return boxes_without_padding , scores , labels class DeTrResnet50 ( DeTr ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) class DeTrResnet50Pytorch ( DeTr ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) def compute_detr_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor ): \"\"\"Useful metrics that allows to track how behave the training. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_object_queries, num_classes] y_pred: A tensor with shape [batch_size, num_object_queries, num_classes], representing the classification logits. Returns: tf.Tensor: Recall Among all the boxes that we had to find how much did we found. \"\"\" #Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) recall = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = 'recall' ) return recall remove_unwanted_doc ( DeTr , __pdoc__ ) remove_unwanted_doc ( DeTrResnet50 , __pdoc__ ) remove_unwanted_doc ( DeTrResnet50Pytorch , __pdoc__ ) Functions compute_detr_metrics def compute_detr_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training. Parameters: Name Description y_true A one-hot encoded vector with shape [batch_size, num_object_queries, num_classes] y_pred A tensor with shape [batch_size, num_object_queries, num_classes], representing the classification logits. Returns: Type Description tf.Tensor Recall Among all the boxes that we had to find how much did we found. View Source def compute_detr_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training. Arguments: y_true: A one - hot encoded vector with shape [ batch_size , num_object_queries , num_classes ] y_pred: A tensor with shape [ batch_size , num_object_queries , num_classes ], representing the classification logits . Returns: tf . Tensor: Recall Among all the boxes that we had to find how much did we found . \"\"\" # Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = ' label_prediction ', output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) recall = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = ' recall ') return recall Classes DeTr class DeTr ( num_classes : int , backbone , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector Descendants kerod.model.detr.DeTrResnet50 kerod.model.detr.DeTrResnet50Pytorch Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor Scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: Scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) DeTrResnet50 class DeTrResnet50 ( num_classes , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) kerod.model.detr.DeTr tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor Scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: Scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) DeTrResnet50Pytorch class DeTrResnet50Pytorch ( num_classes , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) kerod.model.detr.DeTr tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor Scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: Scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"Detr"},{"location":"reference/kerod/model/detr/#module-kerodmodeldetr","text":"None None View Source from typing import Dict import tensorflow as tf from tensorflow.keras.losses import SparseCategoricalCrossentropy from tensorflow.python.keras.engine import data_adapter from tensorflow_addons.losses.giou_loss import GIoULoss from kerod.core.box_ops import ( convert_to_center_coordinates , convert_to_xyxy_coordinates ) from kerod.core.losses import L1Loss from kerod.core.matcher import hungarian_matching from kerod.core.similarity import DetrSimilarity from kerod.core.standard_fields import BoxField , DatasetField from kerod.core.target_assigner import TargetAssigner from kerod.layers import PositionEmbeddingSine , Transformer from kerod.layers.post_processing.post_processing_detr import \\ post_processing as detr_postprocessing from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.utils import item_assignment from kerod.utils.documentation import remove_unwanted_doc __pdoc__ = {} class DeTr ( tf . keras . Model ): \"\"\"Build a DeTr model according to the paper [End-to-End Object Detection with Transformers](https://arxiv.org/abs/2005.12872) You can use it as follow: ```python model = DeTrResnet50Pytorch(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model.compile(optimizer=optimizer, loss=None) model.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A vision model like ResNet50. num_queries: number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries. Call arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Call returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" def __init__ ( self , num_classes : int , backbone , num_queries = 100 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . num_queries = num_queries self . hidden_dim = 256 self . backbone = backbone self . input_proj = tf . keras . layers . Conv2D ( self . hidden_dim , 1 ) self . pos_embed = PositionEmbeddingSine ( output_dim = self . hidden_dim ) self . transformer_num_layers = 6 self . transformer = Transformer ( num_layers = self . transformer_num_layers , d_model = self . hidden_dim , num_heads = 8 , dim_feedforward = 2048 ) self . bbox_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 4 , activation = 'sigmoid' , dtype = tf . float32 ) # (x1, y1, x2, y2) ]) self . class_embed = tf . keras . layers . Dense ( num_classes + 1 , dtype = tf . float32 ) # Will create a learnable embedding matrix for all our queries # It is a matrix of [num_queries, self.hidden_dim] # The embedding layers self . query_embed = tf . keras . layers . Embedding ( num_queries , self . hidden_dim , embeddings_initializer = tf . keras . initializers . RandomNormal ( mean = 0. , stddev = 1. )) self . all_the_queries = tf . range ( num_queries ) # Loss computation self . weight_class , self . weight_l1 , self . weight_giou = 1 , 5 , 2 similarity_func = DetrSimilarity ( self . weight_class , self . weight_l1 , self . weight_giou ) self . target_assigner = TargetAssigner ( similarity_func , hungarian_matching , lambda gt , pred : gt , negative_class_weight = 1.0 ) # Relative classification weight applied to the no-object category # It down-weight the log-probability term of a no-object # by a factor 10 to account for class imbalance self . non_object_weight = tf . constant ( 0.1 , dtype = self . compute_dtype ) # Losses self . giou = GIoULoss ( reduction = tf . keras . losses . Reduction . NONE ) self . l1 = L1Loss ( reduction = tf . keras . losses . Reduction . NONE ) self . scc = SparseCategoricalCrossentropy ( reduction = tf . keras . losses . Reduction . NONE , from_logits = True ) # Metrics self . giou_metric = tf . keras . metrics . Mean ( name = \"giou_last_layer\" ) self . l1_metric = tf . keras . metrics . Mean ( name = \"l1_last_layer\" ) self . scc_metric = tf . keras . metrics . Mean ( name = \"scc_last_layer\" ) self . loss_metric = tf . keras . metrics . Mean ( name = \"loss\" ) self . precision_metric = tf . keras . metrics . SparseCategoricalAccuracy () # Object recall = foreground self . recall_metric = tf . keras . metrics . Mean ( name = \"object_recall\" ) @property def metrics ( self ): return [ self . loss_metric , self . giou_metric , self . l1_metric , self . scc_metric , self . precision_metric , self . recall_metric ] def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images )[ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images )[ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ... , None ], tf . float32 ), tf . shape ( x )[ 1 : 3 ], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ], ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } def compute_loss ( self , ground_truths : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: Scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField . BOXES ] / tf . tile ( input_shape [ None ], [ 1 , 2 ]) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField . BOXES ], self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField . SCORES ], self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [{ BoxField . BOXES : boxes , BoxField . SCORES : logits } for boxes , logits in zip ( boxes_per_lvl , logits_per_lvl )] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField . NUM_BOXES ]), tf . float32 ) loss = 0 # Compute the Giou, L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ): # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss def _compute_loss ( self , y_pred : Dict [ str , tf . Tensor ], ground_truths : Dict [ str , tf . Tensor ], num_boxes : int , compute_metrics = False , ) -> tf . Tensor : y_true , weights = self . target_assigner . assign ( y_pred , ground_truths ) # Reduce the class imbalanced by applying to the weights # self.non_object_weight for the non object (pos 0) weights [ BoxField . LABELS ] = item_assignment ( weights [ BoxField . LABELS ], y_true [ BoxField . LABELS ] == 0 , self . non_object_weight ) # Caveats GIoU is buggy and if the batch_size is 1 and the sample_weight # is provided will raise an error giou = self . giou ( convert_to_xyxy_coordinates ( y_true [ BoxField . BOXES ]), convert_to_xyxy_coordinates ( y_pred [ BoxField . BOXES ]), sample_weight = weights [ BoxField . BOXES ]) l1 = self . l1 ( y_true [ BoxField . BOXES ], y_pred [ BoxField . BOXES ], sample_weight = weights [ BoxField . BOXES ]) # SparseCategoricalCrossentropy scc = self . scc ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) giou = self . weight_giou * tf . reduce_sum ( giou ) / num_boxes l1 = self . weight_l1 * tf . reduce_sum ( l1 ) / num_boxes scc = self . weight_class * tf . reduce_sum ( scc ) / tf . reduce_sum ( weights [ BoxField . LABELS ]) if compute_metrics : self . giou_metric . update_state ( giou ) self . l1_metric . update_state ( l1 ) self . scc_metric . update_state ( scc ) self . precision_metric . update_state ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) recall = compute_detr_metrics ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ]) self . recall_metric . update_state ( recall ) return giou + l1 + scc def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): \"\"\"Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: boxes: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. scores: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. classes: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], ) return boxes_without_padding , scores , labels class DeTrResnet50 ( DeTr ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) class DeTrResnet50Pytorch ( DeTr ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) def compute_detr_metrics ( y_true : tf . Tensor , y_pred : tf . Tensor ): \"\"\"Useful metrics that allows to track how behave the training. Arguments: y_true: A one-hot encoded vector with shape [batch_size, num_object_queries, num_classes] y_pred: A tensor with shape [batch_size, num_object_queries, num_classes], representing the classification logits. Returns: tf.Tensor: Recall Among all the boxes that we had to find how much did we found. \"\"\" #Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = 'label_prediction' , output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) recall = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = 'recall' ) return recall remove_unwanted_doc ( DeTr , __pdoc__ ) remove_unwanted_doc ( DeTrResnet50 , __pdoc__ ) remove_unwanted_doc ( DeTrResnet50Pytorch , __pdoc__ )","title":"Module kerod.model.detr"},{"location":"reference/kerod/model/detr/#functions","text":"","title":"Functions"},{"location":"reference/kerod/model/detr/#compute_detr_metrics","text":"def compute_detr_metrics ( y_true : tensorflow . python . framework . ops . Tensor , y_pred : tensorflow . python . framework . ops . Tensor ) Useful metrics that allows to track how behave the training. Parameters: Name Description y_true A one-hot encoded vector with shape [batch_size, num_object_queries, num_classes] y_pred A tensor with shape [batch_size, num_object_queries, num_classes], representing the classification logits. Returns: Type Description tf.Tensor Recall Among all the boxes that we had to find how much did we found. View Source def compute_detr_metrics ( y_true: tf . Tensor , y_pred: tf . Tensor ) : \"\"\"Useful metrics that allows to track how behave the training. Arguments: y_true: A one - hot encoded vector with shape [ batch_size , num_object_queries , num_classes ] y_pred: A tensor with shape [ batch_size , num_object_queries , num_classes ], representing the classification logits . Returns: tf . Tensor: Recall Among all the boxes that we had to find how much did we found . \"\"\" # Even if the softmax has not been applyed the argmax can be usefull prediction = tf . argmax ( y_pred , axis =- 1 , name = ' label_prediction ', output_type = tf . int32 ) correct = tf . cast ( prediction == y_true , tf . float32 ) # Compute accuracy and false negative on all the foreground boxes fg_inds = tf . where ( y_true > 0 ) recall = tf . reduce_mean ( tf . gather_nd ( correct , fg_inds ), name = ' recall ') return recall","title":"compute_detr_metrics"},{"location":"reference/kerod/model/detr/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/detr/#detr","text":"class DeTr ( num_classes : int , backbone , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"DeTr"},{"location":"reference/kerod/model/detr/#arguments","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries.","title":"Arguments"},{"location":"reference/kerod/model/detr/#call-arguments","text":"Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/detr/#call-returns","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/detr/#ancestors-in-mro","text":"tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/detr/#descendants","text":"kerod.model.detr.DeTrResnet50 kerod.model.detr.DeTrResnet50Pytorch","title":"Descendants"},{"location":"reference/kerod/model/detr/#methods","text":"","title":"Methods"},{"location":"reference/kerod/model/detr/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/model/detr/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/model/detr/#add_update","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/model/detr/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/model/detr/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/model/detr/#apply","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/model/detr/#call","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/detr/#compute_loss","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor Scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: Scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/detr/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/model/detr/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/model/detr/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/model/detr/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/model/detr/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/model/detr/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/model/detr/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/model/detr/#get_losses_for","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/model/detr/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/model/detr/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/model/detr/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/model/detr/#get_updates_for","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/model/detr/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/model/detr/#detrresnet50","text":"class DeTrResnet50 ( num_classes , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"DeTrResnet50"},{"location":"reference/kerod/model/detr/#arguments_1","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries.","title":"Arguments"},{"location":"reference/kerod/model/detr/#call-arguments_1","text":"Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/detr/#call-returns_1","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/detr/#ancestors-in-mro_1","text":"kerod.model.detr.DeTr tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/detr/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/model/detr/#add_loss_1","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/model/detr/#add_metric_1","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/model/detr/#add_update_1","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/model/detr/#add_variable_1","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/model/detr/#add_weight_1","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/model/detr/#apply_1","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/model/detr/#call_1","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/detr/#compute_loss_1","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor Scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: Scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/detr/#compute_mask_1","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/model/detr/#compute_output_shape_1","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/model/detr/#compute_output_signature_1","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/model/detr/#count_params_1","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/model/detr/#get_input_at_1","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/model/detr/#get_input_mask_at_1","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/model/detr/#get_input_shape_at_1","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/model/detr/#get_losses_for_1","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/model/detr/#get_output_at_1","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/model/detr/#get_output_mask_at_1","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/model/detr/#get_output_shape_at_1","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/model/detr/#get_updates_for_1","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/model/detr/#set_weights_1","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/model/detr/#detrresnet50pytorch","text":"class DeTrResnet50Pytorch ( num_classes , num_queries = 100 , ** kwargs ) End-to-End Object Detection with Transformers You can use it as follow: model = DeTrResnet50Pytorch ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"DeTrResnet50Pytorch"},{"location":"reference/kerod/model/detr/#arguments_2","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects DETR can detect in a single image. For COCO, we recommend 100 queries.","title":"Arguments"},{"location":"reference/kerod/model/detr/#call-arguments_2","text":"Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/detr/#call-returns_2","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/detr/#ancestors-in-mro_2","text":"kerod.model.detr.DeTr tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/detr/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/model/detr/#add_loss_2","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/model/detr/#add_metric_2","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/model/detr/#add_update_2","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/model/detr/#add_variable_2","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/model/detr/#add_weight_2","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/model/detr/#apply_2","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/model/detr/#call_2","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) # add positional_encoding to x [batch_size, h, w, self.hidden_dim] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , training = training ) boxes = self . bbox_embed ( decoder_out ) logits = self . class_embed ( decoder_out ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/detr/#compute_loss_2","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor Scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Arguments: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: Scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/detr/#compute_mask_2","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/model/detr/#compute_output_shape_2","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/model/detr/#compute_output_signature_2","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/model/detr/#count_params_2","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/model/detr/#get_input_at_2","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/model/detr/#get_input_mask_at_2","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/model/detr/#get_input_shape_at_2","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/model/detr/#get_losses_for_2","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/model/detr/#get_output_at_2","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/model/detr/#get_output_mask_at_2","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/model/detr/#get_output_shape_at_2","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/model/detr/#get_updates_for_2","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/model/detr/#set_weights_2","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/model/factory/","text":"Module kerod.model.factory None None View Source from enum import Enum import tensorflow as tf from kerod.model.detr import DeTrResnet50 , DeTrResnet50Pytorch from kerod.model.faster_rcnn import ( FasterRcnnFPNResnet50Caffe , FasterRcnnFPNResnet50Pytorch ) from kerod.model.smca_detr import SMCAR50Pytorch from kerod.utils.training import ( freeze_batch_normalization , freeze_layers_before ) class KerodModel ( str , Enum ): faster_rcnn_resnet50_pytorch = 'resnet50_pytorch' faster_rcnn_resnet50_caffe = 'resnet50_caffe' detr_resnet50 = 'detr_resnet50_pytorch' detr_resnet50_caffe = 'detr_resnet50' smca_r50 = 'smca_resnet50' def build_model ( num_classes : int , name : str = KerodModel . faster_rcnn_resnet50_pytorch . value ) -> tf . keras . Model : \"\"\"Build a localization model with all the `tf.keras.layers.BatchNormalization` frozen and all the layers before second residual block. Args: num_classes: Number of classes of your model. Do not include the background class. name: Target model that you wish to use: 'resnet50_pytorch', 'resnet50_caffe', 'detrresnet50_pytorch', 'smca_resnet50'. Returns: A `keras.Model` instance. Raises: NotImplementedError: If the provided isn't supported \"\"\" if name == KerodModel . faster_rcnn_resnet50_pytorch : model = FasterRcnnFPNResnet50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model elif name == KerodModel . faster_rcnn_resnet50_caffe : model = FasterRcnnFPNResnet50Caffe ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'conv2_block3_out' ) return model elif name == KerodModel . detr_resnet50 : model = DeTrResnet50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model elif name == KerodModel . detr_resnet50_caffe : model = DeTrResnet50 ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'conv2_block3_out' ) return model elif name == KerodModel . smca_r50 : model = SMCAR50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model raise NotImplementedError ( f 'Name: { name } is not implemented.' ) Functions build_model def build_model ( num_classes : int , name : str = 'resnet50_pytorch' ) -> tensorflow . python . keras . engine . training . Model Build a localization model with all the tf.keras.layers.BatchNormalization frozen and all the layers before second residual block. Parameters: Name Description num_classes Number of classes of your model. Do not include the background class. name Target model that you wish to use: 'resnet50_pytorch', 'resnet50_caffe', 'detrresnet50_pytorch', 'smca_resnet50'. Returns: Type Description None A keras.Model instance. Raises: Type Description NotImplementedError If the provided isn't supported View Source def build_model ( num_classes : int , name : str = KerodModel . faster_rcnn_resnet50_pytorch . value ) -> tf . keras . Model : \"\"\"Build a localization model with all the `tf.keras.layers.BatchNormalization` frozen and all the layers before second residual block. Args: num_classes: Number of classes of your model. Do not include the background class. name: Target model that you wish to use: ' resnet50_pytorch ', ' resnet50_caffe ', ' detrresnet50_pytorch ', ' smca_resnet50 '. Returns: A `keras.Model` instance. Raises: NotImplementedError: If the provided isn' t supported \"\"\" if name == KerodModel.faster_rcnn_resnet50_pytorch: model = FasterRcnnFPNResnet50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model elif name == KerodModel.faster_rcnn_resnet50_caffe: model = FasterRcnnFPNResnet50Caffe(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' conv2_block3_out ') return model elif name == KerodModel.detr_resnet50: model = DeTrResnet50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model elif name == KerodModel.detr_resnet50_caffe: model = DeTrResnet50(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' conv2_block3_out ') return model elif name == KerodModel.smca_r50: model = SMCAR50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model raise NotImplementedError(f' Name : { name } is not implemented . ') Classes KerodModel class KerodModel ( / , * args , ** kwargs ) Ancestors (in MRO) builtins.str enum.Enum Class variables detr_resnet50 detr_resnet50_caffe faster_rcnn_resnet50_caffe faster_rcnn_resnet50_pytorch name smca_r50 value","title":"Factory"},{"location":"reference/kerod/model/factory/#module-kerodmodelfactory","text":"None None View Source from enum import Enum import tensorflow as tf from kerod.model.detr import DeTrResnet50 , DeTrResnet50Pytorch from kerod.model.faster_rcnn import ( FasterRcnnFPNResnet50Caffe , FasterRcnnFPNResnet50Pytorch ) from kerod.model.smca_detr import SMCAR50Pytorch from kerod.utils.training import ( freeze_batch_normalization , freeze_layers_before ) class KerodModel ( str , Enum ): faster_rcnn_resnet50_pytorch = 'resnet50_pytorch' faster_rcnn_resnet50_caffe = 'resnet50_caffe' detr_resnet50 = 'detr_resnet50_pytorch' detr_resnet50_caffe = 'detr_resnet50' smca_r50 = 'smca_resnet50' def build_model ( num_classes : int , name : str = KerodModel . faster_rcnn_resnet50_pytorch . value ) -> tf . keras . Model : \"\"\"Build a localization model with all the `tf.keras.layers.BatchNormalization` frozen and all the layers before second residual block. Args: num_classes: Number of classes of your model. Do not include the background class. name: Target model that you wish to use: 'resnet50_pytorch', 'resnet50_caffe', 'detrresnet50_pytorch', 'smca_resnet50'. Returns: A `keras.Model` instance. Raises: NotImplementedError: If the provided isn't supported \"\"\" if name == KerodModel . faster_rcnn_resnet50_pytorch : model = FasterRcnnFPNResnet50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model elif name == KerodModel . faster_rcnn_resnet50_caffe : model = FasterRcnnFPNResnet50Caffe ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'conv2_block3_out' ) return model elif name == KerodModel . detr_resnet50 : model = DeTrResnet50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model elif name == KerodModel . detr_resnet50_caffe : model = DeTrResnet50 ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'conv2_block3_out' ) return model elif name == KerodModel . smca_r50 : model = SMCAR50Pytorch ( num_classes ) freeze_batch_normalization ( model . backbone ) freeze_layers_before ( model . backbone , 'resnet50/group0/block2/last_relu' ) return model raise NotImplementedError ( f 'Name: { name } is not implemented.' )","title":"Module kerod.model.factory"},{"location":"reference/kerod/model/factory/#functions","text":"","title":"Functions"},{"location":"reference/kerod/model/factory/#build_model","text":"def build_model ( num_classes : int , name : str = 'resnet50_pytorch' ) -> tensorflow . python . keras . engine . training . Model Build a localization model with all the tf.keras.layers.BatchNormalization frozen and all the layers before second residual block. Parameters: Name Description num_classes Number of classes of your model. Do not include the background class. name Target model that you wish to use: 'resnet50_pytorch', 'resnet50_caffe', 'detrresnet50_pytorch', 'smca_resnet50'. Returns: Type Description None A keras.Model instance. Raises: Type Description NotImplementedError If the provided isn't supported View Source def build_model ( num_classes : int , name : str = KerodModel . faster_rcnn_resnet50_pytorch . value ) -> tf . keras . Model : \"\"\"Build a localization model with all the `tf.keras.layers.BatchNormalization` frozen and all the layers before second residual block. Args: num_classes: Number of classes of your model. Do not include the background class. name: Target model that you wish to use: ' resnet50_pytorch ', ' resnet50_caffe ', ' detrresnet50_pytorch ', ' smca_resnet50 '. Returns: A `keras.Model` instance. Raises: NotImplementedError: If the provided isn' t supported \"\"\" if name == KerodModel.faster_rcnn_resnet50_pytorch: model = FasterRcnnFPNResnet50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model elif name == KerodModel.faster_rcnn_resnet50_caffe: model = FasterRcnnFPNResnet50Caffe(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' conv2_block3_out ') return model elif name == KerodModel.detr_resnet50: model = DeTrResnet50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model elif name == KerodModel.detr_resnet50_caffe: model = DeTrResnet50(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' conv2_block3_out ') return model elif name == KerodModel.smca_r50: model = SMCAR50Pytorch(num_classes) freeze_batch_normalization(model.backbone) freeze_layers_before(model.backbone, ' resnet50 / group0 / block2 / last_relu ') return model raise NotImplementedError(f' Name : { name } is not implemented . ')","title":"build_model"},{"location":"reference/kerod/model/factory/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/factory/#kerodmodel","text":"class KerodModel ( / , * args , ** kwargs )","title":"KerodModel"},{"location":"reference/kerod/model/factory/#ancestors-in-mro","text":"builtins.str enum.Enum","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/factory/#class-variables","text":"detr_resnet50 detr_resnet50_caffe faster_rcnn_resnet50_caffe faster_rcnn_resnet50_pytorch name smca_r50 value","title":"Class variables"},{"location":"reference/kerod/model/faster_rcnn/","text":"Module kerod.model.faster_rcnn None None View Source import tensorflow as tf from kerod.core.standard_fields import BoxField , DatasetField from kerod.model.backbone.fpn import FPN from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.layers import FastRCNN , RegionProposalNetwork from kerod.layers.post_processing import ( post_process_fast_rcnn_boxes , post_process_rpn ) from kerod.utils.documentation import remove_unwanted_doc from kerod.utils.training import apply_kernel_regularization from tensorflow.python.keras.engine import data_adapter __pdoc__ = {} class FasterRcnnFPN ( tf . keras . Model ): \"\"\"Build a FPN Resnet 50 Faster RCNN network ready to use for training. You can use it as follow: ```python model_faster_rcnn = FasterRcnnFPNResnet50(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model_faster_rcnn.compile(optimizer=optimizer, loss=None) model_faster_rcnn.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A tensorflow Model. \"\"\" def __init__ ( self , num_classes , backbone , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . l2 = tf . keras . regularizers . l2 ( 1e-4 ) self . backbone = backbone self . fpn = FPN ( kernel_regularizer = self . l2 ) self . rpn = RegionProposalNetwork ( kernel_regularizer = self . l2 ) self . fast_rcnn = FastRCNN ( self . num_classes + 1 , kernel_regularizer = self . l2 ) # See docstring self.export_for_serving for usage self . _serving = False def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ]) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ([ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]], axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ([ pyramid , rois ]) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [ 'ground_truths' ] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes ) @tf . function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ]) def serving_step ( self , images , images_info ): \"\"\"Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\"\" return self . predict_step ({ DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info }) def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' ) def export_for_serving ( self , filepath ): \"\"\"Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\"\" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output }) self . _serving = False class FasterRcnnFPNResnet50Caffe ( FasterRcnnFPN ): def __init__ ( self , num_classes , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , ** kwargs ) class FasterRcnnFPNResnet50Pytorch ( FasterRcnnFPN ): def __init__ ( self , num_classes , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , ** kwargs ) remove_unwanted_doc ( FasterRcnnFPN , __pdoc__ ) remove_unwanted_doc ( FasterRcnnFPNResnet50Caffe , __pdoc__ ) remove_unwanted_doc ( FasterRcnnFPNResnet50Pytorch , __pdoc__ ) Classes FasterRcnnFPN class FasterRcnnFPN ( num_classes , backbone , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model. Ancestors (in MRO) tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector Descendants kerod.model.faster_rcnn.FasterRcnnFPNResnet50Caffe kerod.model.faster_rcnn.FasterRcnnFPNResnet50Pytorch Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) export_for_serving def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates serving_step def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) FasterRcnnFPNResnet50Caffe class FasterRcnnFPNResnet50Caffe ( num_classes , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model. Ancestors (in MRO) kerod.model.faster_rcnn.FasterRcnnFPN tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) export_for_serving def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates serving_step def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) FasterRcnnFPNResnet50Pytorch class FasterRcnnFPNResnet50Pytorch ( num_classes , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model. Ancestors (in MRO) kerod.model.faster_rcnn.FasterRcnnFPN tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) export_for_serving def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates serving_step def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } ) set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"Faster Rcnn"},{"location":"reference/kerod/model/faster_rcnn/#module-kerodmodelfaster_rcnn","text":"None None View Source import tensorflow as tf from kerod.core.standard_fields import BoxField , DatasetField from kerod.model.backbone.fpn import FPN from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.layers import FastRCNN , RegionProposalNetwork from kerod.layers.post_processing import ( post_process_fast_rcnn_boxes , post_process_rpn ) from kerod.utils.documentation import remove_unwanted_doc from kerod.utils.training import apply_kernel_regularization from tensorflow.python.keras.engine import data_adapter __pdoc__ = {} class FasterRcnnFPN ( tf . keras . Model ): \"\"\"Build a FPN Resnet 50 Faster RCNN network ready to use for training. You can use it as follow: ```python model_faster_rcnn = FasterRcnnFPNResnet50(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model_faster_rcnn.compile(optimizer=optimizer, loss=None) model_faster_rcnn.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A tensorflow Model. \"\"\" def __init__ ( self , num_classes , backbone , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . l2 = tf . keras . regularizers . l2 ( 1e-4 ) self . backbone = backbone self . fpn = FPN ( kernel_regularizer = self . l2 ) self . rpn = RegionProposalNetwork ( kernel_regularizer = self . l2 ) self . fast_rcnn = FastRCNN ( self . num_classes + 1 , kernel_regularizer = self . l2 ) # See docstring self.export_for_serving for usage self . _serving = False def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\"\" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ]) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ([ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]], axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ([ pyramid , rois ]) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois def train_step ( self , data ): # These are the only transformations `Model.fit` applies to user-input # data when a `tf.data.Dataset` is provided. These utilities will be exposed # publicly. data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : x [ 'ground_truths' ] = y y_pred = self ( x , training = True ) # All the losses are computed in the call. It can seems weird but it those # the job in a clean way. They are automatically added to self.losses loss = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , y , _ = data_adapter . unpack_x_y_sample_weight ( data ) x [ 'ground_truths' ] = y # In our graph all the metrics are computed inside the call method # So we set training to True to benefit from those metrics # Of course there is no backpropagation at the test step y_pred = self ( x , training = True ) _ = self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) classification_pred , localization_pred , rois = self ( x , training = False ) # Remove the background classes classification_pred = classification_pred [:, :, 1 :] return post_process_fast_rcnn_boxes ( classification_pred , localization_pred , rois , x [ DatasetField . IMAGES_INFO ], self . num_classes ) @tf . function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ]) def serving_step ( self , images , images_info ): \"\"\"Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\"\" return self . predict_step ({ DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info }) def save ( self , filepath , overwrite = True , include_optimizer = True , save_format = None , signatures = None , options = None ): try : super () . save ( filepath , overwrite = overwrite , include_optimizer = include_optimizer , save_format = save_format , signatures = signatures , options = options ) except Exception as e : raise Exception ( 'Saving does not work with dynamic inputs the ground_truths are injected in the inputs. ' 'Please use export_model method instead to bypass this error.' ) def export_for_serving ( self , filepath ): \"\"\"Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\"\" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output }) self . _serving = False class FasterRcnnFPNResnet50Caffe ( FasterRcnnFPN ): def __init__ ( self , num_classes , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , ** kwargs ) class FasterRcnnFPNResnet50Pytorch ( FasterRcnnFPN ): def __init__ ( self , num_classes , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , ** kwargs ) remove_unwanted_doc ( FasterRcnnFPN , __pdoc__ ) remove_unwanted_doc ( FasterRcnnFPNResnet50Caffe , __pdoc__ ) remove_unwanted_doc ( FasterRcnnFPNResnet50Pytorch , __pdoc__ )","title":"Module kerod.model.faster_rcnn"},{"location":"reference/kerod/model/faster_rcnn/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/faster_rcnn/#fasterrcnnfpn","text":"class FasterRcnnFPN ( num_classes , backbone , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"FasterRcnnFPN"},{"location":"reference/kerod/model/faster_rcnn/#arguments","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model.","title":"Arguments"},{"location":"reference/kerod/model/faster_rcnn/#ancestors-in-mro","text":"tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/faster_rcnn/#descendants","text":"kerod.model.faster_rcnn.FasterRcnnFPNResnet50Caffe kerod.model.faster_rcnn.FasterRcnnFPNResnet50Pytorch","title":"Descendants"},{"location":"reference/kerod/model/faster_rcnn/#methods","text":"","title":"Methods"},{"location":"reference/kerod/model/faster_rcnn/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/model/faster_rcnn/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/model/faster_rcnn/#add_update","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/model/faster_rcnn/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/model/faster_rcnn/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/model/faster_rcnn/#apply","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/model/faster_rcnn/#call","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois","title":"call"},{"location":"reference/kerod/model/faster_rcnn/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/model/faster_rcnn/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/model/faster_rcnn/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/model/faster_rcnn/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/model/faster_rcnn/#export_for_serving","text":"def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False","title":"export_for_serving"},{"location":"reference/kerod/model/faster_rcnn/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/model/faster_rcnn/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/model/faster_rcnn/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/model/faster_rcnn/#get_losses_for","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/model/faster_rcnn/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/model/faster_rcnn/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/model/faster_rcnn/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/model/faster_rcnn/#get_updates_for","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/model/faster_rcnn/#serving_step","text":"def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } )","title":"serving_step"},{"location":"reference/kerod/model/faster_rcnn/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/model/faster_rcnn/#fasterrcnnfpnresnet50caffe","text":"class FasterRcnnFPNResnet50Caffe ( num_classes , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"FasterRcnnFPNResnet50Caffe"},{"location":"reference/kerod/model/faster_rcnn/#arguments_1","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model.","title":"Arguments"},{"location":"reference/kerod/model/faster_rcnn/#ancestors-in-mro_1","text":"kerod.model.faster_rcnn.FasterRcnnFPN tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/faster_rcnn/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/model/faster_rcnn/#add_loss_1","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/model/faster_rcnn/#add_metric_1","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/model/faster_rcnn/#add_update_1","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/model/faster_rcnn/#add_variable_1","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/model/faster_rcnn/#add_weight_1","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/model/faster_rcnn/#apply_1","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/model/faster_rcnn/#call_1","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois","title":"call"},{"location":"reference/kerod/model/faster_rcnn/#compute_mask_1","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/model/faster_rcnn/#compute_output_shape_1","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/model/faster_rcnn/#compute_output_signature_1","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/model/faster_rcnn/#count_params_1","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/model/faster_rcnn/#export_for_serving_1","text":"def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False","title":"export_for_serving"},{"location":"reference/kerod/model/faster_rcnn/#get_input_at_1","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/model/faster_rcnn/#get_input_mask_at_1","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/model/faster_rcnn/#get_input_shape_at_1","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/model/faster_rcnn/#get_losses_for_1","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/model/faster_rcnn/#get_output_at_1","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/model/faster_rcnn/#get_output_mask_at_1","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/model/faster_rcnn/#get_output_shape_at_1","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/model/faster_rcnn/#get_updates_for_1","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/model/faster_rcnn/#serving_step_1","text":"def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } )","title":"serving_step"},{"location":"reference/kerod/model/faster_rcnn/#set_weights_1","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/model/faster_rcnn/#fasterrcnnfpnresnet50pytorch","text":"class FasterRcnnFPNResnet50Pytorch ( num_classes , ** kwargs ) You can use it as follow: model_faster_rcnn = FasterRcnnFPNResnet50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model_faster_rcnn . compile ( optimizer = optimizer , loss = None ) model_faster_rcnn . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"FasterRcnnFPNResnet50Pytorch"},{"location":"reference/kerod/model/faster_rcnn/#arguments_2","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A tensorflow Model.","title":"Arguments"},{"location":"reference/kerod/model/faster_rcnn/#ancestors-in-mro_2","text":"kerod.model.faster_rcnn.FasterRcnnFPN tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/faster_rcnn/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/model/faster_rcnn/#add_loss_2","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/model/faster_rcnn/#add_metric_2","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/model/faster_rcnn/#add_update_2","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/model/faster_rcnn/#add_variable_2","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/model/faster_rcnn/#add_weight_2","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/model/faster_rcnn/#apply_2","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/model/faster_rcnn/#call_2","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs A dict with the following schema: images : A Tensor of shape [batch_size, height, width, 3] image_informations : A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). ground_truths : A dict - BoxField.LABELS : A 3-D tensor of shape [batch_size, num_gt, num_classes], - BoxField.BOXES : A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - BoxField.LABELS : A 3-D tensor of int32 and shape [batch_size, num_gt] - BoxField.WEIGHTS : A 3-D tensor of float and shape [batch_size, num_gt] - BoxField.NUM_BOXES : A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training Is automatically set to True in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with add_loss and add_metrics . In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the self.compiled_loss method. Returns: Type Description Tuple - classification_pred : A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - localization_pred : A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - anchors : A Tensor of shape [batch_size, num_boxes, 4] View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: A dict with the following schema: `images`: A Tensor of shape [batch_size, height, width, 3] `image_informations`: A float32 Tensor of shape [batch_size, 2] where the last dimension represents the original height and width of the images (without the padding). `ground_truths`: A dict - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes], - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)] - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt] - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt] - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1] which allows to remove the padding created by tf.Data. Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32) then my second box has a padding of 1 training: Is automatically set to `True` in train and test mode (normally test should be at false). Why? Through the call we the losses and the metrics of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`. In test we want to benefit from those and therefore we compute them. It is an inheritance from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the train_step and test_step. However for now this method benefit of the encapsulation of the `self.compiled_loss` method. Returns: Tuple: - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes] representing the class probability. - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)] - `anchors`: A Tensor of shape [batch_size, num_boxes, 4] \"\" \" images = inputs [ DatasetField . IMAGES ] images_information = inputs [ DatasetField . IMAGES_INFO ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) pyramid = self . fpn ( x ) rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl = self . rpn ( pyramid ) if training and not self . _serving : apply_kernel_regularization ( self . l2 , self . backbone ) # add_loss stores the rpn losses computation in self.losses _ = self . rpn . compute_loss ( rpn_loc_pred_per_lvl , rpn_cls_pred_per_lvl , anchors_per_lvl , inputs [ 'ground_truths' ] ) num_boxes = 2000 if training else 1000 rois = post_process_rpn ( rpn_cls_pred_per_lvl , rpn_loc_pred_per_lvl , anchors_per_lvl , images_information , pre_nms_topk_per_lvl = num_boxes , post_nms_topk = num_boxes ) if training and not self . _serving : ground_truths = inputs [ 'ground_truths' ] # Include the ground_truths as RoIs for the training rois = tf . concat ( [ tf . cast ( rois , self . _compute_dtype ), ground_truths [ BoxField . BOXES ]] , axis = 1 ) # Sample the boxes needed for inference y_true , weights , rois = self . fast_rcnn . sample_boxes ( rois , ground_truths ) classification_pred , localization_pred = self . fast_rcnn ( [ pyramid , rois ] ) if training and not self . _serving : # add_loss stores the fast_rcnn losses computation in self.losses _ = self . fast_rcnn . compute_loss ( y_true , weights , classification_pred , localization_pred ) classification_pred = tf . nn . softmax ( classification_pred ) return classification_pred , localization_pred , rois","title":"call"},{"location":"reference/kerod/model/faster_rcnn/#compute_mask_2","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/model/faster_rcnn/#compute_output_shape_2","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/model/faster_rcnn/#compute_output_signature_2","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/model/faster_rcnn/#count_params_2","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/model/faster_rcnn/#export_for_serving_2","text":"def export_for_serving ( self , filepath ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the training arguments is defined int the method call , tf.save_model.save method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. View Source def export_for_serving ( self , filepath ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow when the `training` arguments is defined int the method `call`, `tf.save_model.save` method performs a check on the graph for training=False and training=True. However, we don't want this check to be perform because our ground_truths inputs aren't defined. \"\" \" self . _serving = True call_output = self . serving_step . get_concrete_function () tf . saved_model . save ( self , filepath , signatures = { 'serving_default' : call_output } ) self . _serving = False","title":"export_for_serving"},{"location":"reference/kerod/model/faster_rcnn/#get_input_at_2","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/model/faster_rcnn/#get_input_mask_at_2","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/model/faster_rcnn/#get_input_shape_at_2","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/model/faster_rcnn/#get_losses_for_2","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/model/faster_rcnn/#get_output_at_2","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/model/faster_rcnn/#get_output_mask_at_2","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/model/faster_rcnn/#get_output_shape_at_2","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/model/faster_rcnn/#get_updates_for_2","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/model/faster_rcnn/#serving_step_2","text":"def serving_step ( self , images , images_info ) Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the images and images_information are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. View Source @tf.function ( input_signature = [ tf . TensorSpec ( shape = ( None , None , None , 3 ), dtype = tf . float32 , name = DatasetField . IMAGES ), tf . TensorSpec ( shape = ( None , 2 ), dtype = tf . float32 , name = DatasetField . IMAGES_INFO ) ] ) def serving_step ( self , images , images_info ) : \" \"\" Allow to bypass the save_model behavior the graph in serving mode. Currently, the issue is that in training the ground_truths are passed to the call method but not in inference. For the serving only the `images` and `images_information` are defined. It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow absolutely want it and will return an exception if the ground_truth isn't provided. \"\" \" return self . predict_step ( { DatasetField . IMAGES : images , DatasetField . IMAGES_INFO : images_info } )","title":"serving_step"},{"location":"reference/kerod/model/faster_rcnn/#set_weights_2","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/model/smca_detr/","text":"Module kerod.model.smca_detr None None View Source from typing import Dict import tensorflow as tf import tensorflow_addons as tfa from kerod.core.box_ops import ( convert_to_center_coordinates , convert_to_xyxy_coordinates ) from kerod.core.losses import L1Loss from kerod.core.matcher import hungarian_matching from kerod.core.similarity import DetrSimilarity from kerod.core.standard_fields import BoxField , DatasetField from kerod.core.target_assigner import TargetAssigner from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.model.detr import compute_detr_metrics from kerod.layers import ( DynamicalWeightMaps , PositionEmbeddingSine , Transformer , SMCAReferencePoints ) from kerod.layers.post_processing.post_processing_detr import \\ post_processing as detr_postprocessing from kerod.utils.documentation import remove_unwanted_doc from tensorflow.python.keras.engine import data_adapter __pdoc__ = {} class SMCA ( tf . keras . Model ): \"\"\"Build a single scale SCMA model according to the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: ```python model = SMCAR50(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model.compile(optimizer=optimizer, loss=None) model.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A vision model like ResNet50. num_queries: number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Call returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" def __init__ ( self , num_classes , backbone , num_queries = 300 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . num_queries = num_queries self . hidden_dim = 256 self . backbone = backbone self . input_proj = tf . keras . layers . Conv2D ( self . hidden_dim , 1 ) self . pos_embed = PositionEmbeddingSine ( output_dim = self . hidden_dim ) num_heads = 8 self . transformer_num_layers = 6 self . transformer = Transformer ( num_layers = self . transformer_num_layers , d_model = self . hidden_dim , num_heads = num_heads , dim_feedforward = 2048 ) # MCMA layers self . dyn_weight_map = DynamicalWeightMaps () self . ref_points = SMCAReferencePoints ( self . hidden_dim , num_heads ) self . bbox_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 4 , dtype = tf . float32 ) # (x1, y1, x2, y2) ]) self . class_embed = tf . keras . layers . Dense ( num_classes + 1 , dtype = tf . float32 ) # Will create a learnable embedding matrix for all our queries # It is a matrix of [num_queries, self.hidden_dim] # The embedding layers self . query_embed = tf . keras . layers . Embedding ( num_queries , self . hidden_dim , embeddings_initializer = tf . keras . initializers . RandomNormal ( mean = 0. , stddev = 1. )) self . all_the_queries = tf . range ( num_queries ) # Loss computation self . weight_class , self . weight_l1 , self . weight_giou = 2 , 5 , 2 similarity_func = DetrSimilarity ( self . weight_class , self . weight_l1 , self . weight_giou ) self . target_assigner = TargetAssigner ( similarity_func , hungarian_matching , lambda gt , pred : gt , negative_class_weight = 1.0 ) # Losses self . giou = tfa . losses . GIoULoss ( reduction = tf . keras . losses . Reduction . NONE ) self . l1 = L1Loss ( reduction = tf . keras . losses . Reduction . NONE ) self . focal_loss = tfa . losses . SigmoidFocalCrossEntropy ( alpha = 0.25 , gamma = 2 , reduction = tf . keras . losses . Reduction . NONE , from_logits = True ) # Metrics self . giou_metric = tf . keras . metrics . Mean ( name = \"giou_last_layer\" ) self . l1_metric = tf . keras . metrics . Mean ( name = \"l1_last_layer\" ) self . focal_loss_metric = tf . keras . metrics . Mean ( name = \"focal_loss_last_layer\" ) self . loss_metric = tf . keras . metrics . Mean ( name = \"loss\" ) self . precision_metric = tf . keras . metrics . SparseCategoricalAccuracy () # Object recall = foreground self . recall_metric = tf . keras . metrics . Mean ( name = \"object_recall\" ) @property def metrics ( self ): return [ self . loss_metric , self . giou_metric , self . l1_metric , self . focal_loss_metric , self . precision_metric , self . recall_metric ] def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images )[ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images )[ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ... , None ], tf . float32 ), tf . shape ( x )[ 1 : 3 ], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ], ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x )[ 1 ], tf . shape ( x )[ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid )[ 1 ], 2 ))], axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } def compute_loss ( self , ground_truths : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: A scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField . BOXES ] / tf . tile ( input_shape [ None ], [ 1 , 2 ]) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField . BOXES ], self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField . SCORES ], self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [{ BoxField . BOXES : boxes , BoxField . SCORES : logits } for boxes , logits in zip ( boxes_per_lvl , logits_per_lvl )] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField . NUM_BOXES ]), tf . float32 ) loss = 0 # Compute the Giou, L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ): # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss def _compute_loss ( self , y_pred : Dict [ str , tf . Tensor ], ground_truths : Dict [ str , tf . Tensor ], num_boxes : int , compute_metrics = False , ): y_true , weights = self . target_assigner . assign ( y_pred , ground_truths ) # Caveats GIoU is buggy and if the batch_size is 1 and the sample_weight # is provided will raise an error giou = self . giou ( convert_to_xyxy_coordinates ( y_true [ BoxField . BOXES ]), convert_to_xyxy_coordinates ( y_pred [ BoxField . BOXES ]), sample_weight = weights [ BoxField . BOXES ]) l1 = self . l1 ( y_true [ BoxField . BOXES ], y_pred [ BoxField . BOXES ], sample_weight = weights [ BoxField . BOXES ]) cls_labels = tf . one_hot ( y_true [ BoxField . LABELS ], depth = self . num_classes + 1 , dtype = tf . float32 , ) focal_loss = self . focal_loss ( cls_labels , y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) giou = self . weight_giou * tf . reduce_sum ( giou ) / num_boxes l1 = self . weight_l1 * tf . reduce_sum ( l1 ) / num_boxes focal_loss = self . weight_class * tf . reduce_sum ( focal_loss ) / tf . reduce_sum ( weights [ BoxField . LABELS ]) if compute_metrics : self . giou_metric . update_state ( giou ) self . l1_metric . update_state ( l1 ) self . focal_loss_metric . update_state ( focal_loss ) self . precision_metric . update_state ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) recall = compute_detr_metrics ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ]) self . recall_metric . update_state ( recall ) return giou + l1 + focal_loss def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): \"\"\"Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], ) return boxes_without_padding , scores , labels class SMCAR50 ( SMCA ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) class SMCAR50Pytorch ( SMCA ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) remove_unwanted_doc ( SMCA , __pdoc__ ) remove_unwanted_doc ( SMCAR50 , __pdoc__ ) remove_unwanted_doc ( SMCAR50Pytorch , __pdoc__ ) Classes SMCA class SMCA ( num_classes , backbone , num_queries = 300 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector Descendants kerod.model.smca_detr.SMCAR50 kerod.model.smca_detr.SMCAR50Pytorch Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor A scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: A scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) SMCAR50 class SMCAR50 ( num_classes , num_queries = 100 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) kerod.model.smca_detr.SMCA tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor A scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: A scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples ) SMCAR50Pytorch class SMCAR50Pytorch ( num_classes , num_queries = 100 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,) Arguments Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Call returns Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. Ancestors (in MRO) kerod.model.smca_detr.SMCA tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector Methods add_loss def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss ) add_metric def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name ) add_update def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable add_variable def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs ) add_weight def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable apply def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs ) call def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } compute_loss def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor A scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: A scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss compute_mask def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask compute_output_shape def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ ) compute_output_signature def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape ) count_params def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights ) get_input_at def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' ) get_input_mask_at def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None ) get_input_shape_at def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' ) get_losses_for def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses get_output_at def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' ) get_output_mask_at def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None ) get_output_shape_at def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' ) get_updates_for def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates set_weights def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"Smca Detr"},{"location":"reference/kerod/model/smca_detr/#module-kerodmodelsmca_detr","text":"None None View Source from typing import Dict import tensorflow as tf import tensorflow_addons as tfa from kerod.core.box_ops import ( convert_to_center_coordinates , convert_to_xyxy_coordinates ) from kerod.core.losses import L1Loss from kerod.core.matcher import hungarian_matching from kerod.core.similarity import DetrSimilarity from kerod.core.standard_fields import BoxField , DatasetField from kerod.core.target_assigner import TargetAssigner from kerod.model.backbone.resnet import ResNet50 , ResNet50PytorchStyle from kerod.model.detr import compute_detr_metrics from kerod.layers import ( DynamicalWeightMaps , PositionEmbeddingSine , Transformer , SMCAReferencePoints ) from kerod.layers.post_processing.post_processing_detr import \\ post_processing as detr_postprocessing from kerod.utils.documentation import remove_unwanted_doc from tensorflow.python.keras.engine import data_adapter __pdoc__ = {} class SMCA ( tf . keras . Model ): \"\"\"Build a single scale SCMA model according to the paper [Fast Convergence of DETR with Spatially Modulated Co-Attention](https://arxiv.org/pdf/2101.07448.pdf). In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: ```python model = SMCAR50(80) base_lr = 0.1 optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr) model.compile(optimizer=optimizer, loss=None) model.fit(ds_train, validation_data=ds_test, epochs=11,) ``` Arguments: num_classes: The number of classes of your dataset (**do not include the background class** it is handle for you) backbone: A vision model like ResNet50. num_queries: number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries. Call arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Call returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" def __init__ ( self , num_classes , backbone , num_queries = 300 , ** kwargs ): super () . __init__ ( ** kwargs ) self . num_classes = num_classes self . num_queries = num_queries self . hidden_dim = 256 self . backbone = backbone self . input_proj = tf . keras . layers . Conv2D ( self . hidden_dim , 1 ) self . pos_embed = PositionEmbeddingSine ( output_dim = self . hidden_dim ) num_heads = 8 self . transformer_num_layers = 6 self . transformer = Transformer ( num_layers = self . transformer_num_layers , d_model = self . hidden_dim , num_heads = num_heads , dim_feedforward = 2048 ) # MCMA layers self . dyn_weight_map = DynamicalWeightMaps () self . ref_points = SMCAReferencePoints ( self . hidden_dim , num_heads ) self . bbox_embed = tf . keras . models . Sequential ([ tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( self . hidden_dim , activation = 'relu' ), tf . keras . layers . Dense ( 4 , dtype = tf . float32 ) # (x1, y1, x2, y2) ]) self . class_embed = tf . keras . layers . Dense ( num_classes + 1 , dtype = tf . float32 ) # Will create a learnable embedding matrix for all our queries # It is a matrix of [num_queries, self.hidden_dim] # The embedding layers self . query_embed = tf . keras . layers . Embedding ( num_queries , self . hidden_dim , embeddings_initializer = tf . keras . initializers . RandomNormal ( mean = 0. , stddev = 1. )) self . all_the_queries = tf . range ( num_queries ) # Loss computation self . weight_class , self . weight_l1 , self . weight_giou = 2 , 5 , 2 similarity_func = DetrSimilarity ( self . weight_class , self . weight_l1 , self . weight_giou ) self . target_assigner = TargetAssigner ( similarity_func , hungarian_matching , lambda gt , pred : gt , negative_class_weight = 1.0 ) # Losses self . giou = tfa . losses . GIoULoss ( reduction = tf . keras . losses . Reduction . NONE ) self . l1 = L1Loss ( reduction = tf . keras . losses . Reduction . NONE ) self . focal_loss = tfa . losses . SigmoidFocalCrossEntropy ( alpha = 0.25 , gamma = 2 , reduction = tf . keras . losses . Reduction . NONE , from_logits = True ) # Metrics self . giou_metric = tf . keras . metrics . Mean ( name = \"giou_last_layer\" ) self . l1_metric = tf . keras . metrics . Mean ( name = \"l1_last_layer\" ) self . focal_loss_metric = tf . keras . metrics . Mean ( name = \"focal_loss_last_layer\" ) self . loss_metric = tf . keras . metrics . Mean ( name = \"loss\" ) self . precision_metric = tf . keras . metrics . SparseCategoricalAccuracy () # Object recall = foreground self . recall_metric = tf . keras . metrics . Mean ( name = \"object_recall\" ) @property def metrics ( self ): return [ self . loss_metric , self . giou_metric , self . l1_metric , self . focal_loss_metric , self . precision_metric , self . recall_metric ] def call ( self , inputs , training = None ): \"\"\"Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\"\" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images )[ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images )[ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ... , None ], tf . float32 ), tf . shape ( x )[ 1 : 3 ], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ], ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x )[ 1 ], tf . shape ( x )[ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid )[ 1 ], 2 ))], axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , } def compute_loss ( self , ground_truths : Dict [ str , tf . Tensor ], y_pred : Dict [ str , tf . Tensor ], input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: A scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField . BOXES ] / tf . tile ( input_shape [ None ], [ 1 , 2 ]) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [BoxField.LABELS] BoxField . LABELS : ground_truths [ BoxField . LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField . WEIGHTS ], BoxField . NUM_BOXES : ground_truths [ BoxField . NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField . BOXES ], self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField . SCORES ], self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [{ BoxField . BOXES : boxes , BoxField . SCORES : logits } for boxes , logits in zip ( boxes_per_lvl , logits_per_lvl )] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField . NUM_BOXES ]), tf . float32 ) loss = 0 # Compute the Giou, L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ): # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss def _compute_loss ( self , y_pred : Dict [ str , tf . Tensor ], ground_truths : Dict [ str , tf . Tensor ], num_boxes : int , compute_metrics = False , ): y_true , weights = self . target_assigner . assign ( y_pred , ground_truths ) # Caveats GIoU is buggy and if the batch_size is 1 and the sample_weight # is provided will raise an error giou = self . giou ( convert_to_xyxy_coordinates ( y_true [ BoxField . BOXES ]), convert_to_xyxy_coordinates ( y_pred [ BoxField . BOXES ]), sample_weight = weights [ BoxField . BOXES ]) l1 = self . l1 ( y_true [ BoxField . BOXES ], y_pred [ BoxField . BOXES ], sample_weight = weights [ BoxField . BOXES ]) cls_labels = tf . one_hot ( y_true [ BoxField . LABELS ], depth = self . num_classes + 1 , dtype = tf . float32 , ) focal_loss = self . focal_loss ( cls_labels , y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) giou = self . weight_giou * tf . reduce_sum ( giou ) / num_boxes l1 = self . weight_l1 * tf . reduce_sum ( l1 ) / num_boxes focal_loss = self . weight_class * tf . reduce_sum ( focal_loss ) / tf . reduce_sum ( weights [ BoxField . LABELS ]) if compute_metrics : self . giou_metric . update_state ( giou ) self . l1_metric . update_state ( l1 ) self . focal_loss_metric . update_state ( focal_loss ) self . precision_metric . update_state ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ], sample_weight = weights [ BoxField . LABELS ]) recall = compute_detr_metrics ( y_true [ BoxField . LABELS ], y_pred [ BoxField . SCORES ]) self . recall_metric . update_state ( recall ) return giou + l1 + focal_loss def train_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) with tf . GradientTape () as tape : y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . optimizer . minimize ( loss , self . trainable_variables , tape = tape ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def test_step ( self , data ): data = data_adapter . expand_1d ( data ) x , ground_truths , _ = data_adapter . unpack_x_y_sample_weight ( data ) # To compute the loss we need to get the results of each decoder layer # Setting training to True will provide it y_pred = self ( x , training = True ) input_shape = tf . cast ( tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], self . compute_dtype ) loss = self . compute_loss ( ground_truths , y_pred , input_shape ) loss += self . compiled_loss ( None , y_pred , None , regularization_losses = self . losses ) self . loss_metric . update_state ( loss ) return { m . name : m . result () for m in self . metrics } def predict_step ( self , data ): \"\"\"Perform an inference and returns the boxes, scores and labels associated. Background is discarded the max and argmax operation are performed. It means that if background was predicted the second maximum score would be outputed. Example: background + 3 classes [0.54, 0.40, 0.03, 0.03] => score = 0.40, label = 0 (1 - 1) \"To optimize for AP, we override the prediction of these slots with the second highest scoring class, using the corresponding confidence\" Part 4. Experiments of Object Detection with Transformers Returns: Tuple: - `boxes`: A Tensor of shape [batch_size, self.num_queries, (y1,x1,y2,x2)] containing the boxes with the coordinates between 0 and 1. - `scores`: A Tensor of shape [batch_size, self.num_queries] containing the score of the boxes. - `classes`: A Tensor of shape [batch_size, self.num_queries] containing the class of the boxes [0, num_classes). \"\"\" data = data_adapter . expand_1d ( data ) x , _ , _ = data_adapter . unpack_x_y_sample_weight ( data ) y_pred = self ( x , training = False ) boxes_without_padding , scores , labels = detr_postprocessing ( y_pred [ BoxField . BOXES ], y_pred [ BoxField . SCORES ], x [ DatasetField . IMAGES_INFO ], tf . shape ( x [ DatasetField . IMAGES ])[ 1 : 3 ], ) return boxes_without_padding , scores , labels class SMCAR50 ( SMCA ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50 ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) class SMCAR50Pytorch ( SMCA ): def __init__ ( self , num_classes , num_queries = 100 , ** kwargs ): resnet = ResNet50PytorchStyle ( input_shape = [ None , None , 3 ], weights = 'imagenet' ) super () . __init__ ( num_classes , resnet , num_queries = num_queries , ** kwargs ) remove_unwanted_doc ( SMCA , __pdoc__ ) remove_unwanted_doc ( SMCAR50 , __pdoc__ ) remove_unwanted_doc ( SMCAR50Pytorch , __pdoc__ )","title":"Module kerod.model.smca_detr"},{"location":"reference/kerod/model/smca_detr/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/smca_detr/#smca","text":"class SMCA ( num_classes , backbone , num_queries = 300 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"SMCA"},{"location":"reference/kerod/model/smca_detr/#arguments","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries.","title":"Arguments"},{"location":"reference/kerod/model/smca_detr/#call-arguments","text":"Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/smca_detr/#call-returns","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/smca_detr/#ancestors-in-mro","text":"tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/smca_detr/#descendants","text":"kerod.model.smca_detr.SMCAR50 kerod.model.smca_detr.SMCAR50Pytorch","title":"Descendants"},{"location":"reference/kerod/model/smca_detr/#methods","text":"","title":"Methods"},{"location":"reference/kerod/model/smca_detr/#add_loss","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/model/smca_detr/#add_metric","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/model/smca_detr/#add_update","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/model/smca_detr/#add_variable","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/model/smca_detr/#add_weight","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/model/smca_detr/#apply","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/model/smca_detr/#call","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/smca_detr/#compute_loss","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor A scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: A scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/smca_detr/#compute_mask","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/model/smca_detr/#compute_output_shape","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/model/smca_detr/#compute_output_signature","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/model/smca_detr/#count_params","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/model/smca_detr/#get_input_at","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/model/smca_detr/#get_input_mask_at","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/model/smca_detr/#get_input_shape_at","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/model/smca_detr/#get_losses_for","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/model/smca_detr/#get_output_at","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/model/smca_detr/#get_output_mask_at","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/model/smca_detr/#get_output_shape_at","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/model/smca_detr/#get_updates_for","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/model/smca_detr/#set_weights","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/model/smca_detr/#smcar50","text":"class SMCAR50 ( num_classes , num_queries = 100 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"SMCAR50"},{"location":"reference/kerod/model/smca_detr/#arguments_1","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries.","title":"Arguments"},{"location":"reference/kerod/model/smca_detr/#call-arguments_1","text":"Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/smca_detr/#call-returns_1","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/smca_detr/#ancestors-in-mro_1","text":"kerod.model.smca_detr.SMCA tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/smca_detr/#methods_1","text":"","title":"Methods"},{"location":"reference/kerod/model/smca_detr/#add_loss_1","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/model/smca_detr/#add_metric_1","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/model/smca_detr/#add_update_1","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/model/smca_detr/#add_variable_1","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/model/smca_detr/#add_weight_1","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/model/smca_detr/#apply_1","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/model/smca_detr/#call_1","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/smca_detr/#compute_loss_1","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor A scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: A scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/smca_detr/#compute_mask_1","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/model/smca_detr/#compute_output_shape_1","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/model/smca_detr/#compute_output_signature_1","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/model/smca_detr/#count_params_1","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/model/smca_detr/#get_input_at_1","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/model/smca_detr/#get_input_mask_at_1","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/model/smca_detr/#get_input_shape_at_1","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/model/smca_detr/#get_losses_for_1","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/model/smca_detr/#get_output_at_1","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/model/smca_detr/#get_output_mask_at_1","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/model/smca_detr/#get_output_shape_at_1","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/model/smca_detr/#get_updates_for_1","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/model/smca_detr/#set_weights_1","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/model/smca_detr/#smcar50pytorch","text":"class SMCAR50Pytorch ( num_classes , num_queries = 100 , ** kwargs ) Fast Convergence of DETR with Spatially Modulated Co-Attention . In what is it different from DETR ? Just imagine that your object queries are learned anchors. Those learned \"anchors\" will modulate the attention map during the coattention stage of the decoder. They will help to target faster some sweet spots which leads to a speed up by 10 of the training. It maintains the same performance than DETR. You can use it as follow: model = SMCAR50 ( 80 ) base_lr = 0.1 optimizer = tf . keras . optimizers . SGD ( learning_rate = base_lr ) model . compile ( optimizer = optimizer , loss = None ) model . fit ( ds_train , validation_data = ds_test , epochs = 11 ,)","title":"SMCAR50Pytorch"},{"location":"reference/kerod/model/smca_detr/#arguments_2","text":"Name Description num_classes The number of classes of your dataset ( do not include the background class it is handle for you) backbone A vision model like ResNet50. num_queries number of object queries, ie detection slot. This is the maximal number of objects SCMA can detect in a single image. For COCO, we recommend 300 queries.","title":"Arguments"},{"location":"reference/kerod/model/smca_detr/#call-arguments_2","text":"Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode","title":"Call arguments"},{"location":"reference/kerod/model/smca_detr/#call-returns_2","text":"Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise.","title":"Call returns"},{"location":"reference/kerod/model/smca_detr/#ancestors-in-mro_2","text":"kerod.model.smca_detr.SMCA tensorflow.python.keras.engine.training.Model tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector tensorflow.python.keras.utils.version_utils.ModelVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/smca_detr/#methods_2","text":"","title":"Methods"},{"location":"reference/kerod/model/smca_detr/#add_loss_2","text":"def add_loss ( self , losses , ** kwargs ) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.losses may be dependent on a and some on b . This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's call function, in which case losses should be a Tensor or list of Tensors. Example: class MyLayer ( tf . keras . layers . Layer ): def call ( self , inputs ): self . add_loss ( tf . abs ( tf . reduce_mean ( inputs ))) return inputs This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's Input s. These losses become part of the model's topology and are tracked in get_config . Example: inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Activity regularization. model . add_loss ( tf . abs ( tf . reduce_mean ( x ))) If this is not the case for your loss (if, for example, your loss references a Variable of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: inputs = tf . keras . Input ( shape = ( 10 ,)) d = tf . keras . layers . Dense ( 10 ) x = d ( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) # Weight regularization. model . add_loss ( lambda : tf . reduce_mean ( d . kernel )) Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. View Source def add_loss ( self , losses , ** kwargs ): \"\"\"Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. \"\"\" kwargs . pop ( 'inputs' , None ) if kwargs: raise TypeError ( 'Unknown keyword arguments: %s' % ( kwargs . keys (),)) def _tag_callable ( loss ): \"\"\"Tags callable loss tensor as `_unconditional_loss`.\"\"\" if callable ( loss ): # We run the loss without autocasting, as regularizers are often # numerically unstable in float16. with autocast_variable . enable_auto_cast_variables ( None ): loss = loss () if loss is None: return None # Will be filtered out when computing the .losses property if not tensor_util . is_tensor ( loss ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) loss . _unconditional_loss = True # pylint: disable=protected-access return loss losses = nest . flatten ( losses ) callable_losses = [] eager_losses = [] symbolic_losses = [] for loss in losses: if callable ( loss ): callable_losses . append ( functools . partial ( _tag_callable , loss )) continue if loss is None: continue if not tensor_util . is_tensor ( loss ) and not isinstance ( loss , keras_tensor . KerasTensor ): loss = ops . convert_to_tensor_v2_with_dispatch ( loss , dtype = backend . floatx ()) # TF Functions should take the eager path. if (( tf_utils . is_symbolic_tensor ( loss ) or isinstance ( loss , keras_tensor . KerasTensor )) and not base_layer_utils . is_in_tf_function ()): symbolic_losses . append ( loss ) elif tensor_util . is_tensor ( loss ): eager_losses . append ( loss ) self . _callable_losses . extend ( callable_losses ) in_call_context = base_layer_utils . call_context (). in_call if eager_losses and not in_call_context: raise ValueError ( 'Expected a symbolic Tensors or a callable for the loss value. ' 'Please wrap your loss computation in a zero argument `lambda`.' ) self . _eager_losses . extend ( eager_losses ) if in_call_context and not keras_tensor . keras_tensors_enabled (): for symbolic_loss in symbolic_losses: self . _losses . append ( symbolic_loss ) else: for symbolic_loss in symbolic_losses: if getattr ( self , '_is_graph_network' , False ): self . _graph_network_add_loss ( symbolic_loss ) else: # Possible a loss was added in a Layer's `build`. self . _losses . append ( symbolic_loss )","title":"add_loss"},{"location":"reference/kerod/model/smca_detr/#add_metric_2","text":"def add_metric ( self , value , name = None , ** kwargs ) Adds metric tensor to the layer. This method can be used inside the call() method of a subclassed layer or model. class MyMetricLayer ( tf . keras . layers . Layer ): def __init__ ( self ): super ( MyMetricLayer , self ) . __init__ ( name = 'my_metric_layer' ) self . mean = tf . keras . metrics . Mean ( name = 'metric_1' ) def call ( self , inputs ): self . add_metric ( self . mean ( x )) self . add_metric ( tf . reduce_sum ( x ), name = 'metric_2' ) return inputs This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's Input s. These metrics become part of the model's topology and are tracked when you save the model via save() . inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( math_ops . reduce_sum ( x ), name = 'metric_1' ) Note: Calling add_metric() with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. inputs = tf . keras . Input ( shape = ( 10 ,)) x = tf . keras . layers . Dense ( 10 )( inputs ) outputs = tf . keras . layers . Dense ( 1 )( x ) model = tf . keras . Model ( inputs , outputs ) model . add_metric ( tf . keras . metrics . Mean ()( x ), name = 'metric_1' ) Parameters: Name Description value Metric tensor. name String metric name. **kwargs Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . View Source def add_metric ( self , value , name = None , ** kwargs ) : \" \"\" Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: `aggregation` - When the `value` tensor provided is not the result of calling a `keras.Metric` instance, it will be aggregated by default using a `keras.Metric.Mean`. \"\" \" kwargs_keys = list ( kwargs . keys ()) if ( len ( kwargs_keys ) > 1 or ( len ( kwargs_keys ) == 1 and kwargs_keys [ 0 ] != 'aggregation' )) : raise TypeError ( 'Unknown keyword arguments: ' , str ( kwargs . keys ())) from_metric_obj = hasattr ( value , '_metric_obj' ) if keras_tensor . keras_tensors_enabled () : is_symbolic = isinstance ( value , keras_tensor . KerasTensor ) else : is_symbolic = tf_utils . is_symbolic_tensor ( value ) in_call_context = base_layer_utils . call_context (). in_call if name is None and not from_metric_obj : # Eg. `self.add_metric(math_ops.reduce_sum(x))` # In eager mode, we use metric name to lookup a metric. Without a name, # a new Mean metric wrapper will be created on every model/layer call. # So, we raise an error when no name is provided. # We will do the same for symbolic mode for consistency although a name # will be generated if no name is provided. # We will not raise this error in the foll use case for the sake of # consistency as name in provided in the metric constructor. # mean = metrics.Mean(name='my_metric') # model.add_metric(mean(outputs)) raise ValueError ( 'Please provide a name for your metric like ' '`self.add_metric(tf.reduce_sum(inputs), ' 'name= \\' mean_activation \\' )`' ) elif from_metric_obj : name = value . _metric_obj . name if not in_call_context and not is_symbolic : raise ValueError ( 'Expected a symbolic Tensor for the metric value, ' 'received: ' + str ( value )) # If a metric was added in a Layer's `call` or `build`. if in_call_context or not getattr ( self , '_is_graph_network' , False ) : # TF Function path should take the eager path. # If the given metric is available in `metrics` list we just update state # on it, otherwise we create a new metric instance and # add it to the `metrics` list. metric_obj = getattr ( value , '_metric_obj' , None ) # Tensors that come from a Metric object already updated the Metric state. should_update_state = not metric_obj name = metric_obj . name if metric_obj else name with self . _metrics_lock : match = self . _get_existing_metric ( name ) if match : metric_obj = match elif metric_obj : self . _metrics . append ( metric_obj ) else : # Build the metric object with the value's dtype if it defines one metric_obj = metrics_mod . Mean ( name = name , dtype = getattr ( value , 'dtype' , None )) self . _metrics . append ( metric_obj ) if should_update_state : metric_obj ( value ) else : if from_metric_obj : raise ValueError ( 'Using the result of calling a `Metric` object ' 'when calling `add_metric` on a Functional ' 'Model is not supported. Please pass the ' 'Tensor to monitor directly.' ) # Insert layers into the Keras Graph Network. aggregation = None if from_metric_obj else 'mean' self . _graph_network_add_metric ( value , aggregation , name )","title":"add_metric"},{"location":"reference/kerod/model/smca_detr/#add_update_2","text":"def add_update ( self , updates , inputs = None ) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs a and b , some entries in layer.updates may be dependent on a and some on b . This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Parameters: Name Description updates Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting trainable=False on this Layer, when executing in Eager mode. inputs Deprecated, will be automatically inferred. View Source @doc_controls.do_not_doc_inheritable def add_update ( self , updates , inputs = None ) : \" \"\" Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. \"\" \" if inputs is not None : tf_logging . warning ( '`add_update` `inputs` kwarg has been deprecated. You no longer need ' 'to pass a value to `inputs` as it is being automatically inferred.' ) call_context = base_layer_utils . call_context () # No need to run updates during Functional API construction. if call_context . in_keras_graph : return # Callable updates are disabled by setting `trainable=False`. if not call_context . frozen : for update in nest . flatten ( updates ) : if callable ( update ) : update () # pylint: disable=not-callable","title":"add_update"},{"location":"reference/kerod/model/smca_detr/#add_variable_2","text":"def add_variable ( self , * args , ** kwargs ) Deprecated, do NOT use! Alias for add_weight . View Source @doc_controls.do_not_doc_inheritable def add_variable ( self , * args , ** kwargs ) : \" \"\" Deprecated, do NOT use! Alias for `add_weight`. \"\" \" warnings . warn ( '`layer.add_variable` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.add_weight` method instead.' ) return self . add_weight ( * args , ** kwargs )","title":"add_variable"},{"location":"reference/kerod/model/smca_detr/#add_weight_2","text":"def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization =< VariableSynchronization . AUTO : 0 > , aggregation =< VariableAggregation . NONE : 0 > , ** kwargs ) Adds a new variable to the layer. Parameters: Name Description name Variable name. shape Variable shape. Defaults to scalar if unspecified. dtype The type of the variable. Defaults to self.dtype . initializer Initializer instance (callable). regularizer Regularizer instance (callable). trainable Boolean, whether the variable should be part of the layer's \"trainable_variables\" (e.g. variables, biases) or \"non_trainable_variables\" (e.g. BatchNorm mean and variance). Note that trainable cannot be True if synchronization is set to ON_READ . constraint Constraint instance (callable). use_resource Whether to use ResourceVariable . synchronization Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class tf.VariableSynchronization . By default the synchronization is set to AUTO and the current DistributionStrategy chooses when to synchronize. If synchronization is set to ON_READ , trainable must not be set to True . aggregation Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class tf.VariableAggregation . **kwargs Additional keyword arguments. Accepted values are getter , collections , experimental_autocast and caching_device . Returns: Type Description None The variable created. Raises: Type Description ValueError When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as ON_READ . View Source @doc_controls.for_subclass_implementers def add_weight ( self , name = None , shape = None , dtype = None , initializer = None , regularizer = None , trainable = None , constraint = None , use_resource = None , synchronization = tf_variables . VariableSynchronization . AUTO , aggregation = tf_variables . VariableAggregation . NONE , ** kwargs ) : \" \"\" Adds a new variable to the layer. Arguments: name: Variable name. shape: Variable shape. Defaults to scalar if unspecified. dtype: The type of the variable. Defaults to `self.dtype`. initializer: Initializer instance (callable). regularizer: Regularizer instance (callable). trainable: Boolean, whether the variable should be part of the layer's \" trainable_variables \" (e.g. variables, biases) or \" non_trainable_variables \" (e.g. BatchNorm mean and variance). Note that `trainable` cannot be `True` if `synchronization` is set to `ON_READ`. constraint: Constraint instance (callable). use_resource: Whether to use `ResourceVariable`. synchronization: Indicates when a distributed a variable will be aggregated. Accepted values are constants defined in the class `tf.VariableSynchronization`. By default the synchronization is set to `AUTO` and the current `DistributionStrategy` chooses when to synchronize. If `synchronization` is set to `ON_READ`, `trainable` must not be set to `True`. aggregation: Indicates how a distributed variable will be aggregated. Accepted values are constants defined in the class `tf.VariableAggregation`. **kwargs: Additional keyword arguments. Accepted values are `getter`, `collections`, `experimental_autocast` and `caching_device`. Returns: The variable created. Raises: ValueError: When giving unsupported dtype and no initializer or when trainable has been set to True with synchronization set as `ON_READ`. \"\" \" if shape is None : shape = () kwargs . pop ( 'partitioner' , None ) # Ignored. # Validate optional keyword arguments. for kwarg in kwargs : if kwarg not in [ 'collections' , 'experimental_autocast' , 'caching_device' , 'getter' ] : raise TypeError ( 'Unknown keyword argument:' , kwarg ) collections_arg = kwargs . pop ( 'collections' , None ) # 'experimental_autocast' can be set to False by the caller to indicate an # AutoCastVariable should never be created. autocast = kwargs . pop ( 'experimental_autocast' , True ) # See the docstring for tf.Variable about the details for caching_device. caching_device = kwargs . pop ( 'caching_device' , None ) if dtype is None : dtype = self . dtype or backend . floatx () dtype = dtypes . as_dtype ( dtype ) if self . _dtype_policy . variable_dtype is None : # The policy is \"_infer\", so we infer the policy from the variable dtype. self . _set_dtype_policy ( policy . Policy ( dtype . base_dtype . name )) initializer = initializers . get ( initializer ) regularizer = regularizers . get ( regularizer ) constraint = constraints . get ( constraint ) if synchronization == tf_variables . VariableSynchronization . ON_READ : if trainable : raise ValueError ( 'Synchronization value can be set to ' 'VariableSynchronization.ON_READ only for non-trainable variables. ' 'You have specified trainable=True and ' 'synchronization=VariableSynchronization.ON_READ.' ) else : # Set trainable to be false when variable is to be synced on read. trainable = False elif trainable is None : trainable = True # Initialize variable when no initializer provided if initializer is None : # If dtype is DT_FLOAT, provide a uniform unit scaling initializer if dtype . is_floating : initializer = initializers . get ( 'glorot_uniform' ) # If dtype is DT_INT/DT_UINT, provide a default value `zero` # If dtype is DT_BOOL, provide a default value `FALSE` elif dtype . is_integer or dtype . is_unsigned or dtype . is_bool : initializer = initializers . get ( 'zeros' ) # NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here? else : raise ValueError ( 'An initializer for variable %s of type %s is required' ' for layer %s' % ( name , dtype . base_dtype , self . name )) getter = kwargs . pop ( 'getter' , base_layer_utils . make_variable ) if ( autocast and self . _dtype_policy . compute_dtype != self . _dtype_policy . variable_dtype and dtype . is_floating ) : old_getter = getter # Wrap variable constructor to return an AutoCastVariable. def getter ( * args , ** kwargs ) : # pylint: disable=function-redefined variable = old_getter ( * args , ** kwargs ) return autocast_variable . create_autocast_variable ( variable ) # Also the caching_device does not work with the mixed precision API, # disable it if it is specified. # TODO(b/142020079): Reenable it once the bug is fixed. if caching_device is not None : tf_logging . warn ( '`caching_device` does not work with mixed precision ' 'API. Ignoring user specified `caching_device`.' ) caching_device = None variable = self . _add_variable_with_custom_getter ( name = name , shape = shape , # TODO(allenl): a `make_variable` equivalent should be added as a # `Trackable` method. getter = getter , # Manage errors in Layer rather than Trackable. overwrite = True , initializer = initializer , dtype = dtype , constraint = constraint , trainable = trainable , use_resource = use_resource , collections = collections_arg , synchronization = synchronization , aggregation = aggregation , caching_device = caching_device ) if regularizer is not None : # TODO(fchollet): in the future, this should be handled at the # level of variable creation, and weight regularization losses # should be variable attributes. name_in_scope = variable . name [ : variable . name . find ( ':' ) ] self . _handle_weight_regularization ( name_in_scope , variable , regularizer ) if base_layer_utils . is_split_variable ( variable ) : for v in variable : backend . track_variable ( v ) if trainable : self . _trainable_weights . append ( v ) else : self . _non_trainable_weights . append ( v ) else : backend . track_variable ( variable ) if trainable : self . _trainable_weights . append ( variable ) else : self . _non_trainable_weights . append ( variable ) return variable","title":"add_weight"},{"location":"reference/kerod/model/smca_detr/#apply_2","text":"def apply ( self , inputs , * args , ** kwargs ) Deprecated, do NOT use! This is an alias of self.__call__ . Parameters: Name Description inputs Input tensor(s). *args additional positional arguments to be passed to self.call . **kwargs additional keyword arguments to be passed to self.call . Returns: Type Description None Output tensor(s). View Source @doc_controls . do_not_doc_inheritable def apply ( self , inputs , * args , ** kwargs ) : \"\"\"Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to `self.call`. Returns: Output tensor(s). \"\"\" warnings . warn ( '`layer.apply` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.__call__` method instead.' ) return self . __call__ ( inputs , * args , ** kwargs )","title":"apply"},{"location":"reference/kerod/model/smca_detr/#call_2","text":"def call ( self , inputs , training = None ) Perform an inference in training. Parameters: Name Description inputs Dict with the following keys: - images : A 4-D tensor of float32 and shape [batch_size, None, None, 3] - image_informations : A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - images_padding_mask : A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training Is automatically set to True in train mode Returns: Type Description Tuple - logits : A Tensor of shape [batch_size, h, num_classes + 1] class logits - boxes : A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. View Source def call ( self , inputs , training = None ) : \" \"\" Perform an inference in training. Arguments: inputs: Dict with the following keys: - `images`: A 4-D tensor of float32 and shape [batch_size, None, None, 3] - `image_informations`: A 1D tensor of float32 and shape [(height, width),]. It contains the shape of the image without any padding. - `images_padding_mask`: A 3D tensor of int8 and shape [batch_size, None, None] composed of 0 and 1 which allows to know where a padding has been applied. training: Is automatically set to `True` in train mode Returns: Tuple: - `logits`: A Tensor of shape [batch_size, h, num_classes + 1] class logits - `boxes`: A Tensor of shape [batch_size, h, 4] where h is num_queries * transformer_decoder.transformer_num_layers if training is true and num_queries otherwise. \"\" \" images = inputs [ DatasetField . IMAGES ] images_padding_masks = inputs [ DatasetField . IMAGES_PMASK ] batch_size = tf . shape ( images ) [ 0 ] # The preprocessing dedicated to the backbone is done inside the model. x = self . backbone ( images ) [ - 1 ] features_mask = tf . image . resize ( tf . cast ( images_padding_masks [ ..., None ] , tf . float32 ), tf . shape ( x ) [ 1 : 3 ] , method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) features_mask = tf . cast ( features_mask , tf . bool ) # Positional_encoding for the backbone pos_embed = self . pos_embed ( features_mask ) # [batch_size, num_queries, self.hidden_dim] all_the_queries = tf . tile ( self . all_the_queries [ None ] , ( batch_size , 1 )) # [batch_size, num_queries, self.hidden_dim] query_embed = self . query_embed ( all_the_queries ) h_backbone_out , w_backbone_out = tf . shape ( x ) [ 1 ] , tf . shape ( x ) [ 2 ] x = self . input_proj ( x ) # Flatten the position embedding and the spatial tensor # to allow the preprocessing by the Transformer # [batch_size, h * w, self.hidden_dim] x = tf . reshape ( x , ( batch_size , - 1 , self . hidden_dim )) pos_embed = tf . reshape ( pos_embed , ( batch_size , - 1 , self . hidden_dim )) # Flatten the padding masks features_mask = tf . reshape ( features_mask , ( batch_size , - 1 )) ref_points , ref_points_presigmoid = self . ref_points ( query_embed ) # dyn_weight_map_per_head = G in the paper dyn_weight_map_per_head = self . dyn_weight_map ( h_backbone_out , w_backbone_out , ref_points ) dyn_weight_map_per_head = tf . math . log ( dyn_weight_map_per_head + 10e-4 ) # log G decoder_out , _ = self . transformer ( x , pos_embed , query_embed , key_padding_mask = features_mask , coattn_mask = dyn_weight_map_per_head , training = training ) logits = self . class_embed ( decoder_out ) boxes = self . bbox_embed ( decoder_out ) if training : # In training all the outputs of the decoders are stacked together. # We tile the reference_points to match those outputs ref_points_presigmoid = tf . tile ( ref_points_presigmoid , ( 1 , self . transformer_num_layers , 1 )) # Add initial center to constrain the bounding boxes predictions offset = tf . concat ( [ ref_points_presigmoid , tf . zeros (( batch_size , tf . shape ( ref_points_presigmoid ) [ 1 ] , 2 )) ] , axis =- 1 ) boxes = tf . nn . sigmoid ( boxes + offset ) return { BoxField . SCORES : logits , BoxField . BOXES : boxes , }","title":"call"},{"location":"reference/kerod/model/smca_detr/#compute_loss_2","text":"def compute_loss ( self , ground_truths : Dict [ str , tensorflow . python . framework . ops . Tensor ], y_pred : Dict [ str , tensorflow . python . framework . ops . Tensor ], input_shape : tensorflow . python . framework . ops . Tensor ) -> tensorflow . python . framework . ops . Tensor Apply the GIoU, L1 and SCC to each layers of the transformer decoder Parameters: Name Description ground_truths see output kerod.dataset.preprocessing for the doc y_pred A dict - scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: Type Description tf.Tensor A scalar for the loss View Source def compute_loss ( self , ground_truths : Dict [ str, tf.Tensor ] , y_pred : Dict [ str, tf.Tensor ] , input_shape : tf . Tensor , ) -> tf . Tensor : \"\"\"Apply the GIoU, L1 and SCC to each layers of the transformer decoder Args: ground_truths: see output kerod.dataset.preprocessing for the doc y_pred: A dict - *scores: A Tensor of shape [batch_size, num_queries, num_classes + 1] class logits - *bbox*: A Tensor of shape [batch_size, num_queries, 4] input_shape: [height, width] of the input tensor. It is the shape of the images will all the padding included. It is used to normalize the ground_truths boxes. Returns: tf.Tensor: A scalar for the loss \"\"\" normalized_boxes = ground_truths [ BoxField.BOXES ] / tf . tile ( input_shape [ None ] , [ 1, 2 ] ) centered_normalized_boxes = convert_to_center_coordinates ( normalized_boxes ) ground_truths = { # We add one because the background is not counted in ground_truths [ BoxField.LABELS ] BoxField . LABELS : ground_truths [ BoxField.LABELS ] + 1 , BoxField . BOXES : centered_normalized_boxes , BoxField . WEIGHTS : ground_truths [ BoxField.WEIGHTS ] , BoxField . NUM_BOXES : ground_truths [ BoxField.NUM_BOXES ] } boxes_per_lvl = tf . split ( y_pred [ BoxField.BOXES ] , self . transformer_num_layers , axis = 1 ) logits_per_lvl = tf . split ( y_pred [ BoxField.SCORES ] , self . transformer_num_layers , axis = 1 ) y_pred_per_lvl = [ { BoxField.BOXES: boxes, BoxField.SCORES: logits } for boxes, logits in zip(boxes_per_lvl, logits_per_lvl) ] num_boxes = tf . cast ( tf . reduce_sum ( ground_truths [ BoxField.NUM_BOXES ] ), tf . float32 ) loss = 0 # Compute the Giou , L1 and SCC at each layers of the transformer decoder for i , y_pred in enumerate ( y_pred_per_lvl ) : # Logs the metrics for the last layer of the decoder compute_metrics = i == self . transformer_num_layers - 1 loss += self . _compute_loss ( y_pred , ground_truths , num_boxes , compute_metrics = compute_metrics ) return loss","title":"compute_loss"},{"location":"reference/kerod/model/smca_detr/#compute_mask_2","text":"def compute_mask ( self , inputs , mask = None ) Computes an output mask tensor. Parameters: Name Description inputs Tensor or list of tensors. mask Tensor or list of tensors. Returns: Type Description None None or a tensor (or list of tensors, one per output tensor of the layer). View Source @generic_utils . default def compute_mask ( self , inputs , mask = None ) : # pylint : disable = unused - argument \"\"\"Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). \"\"\" if not self . _supports_masking : if any ( m is not None for m in nest . flatten ( mask )) : raise TypeError ( 'Layer ' + self . name + ' does not support masking, ' 'but was passed an input_mask: ' + str ( mask )) # masking not explicitly supported : return None as mask . return None # if masking is explicitly supported , by default # carry over the input mask return mask","title":"compute_mask"},{"location":"reference/kerod/model/smca_detr/#compute_output_shape_2","text":"def compute_output_shape ( self , input_shape ) Computes the output shape of the layer. If the layer has not been built, this method will call build on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Parameters: Name Description input_shape Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: Type Description None An input shape tuple. View Source def compute_output_shape ( self , input_shape ): \"\"\"Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. \"\"\" if context . executing_eagerly (): # In this case we build the model first in order to do shape inference. # This is acceptable because the framework only calls # `compute_output_shape` on shape values that the layer would later be # built for. It would however cause issues in case a user attempts to # use `compute_output_shape` manually with shapes that are incompatible # with the shape the Layer will be called on (these users will have to # implement `compute_output_shape` themselves). self . _maybe_build ( input_shape ) with func_graph . FuncGraph ( str ( self . name ) + '_scratch_graph' ). as_default (): input_shape = tf_utils . convert_shapes ( input_shape , to_tuples = False ) def _make_placeholder_like ( shape ): ph = backend . placeholder ( shape = shape , dtype = self . dtype ) ph . _keras_mask = None return ph inputs = nest . map_structure ( _make_placeholder_like , input_shape ) try: outputs = self ( inputs , training = False ) except TypeError as e: six . raise_from ( NotImplementedError ( 'We could not automatically infer the static shape of the ' 'layer\\'s output. Please implement the ' '`compute_output_shape` method on your layer (%s).' % self . __class__ . __name__ ), e ) return nest . map_structure ( lambda t: t . shape , outputs ) raise NotImplementedError ( 'Please run in eager mode or implement the `compute_output_shape` ' 'method on your layer (%s).' % self . __class__ . __name__ )","title":"compute_output_shape"},{"location":"reference/kerod/model/smca_detr/#compute_output_signature_2","text":"def compute_output_signature ( self , input_signature ) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use compute_output_shape , and will assume that the output dtype matches the input dtype. Parameters: Name Description input_signature Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Type Description None Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: Type Description TypeError If input_signature contains a non-TensorSpec object. View Source @doc_controls.for_subclass_implementers def compute_output_signature ( self , input_signature ) : \" \"\" Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. \"\" \" def check_type_return_shape ( s ) : if not isinstance ( s , tensor_spec . TensorSpec ) : raise TypeError ( 'Only TensorSpec signature types are supported, ' 'but saw signature signature entry: {}.' . format ( s )) return s . shape input_shape = nest . map_structure ( check_type_return_shape , input_signature ) output_shape = self . compute_output_shape ( input_shape ) dtype = self . _compute_dtype if dtype is None : input_dtypes = [ s . dtype for s in nest . flatten ( input_signature ) ] # Default behavior when self.dtype is None, is to use the first input's # dtype. dtype = input_dtypes [ 0 ] return nest . map_structure ( lambda s : tensor_spec . TensorSpec ( dtype = dtype , shape = s ), output_shape )","title":"compute_output_signature"},{"location":"reference/kerod/model/smca_detr/#count_params_2","text":"def count_params ( self ) Count the total number of scalars composing the weights. Returns: Type Description None An integer count. Raises: Type Description ValueError if the layer isn't yet built (in which case its weights aren't yet defined). View Source def count_params ( self ) : \" \"\" Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). \"\" \" if not self . built : if getattr ( self , '_is_graph_network' , False ) : with tf_utils . maybe_init_scope ( self ) : self . _maybe_build ( self . inputs ) else : raise ValueError ( 'You tried to call `count_params` on ' + self . name + ', but the layer isn \\' t built. ' 'You can build it manually via: `' + self . name + '.build(batch_input_shape)`.' ) return layer_utils . count_params ( self . weights )","title":"count_params"},{"location":"reference/kerod/model/smca_detr/#get_input_at_2","text":"def get_input_at ( self , node_index ) Retrieves the input tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_at ( self , node_index ) : \"\"\"Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_tensors' , 'input' )","title":"get_input_at"},{"location":"reference/kerod/model/smca_detr/#get_input_mask_at_2","text":"def get_input_mask_at ( self , node_index ) Retrieves the input mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple inputs). View Source @doc_controls . do_not_doc_inheritable def get_input_mask_at ( self , node_index ) : \"\"\"Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). \"\"\" inputs = self . get_input_at ( node_index ) if isinstance ( inputs , list ) : return [ getattr(x, '_keras_mask', None) for x in inputs ] else : return getattr ( inputs , '_keras_mask' , None )","title":"get_input_mask_at"},{"location":"reference/kerod/model/smca_detr/#get_input_shape_at_2","text":"def get_input_shape_at ( self , node_index ) Retrieves the input shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_input_shape_at ( self , node_index ) : \"\"\"Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'input_shapes' , 'input shape' )","title":"get_input_shape_at"},{"location":"reference/kerod/model/smca_detr/#get_losses_for_2","text":"def get_losses_for ( self , inputs ) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of loss tensors of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_losses_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_losses_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.losses` instead.' ) return self . losses","title":"get_losses_for"},{"location":"reference/kerod/model/smca_detr/#get_output_at_2","text":"def get_output_at ( self , node_index ) Retrieves the output tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A tensor (or list of tensors if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_at ( self , node_index ) : \"\"\"Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_tensors' , 'output' )","title":"get_output_at"},{"location":"reference/kerod/model/smca_detr/#get_output_mask_at_2","text":"def get_output_mask_at ( self , node_index ) Retrieves the output mask tensor(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A mask tensor (or list of tensors if the layer has multiple outputs). View Source @doc_controls . do_not_doc_inheritable def get_output_mask_at ( self , node_index ) : \"\"\"Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). \"\"\" output = self . get_output_at ( node_index ) if isinstance ( output , list ) : return [ getattr(x, '_keras_mask', None) for x in output ] else : return getattr ( output , '_keras_mask' , None )","title":"get_output_mask_at"},{"location":"reference/kerod/model/smca_detr/#get_output_shape_at_2","text":"def get_output_shape_at ( self , node_index ) Retrieves the output shape(s) of a layer at a given node. Parameters: Name Description node_index Integer, index of the node from which to retrieve the attribute. E.g. node_index=0 will correspond to the first time the layer was called. Returns: Type Description None A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: Type Description RuntimeError If called in Eager mode. View Source @doc_controls . do_not_doc_inheritable def get_output_shape_at ( self , node_index ) : \"\"\"Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. \"\"\" return self . _get_node_attribute_at_index ( node_index , 'output_shapes' , 'output shape' )","title":"get_output_shape_at"},{"location":"reference/kerod/model/smca_detr/#get_updates_for_2","text":"def get_updates_for ( self , inputs ) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Parameters: Name Description inputs Input tensor or list/tuple of input tensors. Returns: Type Description None List of update ops of the layer that depend on inputs . View Source @doc_controls.do_not_generate_docs def get_updates_for ( self , inputs ) : \" \"\" Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. \"\" \" warnings . warn ( '`layer.get_updates_for` is deprecated and ' 'will be removed in a future version. ' 'Please use `layer.updates` method instead.' ) return self . updates","title":"get_updates_for"},{"location":"reference/kerod/model/smca_detr/#set_weights_2","text":"def set_weights ( self , weights ) Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] b.set_weights(a.get_weights()) b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Parameters: Name Description weights a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of get_weights ). Raises: Type Description ValueError If the provided weights list does not match the layer's specifications. View Source def set_weights ( self , weights ): \"\"\"Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. \"\"\" params = self . weights expected_num_weights = 0 for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): expected_num_weights += param . num_tensors else : expected_num_weights += 1 if expected_num_weights != len ( weights ): raise ValueError ( 'You called `set_weights(weights)` on layer \" %s \" ' 'with a weight list of length %s , but the layer was ' 'expecting %s weights. Provided weights: %s ...' % ( self . name , len ( weights ), expected_num_weights , str ( weights )[: 50 ])) weight_index = 0 weight_value_tuples = [] for param in params : if isinstance ( param , base_layer_utils . TrackableWeightHandler ): num_tensors = param . num_tensors tensors = weights [ weight_index : weight_index + num_tensors ] param . set_weights ( tensors ) weight_index += num_tensors else : weight = weights [ weight_index ] ref_shape = param . shape if not ref_shape . is_compatible_with ( weight . shape ): raise ValueError ( 'Layer weight shape %s not compatible with provided weight ' 'shape %s ' % ( ref_shape , weight . shape )) weight_value_tuples . append (( param , weight )) weight_index += 1 backend . batch_set_value ( weight_value_tuples )","title":"set_weights"},{"location":"reference/kerod/model/backbone/","text":"Module kerod.model.backbone None None Sub-modules kerod.model.backbone.fpn kerod.model.backbone.resnet","title":"Index"},{"location":"reference/kerod/model/backbone/#module-kerodmodelbackbone","text":"None None","title":"Module kerod.model.backbone"},{"location":"reference/kerod/model/backbone/#sub-modules","text":"kerod.model.backbone.fpn kerod.model.backbone.resnet","title":"Sub-modules"},{"location":"reference/kerod/model/backbone/fpn/","text":"Module kerod.model.backbone.fpn None None View Source import tensorflow as tf from kerod . utils . documentation import remove_unwanted_doc from tensorflow . keras import layers from tensorflow . keras . initializers import VarianceScaling __ pdoc__ = {} class FPN ( layers . Layer ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack)\"\"\" def __ init__ ( self , dim = 256 , kernel_regularizer = None , **kwargs ) : super (). __ init__ ( **kwargs ) self . _ dim = dim self . _ kernel_regularizer = kernel_regularizer def build ( self , input_shape ) : num_level_pyramid = len ( input_shape [ 0 ]) self . lateral_connection_2345 = [ layers . Conv2D ( self . _ dim , ( 1 , 1 ), padding='same' , kernel_initializer = VarianceScaling ( scale = 1. ), kernel_regularizer = self . _ kernel_regularizer ) for _ in range ( num_level_pyramid ) ] self . anti_aliasing_conv = [ layers . Conv2D ( self . _ dim , ( 3 , 3 ), padding='same' , kernel_initializer = VarianceScaling ( scale = 1. ), kernel_regularizer = self . _ kernel_regularizer ) for _ in range ( num_level_pyramid ) ] super (). build ( input_shape ) def call ( self , inputs ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack) Arguments: inputs: A list of tensors of shape [N, height, widht, channels] Returns: A list of tensors of shape [N + 1, height, width, channels] \"\"\" lateral_connection_2345 = [ conv ( tensor ) for tensor , conv in zip ( inputs , self . lateral_connection_2345 ) ] lat_sum_5432 = [] for idx , block in enumerate ( lateral_connection_2345 [ ::- 1 ]) : if idx > 0 : up_shape = tf . shape ( block ) block = block + tf . image . resize ( lat_sum_5432 [ - 1 ], [ up_shape [ 1 ], up_shape [ 2 ]], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) lat_sum_5432 . append ( block ) # 3 \u00d73convolution on each merged map to generate the final feature map , # which is to reduce the aliasing effect of upsampling . lateral_connection_2345 = [ conv ( tensor ) for conv , tensor in zip ( self . anti_aliasing_conv , lat_sum_5432 [ ::- 1 ]) ] p6 = layers . MaxPool2D ()( lateral_connection_2345 [ - 1 ]) return lateral_connection_2345 + [ p6 ] def get_config ( self ) : base_config = super (). get_config () base_config [ 'dim' ] = self . _ dim return base_config remove_unwanted_doc ( FPN , __ pdoc__ ) Classes FPN class FPN ( dim = 256 , kernel_regularizer = None , ** kwargs ) Ancestors (in MRO) tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector Methods call def call ( self , inputs ) Over your backbone feature build a FPN (inspired from tensorpack) Parameters: Name Description inputs A list of tensors of shape [N, height, widht, channels] Returns: Type Description None A list of tensors of shape [N + 1, height, width, channels] View Source def call ( self , inputs ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack) Arguments: inputs: A list of tensors of shape [N, height, widht, channels] Returns: A list of tensors of shape [N + 1, height, width, channels] \"\"\" lateral_connection_2345 = [ conv ( tensor ) for tensor , conv in zip ( inputs , self . lateral_connection_2345 ) ] lat_sum_5432 = [] for idx , block in enumerate ( lateral_connection_2345 [ ::- 1 ]) : if idx > 0 : up_shape = tf . shape ( block ) block = block + tf . image . resize ( lat_sum_5432 [ - 1 ], [ up_shape [ 1 ], up_shape [ 2 ]], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) lat_sum_5432 . append ( block ) # 3 \u00d73convolution on each merged map to generate the final feature map , # which is to reduce the aliasing effect of upsampling . lateral_connection_2345 = [ conv ( tensor ) for conv , tensor in zip ( self . anti_aliasing_conv , lat_sum_5432 [ ::- 1 ]) ] p6 = layers . MaxPool2D ()( lateral_connection_2345 [ - 1 ]) return lateral_connection_2345 + [ p6 ]","title":"Fpn"},{"location":"reference/kerod/model/backbone/fpn/#module-kerodmodelbackbonefpn","text":"None None View Source import tensorflow as tf from kerod . utils . documentation import remove_unwanted_doc from tensorflow . keras import layers from tensorflow . keras . initializers import VarianceScaling __ pdoc__ = {} class FPN ( layers . Layer ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack)\"\"\" def __ init__ ( self , dim = 256 , kernel_regularizer = None , **kwargs ) : super (). __ init__ ( **kwargs ) self . _ dim = dim self . _ kernel_regularizer = kernel_regularizer def build ( self , input_shape ) : num_level_pyramid = len ( input_shape [ 0 ]) self . lateral_connection_2345 = [ layers . Conv2D ( self . _ dim , ( 1 , 1 ), padding='same' , kernel_initializer = VarianceScaling ( scale = 1. ), kernel_regularizer = self . _ kernel_regularizer ) for _ in range ( num_level_pyramid ) ] self . anti_aliasing_conv = [ layers . Conv2D ( self . _ dim , ( 3 , 3 ), padding='same' , kernel_initializer = VarianceScaling ( scale = 1. ), kernel_regularizer = self . _ kernel_regularizer ) for _ in range ( num_level_pyramid ) ] super (). build ( input_shape ) def call ( self , inputs ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack) Arguments: inputs: A list of tensors of shape [N, height, widht, channels] Returns: A list of tensors of shape [N + 1, height, width, channels] \"\"\" lateral_connection_2345 = [ conv ( tensor ) for tensor , conv in zip ( inputs , self . lateral_connection_2345 ) ] lat_sum_5432 = [] for idx , block in enumerate ( lateral_connection_2345 [ ::- 1 ]) : if idx > 0 : up_shape = tf . shape ( block ) block = block + tf . image . resize ( lat_sum_5432 [ - 1 ], [ up_shape [ 1 ], up_shape [ 2 ]], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) lat_sum_5432 . append ( block ) # 3 \u00d73convolution on each merged map to generate the final feature map , # which is to reduce the aliasing effect of upsampling . lateral_connection_2345 = [ conv ( tensor ) for conv , tensor in zip ( self . anti_aliasing_conv , lat_sum_5432 [ ::- 1 ]) ] p6 = layers . MaxPool2D ()( lateral_connection_2345 [ - 1 ]) return lateral_connection_2345 + [ p6 ] def get_config ( self ) : base_config = super (). get_config () base_config [ 'dim' ] = self . _ dim return base_config remove_unwanted_doc ( FPN , __ pdoc__ )","title":"Module kerod.model.backbone.fpn"},{"location":"reference/kerod/model/backbone/fpn/#classes","text":"","title":"Classes"},{"location":"reference/kerod/model/backbone/fpn/#fpn","text":"class FPN ( dim = 256 , kernel_regularizer = None , ** kwargs )","title":"FPN"},{"location":"reference/kerod/model/backbone/fpn/#ancestors-in-mro","text":"tensorflow.python.keras.engine.base_layer.Layer tensorflow.python.module.module.Module tensorflow.python.training.tracking.tracking.AutoTrackable tensorflow.python.training.tracking.base.Trackable tensorflow.python.keras.utils.version_utils.LayerVersionSelector","title":"Ancestors (in MRO)"},{"location":"reference/kerod/model/backbone/fpn/#methods","text":"","title":"Methods"},{"location":"reference/kerod/model/backbone/fpn/#call","text":"def call ( self , inputs ) Over your backbone feature build a FPN (inspired from tensorpack) Parameters: Name Description inputs A list of tensors of shape [N, height, widht, channels] Returns: Type Description None A list of tensors of shape [N + 1, height, width, channels] View Source def call ( self , inputs ) : \"\"\"Over your backbone feature build a FPN (inspired from tensorpack) Arguments: inputs: A list of tensors of shape [N, height, widht, channels] Returns: A list of tensors of shape [N + 1, height, width, channels] \"\"\" lateral_connection_2345 = [ conv ( tensor ) for tensor , conv in zip ( inputs , self . lateral_connection_2345 ) ] lat_sum_5432 = [] for idx , block in enumerate ( lateral_connection_2345 [ ::- 1 ]) : if idx > 0 : up_shape = tf . shape ( block ) block = block + tf . image . resize ( lat_sum_5432 [ - 1 ], [ up_shape [ 1 ], up_shape [ 2 ]], method = tf . image . ResizeMethod . NEAREST_NEIGHBOR ) lat_sum_5432 . append ( block ) # 3 \u00d73convolution on each merged map to generate the final feature map , # which is to reduce the aliasing effect of upsampling . lateral_connection_2345 = [ conv ( tensor ) for conv , tensor in zip ( self . anti_aliasing_conv , lat_sum_5432 [ ::- 1 ]) ] p6 = layers . MaxPool2D ()( lateral_connection_2345 [ - 1 ]) return lateral_connection_2345 + [ p6 ]","title":"call"},{"location":"reference/kerod/model/backbone/resnet/","text":"Module kerod.model.backbone.resnet ResNet models for Keras. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) View Source # Copyright 2015 The TensorFlow Authors and Modified by Emilien Garreau. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \"\"\"ResNet models for Keras. Reference paper: - [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) \"\"\" import os from typing import Callable import tensorflow as tf from tensorflow.keras import layers from tensorflow.python.keras import backend from tensorflow.python.keras.applications import resnet from tensorflow.python.keras.engine import training from tensorflow.python.keras.utils import data_utils , layer_utils OFFICIAL_WEIGHTS_PATH = ( 'https://storage.googleapis.com/tensorflow/keras-applications/resnet/' ) CUSTOM_WEIGHTS_PATH = ( 'https://files.heuritech.com/raw_files/' ) WEIGHTS_HASHES = { 'resnet50' : ( '4d473c1dd8becc155b73f8504c6f6626' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet50_pytorch' : ( '3ffd584081cc56435a3689d12afd7cf9' , CUSTOM_WEIGHTS_PATH , 'resnet50_tensorpack_conversion.h5' ), 'resnet101' : ( '88cf7a10940856eca736dc7b7e228a21' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet152' : ( 'ee4c566cf9a93f14d82f913c2dc6dd0c' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet50v2' : ( 'fac2f116257151a9d068a22e544a4917' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet101v2' : ( 'c0ed64b8031c3730f411d2eb4eea35b5' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet152v2' : ( 'ed17cf2e0169df9d443503ef94b23b33' , OFFICIAL_WEIGHTS_PATH , None ), 'resnext50' : ( '62527c363bdd9ec598bed41947b379fc' , OFFICIAL_WEIGHTS_PATH , None ), 'resnext101' : ( '0f678c91647380debd923963594981b3' , OFFICIAL_WEIGHTS_PATH , None ) } def padd_for_aligning_pixels ( inputs : tf . Tensor ): \"\"\"This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32. \"\"\" chan = inputs . shape [ 3 ] b4_stride = 32.0 shape2d = tf . shape ( inputs )[ 1 : 3 ] new_shape2d = tf . cast ( tf . math . ceil ( tf . cast ( shape2d , tf . float32 ) / b4_stride ) * b4_stride , tf . int32 ) pad_shape2d = new_shape2d - shape2d inputs = tf . pad ( inputs , tf . stack ([[ 0 , 0 ], [ 3 , 2 + pad_shape2d [ 0 ]], [ 3 , 2 + pad_shape2d [ 1 ]], [ 0 , 0 ]]), name = 'conv1_pad' ) # yapf: disable inputs . set_shape ([ None , None , None , chan ]) return inputs def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tf . keras . Model : \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at `~/.keras/keras.json`. Caution: Be sure to properly pre-process your inputs to the application. Please see `applications.resnet.preprocess_input` for an example. Arguments: stack_fn: a function that returns output tensor for the stacked residual blocks. preprocessing_func: a function that returns the corresponding preprocessing of the network. preact: whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias: whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name: string, model name. include_top: whether to include the fully-connected layer at the top of the network. weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model. input_shape: optional shape tuple, only to be specified if `include_top` is False (otherwise the input shape has to be `(224, 224, 3)` (with `channels_last` data format) or `(3, 224, 224)` (with `channels_first` data format). It should have exactly 3 inputs channels. kwargs: For backwards compatibility only. Returns: A `keras.Model` instance. Raises: *ValueError*: in case of invalid argument for `weights`, or invalid input shape. \"\"\" if kwargs : raise ValueError ( 'Unknown argument(s): %s ' % ( kwargs ,)) if not ( weights in { 'imagenet' , None } or os . path . exists ( weights )): raise ValueError ( 'The `weights` argument should be either ' '`None` (random initialization), `imagenet` ' '(pre-training on ImageNet), ' 'or the path to the weights file to be loaded.' ) if input_tensor is None : img_input = layers . Input ( shape = input_shape ) else : if not backend . is_keras_tensor ( input_tensor ): img_input = layers . Input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 x = layers . Lambda ( preprocessing_func , name = \"preprocess_input\" )( img_input ) x = layers . Lambda ( padd_for_aligning_pixels , name = \"padd_for_aligning_pixels\" )( x ) x = layers . Conv2D ( 64 , 7 , strides = 2 , use_bias = use_bias , name = 'conv1_conv' )( x ) if not preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'conv1_bn' )( x ) x = layers . Activation ( 'relu' , name = 'conv1_relu' )( x ) x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = 'pool1_pad' )( x ) x = layers . MaxPooling2D ( 3 , strides = 2 , name = 'pool1_pool' )( x ) outputs = stack_fn ( x ) if preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'post_bn' )( outputs [ - 1 ]) outputs [ - 1 ] = layers . Activation ( 'relu' , name = 'post_relu' )( x ) # Ensure that the model takes into account # any potential predecessors of `input_tensor`. if input_tensor is not None : inputs = layer_utils . get_source_inputs ( input_tensor ) else : inputs = img_input # Create model. model = training . Model ( inputs , outputs , name = model_name ) # Load weights. if ( weights == 'imagenet' ) and ( model_name in WEIGHTS_HASHES ): file_hash , base_weight_path , file_name = WEIGHTS_HASHES [ model_name ] if file_name is None : file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5' weights_path = data_utils . get_file ( file_name , base_weight_path + file_name , cache_subdir = 'models' , file_hash = file_hash ) model . load_weights ( weights_path ) elif weights is not None : model . load_weights ( weights ) return model def ResNet50 ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 architecture.\"\"\" # Or set to None resnet . layers = tf . keras . layers def stack_fn ( x ): c2 = resnet . stack1 ( x , 64 , 3 , stride1 = 1 , name = 'conv2' ) c3 = resnet . stack1 ( c2 , 128 , 4 , name = 'conv3' ) c4 = resnet . stack1 ( c3 , 256 , 6 , name = 'conv4' ) c5 = resnet . stack1 ( c4 , 512 , 3 , name = 'conv5' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , resnet . preprocess_input , False , True , 'resnet50' , weights , input_tensor , input_shape , ** kwargs ) def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. **Warning**: Do not forget to use `bgr` instead of `rgb`. ```python import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", shuffle_files=True, with_info=True) ds_train = ds_train.map(functools.partial(preprocess, bgr=True), num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` \"\"\" def stack_fn ( x ): c2 = Group ( x , 64 , 3 , strides = 1 , name = 'resnet50/group0' ) c3 = Group ( c2 , 128 , 4 , strides = 2 , name = 'resnet50/group1' ) c4 = Group ( c3 , 256 , 6 , strides = 2 , name = 'resnet50/group2' ) c5 = Group ( c4 , 512 , 3 , strides = 2 , name = 'resnet50/group3' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , preprocess_input_pytorch , False , False , 'resnet50_pytorch' , weights , input_tensor , input_shape , ** kwargs ) def preprocess_input_pytorch ( images : tf . Tensor ): \"\"\"Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. \"\"\" mean = tf . constant ([ 103.53 , 116.28 , 123.675 ], dtype = images . dtype ) std = tf . constant ([ 57.375 , 57.12 , 58.395 ], dtype = images . dtype ) images = ( images - mean ) / std return images def Group ( inputs : tf . Tensor , filters : int , blocks : int , strides : int , name = None ): \"\"\"A set of stacked residual blocks with the pytorch style Arguments: filters: integer, filters of the bottleneck layer in a block. blocks: number of blocks in the stacked blocks. strides: Stride of the second conv layer in the first block. \"\"\" x = Block ( inputs , filters , strides , use_conv_shortcut = True , name = f ' { name } /block0' ) for i in range ( 1 , blocks ): x = Block ( x , filters , 1 , use_conv_shortcut = False , name = f ' { name } /block { i } ' ) return x def Block ( inputs : tf . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ): \"\"\"A residual block with the pytorch_style Arguments: inputs: The inputs tensor filters: integer, filters of the bottleneck layer. strides: default 1, stride of the second convolution layer. In the official Keras implementation the stride is performed on the first convolution. This is different in the pytorch implementation. use_conv_shortcut: Use convolution shortcut if True, otherwise identity shortcut. \"\"\" bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 if use_conv_shortcut : shortcut = layers . Conv2D ( 4 * filters , 1 , strides = strides , use_bias = False , padding = 'same' , name = f ' { name } /convshortcut' )( inputs ) shortcut = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /convshortcut/bn' )( shortcut ) else : shortcut = inputs x = layers . Conv2D ( filters , 1 , strides = 1 , use_bias = False , padding = 'same' , name = f ' { name } /conv1' )( inputs ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv1/bn' )( x ) x = layers . Activation ( 'relu' , name = f ' { name } /conv1/relu' )( x ) if strides == 2 : x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = f ' { name } /pad2' )( x ) x = layers . Conv2D ( filters , 3 , padding = 'valid' , use_bias = False , strides = strides , name = f ' { name } /conv2' )( x ) else : x = layers . Conv2D ( filters , 3 , use_bias = False , padding = 'same' , strides = strides , name = f ' { name } /conv2' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv2/bn' )( x ) x = layers . Activation ( 'relu' , name = f ' { name } /conv2/relu' )( x ) x = layers . Conv2D ( filters * 4 , 1 , use_bias = False , padding = 'same' , name = f ' { name } /conv3' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv3/bn' )( x ) x = layers . Add ()([ shortcut , x ]) return layers . Activation ( 'relu' , name = f ' { name } /last_relu' )( x ) Variables CUSTOM_WEIGHTS_PATH OFFICIAL_WEIGHTS_PATH WEIGHTS_HASHES Functions Block def Block ( inputs : tensorflow . python . framework . ops . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ) A residual block with the pytorch_style Parameters: Name Description inputs The inputs tensor filters integer, filters of the bottleneck layer. strides default 1, stride of the second convolution layer. In the official Keras implementation the stride is performed on the first convolution. This is different in the pytorch implementation. use_conv_shortcut Use convolution shortcut if True, otherwise identity shortcut. View Source def Block ( inputs : tf . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ) : \"\"\" A residual block with the pytorch_style Arguments : inputs : The inputs tensor filters : integer , filters of the bottleneck layer . strides : default 1 , stride of the second convolution layer . In the official Keras implementation the stride is performed on the first convolution . This is different in the pytorch implementation . use_conv_shortcut : Use convolution shortcut if True , otherwise identity shortcut . \"\"\" bn_axis = 3 if backend . image_data_format () == ' channels_last ' else 1 if use_conv_shortcut : shortcut = layers . Conv2D ( 4 * filters , 1 , strides = strides , use_bias = False , padding = ' same ' , name = f ' {name}/convshortcut ' )( inputs ) shortcut = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/convshortcut/bn ' )( shortcut ) else : shortcut = inputs x = layers . Conv2D ( filters , 1 , strides = 1 , use_bias = False , padding = ' same ' , name = f ' {name}/conv1 ' )( inputs ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv1/bn ' )( x ) x = layers . Activation ( ' relu ' , name = f ' {name}/conv1/relu ' )( x ) if strides == 2 : x = layers . ZeroPadding2D ( padding = (( 1 , 0 ) , ( 1 , 0 )) , name = f ' {name}/pad2 ' )( x ) x = layers . Conv2D ( filters , 3 , padding = ' valid ' , use_bias = False , strides = strides , name = f ' {name}/conv2 ' )( x ) else : x = layers . Conv2D ( filters , 3 , use_bias = False , padding = ' same ' , strides = strides , name = f ' {name}/conv2 ' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv2/bn ' )( x ) x = layers . Activation ( ' relu ' , name = f ' {name}/conv2/relu ' )( x ) x = layers . Conv2D ( filters * 4 , 1 , use_bias = False , padding = ' same ' , name = f ' {name}/conv3 ' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv3/bn ' )( x ) x = layers . Add ()( [ shortcut , x ] ) return layers . Activation ( ' relu ' , name = f ' {name}/last_relu ' )( x ) Group def Group ( inputs : tensorflow . python . framework . ops . Tensor , filters : int , blocks : int , strides : int , name = None ) A set of stacked residual blocks with the pytorch style Parameters: Name Description filters integer, filters of the bottleneck layer in a block. blocks number of blocks in the stacked blocks. strides Stride of the second conv layer in the first block. View Source def Group ( inputs : tf . Tensor , filters : int , blocks : int , strides : int , name = None ) : \"\"\" A set of stacked residual blocks with the pytorch style Arguments : filters : integer , filters of the bottleneck layer in a block . blocks : number of blocks in the stacked blocks . strides : Stride of the second conv layer in the first block . \"\"\" x = Block ( inputs , filters , strides , use_conv_shortcut = True , name = f ' {name}/block0 ' ) for i in range ( 1 , blocks ) : x = Block ( x , filters , 1 , use_conv_shortcut = False , name = f ' {name}/block{i} ' ) return x ResNet def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tensorflow . python . keras . engine . training . Model Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at ~/.keras/keras.json . Caution: Be sure to properly pre-process your inputs to the application. Please see applications.resnet.preprocess_input for an example. Parameters: Name Description stack_fn a function that returns output tensor for the stacked residual blocks. preprocessing_func a function that returns the corresponding preprocessing of the network. preact whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name string, model name. include_top whether to include the fully-connected layer at the top of the network. weights one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. kwargs For backwards compatibility only. Returns: Type Description None A keras.Model instance. Raises: Type Description ValueError in case of invalid argument for weights , or invalid input shape. View Source def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tf . keras . Model : \" \"\" Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at `~/.keras/keras.json`. Caution: Be sure to properly pre-process your inputs to the application. Please see `applications.resnet.preprocess_input` for an example. Arguments: stack_fn: a function that returns output tensor for the stacked residual blocks. preprocessing_func: a function that returns the corresponding preprocessing of the network. preact: whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias: whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name: string, model name. include_top: whether to include the fully-connected layer at the top of the network. weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model. input_shape: optional shape tuple, only to be specified if `include_top` is False (otherwise the input shape has to be `(224, 224, 3)` (with `channels_last` data format) or `(3, 224, 224)` (with `channels_first` data format). It should have exactly 3 inputs channels. kwargs: For backwards compatibility only. Returns: A `keras.Model` instance. Raises: *ValueError*: in case of invalid argument for `weights`, or invalid input shape. \"\" \" if kwargs : raise ValueError ( 'Unknown argument(s): %s' % ( kwargs ,)) if not ( weights in { 'imagenet' , None } or os . path . exists ( weights )) : raise ValueError ( 'The `weights` argument should be either ' '`None` (random initialization), `imagenet` ' '(pre-training on ImageNet), ' 'or the path to the weights file to be loaded.' ) if input_tensor is None : img_input = layers . Input ( shape = input_shape ) else : if not backend . is_keras_tensor ( input_tensor ) : img_input = layers . Input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 x = layers . Lambda ( preprocessing_func , name = \"preprocess_input\" )( img_input ) x = layers . Lambda ( padd_for_aligning_pixels , name = \"padd_for_aligning_pixels\" )( x ) x = layers . Conv2D ( 64 , 7 , strides = 2 , use_bias = use_bias , name = 'conv1_conv' )( x ) if not preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'conv1_bn' )( x ) x = layers . Activation ( 'relu' , name = 'conv1_relu' )( x ) x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = 'pool1_pad' )( x ) x = layers . MaxPooling2D ( 3 , strides = 2 , name = 'pool1_pool' )( x ) outputs = stack_fn ( x ) if preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'post_bn' )( outputs [ - 1 ] ) outputs [ - 1 ] = layers . Activation ( 'relu' , name = 'post_relu' )( x ) # Ensure that the model takes into account # any potential predecessors of `input_tensor`. if input_tensor is not None : inputs = layer_utils . get_source_inputs ( input_tensor ) else : inputs = img_input # Create model. model = training . Model ( inputs , outputs , name = model_name ) # Load weights. if ( weights == 'imagenet' ) and ( model_name in WEIGHTS_HASHES ) : file_hash , base_weight_path , file_name = WEIGHTS_HASHES [ model_name ] if file_name is None : file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5' weights_path = data_utils . get_file ( file_name , base_weight_path + file_name , cache_subdir = 'models' , file_hash = file_hash ) model . load_weights ( weights_path ) elif weights is not None : model . load_weights ( weights ) return model ResNet50 def ResNet50 ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) Instantiates the ResNet50 architecture. View Source def ResNet50 ( weights = ' imagenet ' , input_tensor = None , input_shape = None , ** kwargs ) : \"\"\" Instantiates the ResNet50 architecture. \"\"\" # Or set to None resnet . layers = tf . keras . layers def stack_fn ( x ) : c2 = resnet . stack1 ( x , 64 , 3 , stride1 = 1 , name = ' conv2 ' ) c3 = resnet . stack1 ( c2 , 128 , 4 , name = ' conv3 ' ) c4 = resnet . stack1 ( c3 , 256 , 6 , name = ' conv4 ' ) c5 = resnet . stack1 ( c4 , 512 , 3 , name = ' conv5 ' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , resnet . preprocess_input , False , True , ' resnet50 ' , weights , input_tensor , input_shape , ** kwargs ) ResNet50PytorchStyle def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. Warning : Do not forget to use bgr instead of rgb . import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train , ds_info = tfds . load ( name = \"coco/2017\" , split = \"train\" , shuffle_files = True , with_info = True ) ds_train = ds_train . map ( functools . partial ( preprocess , bgr = True ), num_parallel_calls = tf . data . experimental . AUTOTUNE ) View Source def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. **Warning**: Do not forget to use `bgr` instead of `rgb`. ```python import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", shuffle_files=True, with_info=True) ds_train = ds_train.map(functools.partial(preprocess, bgr=True), num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` \"\"\" def stack_fn ( x ): c2 = Group ( x , 64 , 3 , strides = 1 , name = 'resnet50/group0' ) c3 = Group ( c2 , 128 , 4 , strides = 2 , name = 'resnet50/group1' ) c4 = Group ( c3 , 256 , 6 , strides = 2 , name = 'resnet50/group2' ) c5 = Group ( c4 , 512 , 3 , strides = 2 , name = 'resnet50/group3' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , preprocess_input_pytorch , False , False , 'resnet50_pytorch' , weights , input_tensor , input_shape , ** kwargs ) padd_for_aligning_pixels def padd_for_aligning_pixels ( inputs : tensorflow . python . framework . ops . Tensor ) This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32. View Source def padd_for_aligning_pixels ( inputs : tf . Tensor ) : \"\"\" This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32 . \"\"\" chan = inputs . shape [ 3 ] b4_stride = 32 . 0 shape2d = tf . shape ( inputs ) [ 1 : 3 ] new_shape2d = tf . cast ( tf . math . ceil ( tf . cast ( shape2d , tf . float32 ) / b4_stride ) * b4_stride , tf . int32 ) pad_shape2d = new_shape2d - shape2d inputs = tf . pad ( inputs , tf . stack ( [[ 0 , 0 ], [ 3 , 2 + pad_shape2d [ 0 ]], [ 3 , 2 + pad_shape2d [ 1 ]], [ 0 , 0 ]] ) , name = ' conv1_pad ' ) # yapf : disable inputs . set_shape ( [ None , None , None , chan ] ) return inputs preprocess_input_pytorch def preprocess_input_pytorch ( images : tensorflow . python . framework . ops . Tensor ) Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. View Source def preprocess_input_pytorch ( images : tf . Tensor ): \"\"\"Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. \"\"\" mean = tf . constant ([ 103.53 , 116.28 , 123.675 ], dtype = images . dtype ) std = tf . constant ([ 57.375 , 57.12 , 58.395 ], dtype = images . dtype ) images = ( images - mean ) / std return images","title":"Resnet"},{"location":"reference/kerod/model/backbone/resnet/#module-kerodmodelbackboneresnet","text":"ResNet models for Keras. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) View Source # Copyright 2015 The TensorFlow Authors and Modified by Emilien Garreau. All Rights Reserved. # # Licensed under the Apache License, Version 2.0 (the \"License\"); # you may not use this file except in compliance with the License. # You may obtain a copy of the License at # # http://www.apache.org/licenses/LICENSE-2.0 # # Unless required by applicable law or agreed to in writing, software # distributed under the License is distributed on an \"AS IS\" BASIS, # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. # See the License for the specific language governing permissions and # limitations under the License. # ============================================================================== \"\"\"ResNet models for Keras. Reference paper: - [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) \"\"\" import os from typing import Callable import tensorflow as tf from tensorflow.keras import layers from tensorflow.python.keras import backend from tensorflow.python.keras.applications import resnet from tensorflow.python.keras.engine import training from tensorflow.python.keras.utils import data_utils , layer_utils OFFICIAL_WEIGHTS_PATH = ( 'https://storage.googleapis.com/tensorflow/keras-applications/resnet/' ) CUSTOM_WEIGHTS_PATH = ( 'https://files.heuritech.com/raw_files/' ) WEIGHTS_HASHES = { 'resnet50' : ( '4d473c1dd8becc155b73f8504c6f6626' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet50_pytorch' : ( '3ffd584081cc56435a3689d12afd7cf9' , CUSTOM_WEIGHTS_PATH , 'resnet50_tensorpack_conversion.h5' ), 'resnet101' : ( '88cf7a10940856eca736dc7b7e228a21' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet152' : ( 'ee4c566cf9a93f14d82f913c2dc6dd0c' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet50v2' : ( 'fac2f116257151a9d068a22e544a4917' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet101v2' : ( 'c0ed64b8031c3730f411d2eb4eea35b5' , OFFICIAL_WEIGHTS_PATH , None ), 'resnet152v2' : ( 'ed17cf2e0169df9d443503ef94b23b33' , OFFICIAL_WEIGHTS_PATH , None ), 'resnext50' : ( '62527c363bdd9ec598bed41947b379fc' , OFFICIAL_WEIGHTS_PATH , None ), 'resnext101' : ( '0f678c91647380debd923963594981b3' , OFFICIAL_WEIGHTS_PATH , None ) } def padd_for_aligning_pixels ( inputs : tf . Tensor ): \"\"\"This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32. \"\"\" chan = inputs . shape [ 3 ] b4_stride = 32.0 shape2d = tf . shape ( inputs )[ 1 : 3 ] new_shape2d = tf . cast ( tf . math . ceil ( tf . cast ( shape2d , tf . float32 ) / b4_stride ) * b4_stride , tf . int32 ) pad_shape2d = new_shape2d - shape2d inputs = tf . pad ( inputs , tf . stack ([[ 0 , 0 ], [ 3 , 2 + pad_shape2d [ 0 ]], [ 3 , 2 + pad_shape2d [ 1 ]], [ 0 , 0 ]]), name = 'conv1_pad' ) # yapf: disable inputs . set_shape ([ None , None , None , chan ]) return inputs def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tf . keras . Model : \"\"\"Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at `~/.keras/keras.json`. Caution: Be sure to properly pre-process your inputs to the application. Please see `applications.resnet.preprocess_input` for an example. Arguments: stack_fn: a function that returns output tensor for the stacked residual blocks. preprocessing_func: a function that returns the corresponding preprocessing of the network. preact: whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias: whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name: string, model name. include_top: whether to include the fully-connected layer at the top of the network. weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model. input_shape: optional shape tuple, only to be specified if `include_top` is False (otherwise the input shape has to be `(224, 224, 3)` (with `channels_last` data format) or `(3, 224, 224)` (with `channels_first` data format). It should have exactly 3 inputs channels. kwargs: For backwards compatibility only. Returns: A `keras.Model` instance. Raises: *ValueError*: in case of invalid argument for `weights`, or invalid input shape. \"\"\" if kwargs : raise ValueError ( 'Unknown argument(s): %s ' % ( kwargs ,)) if not ( weights in { 'imagenet' , None } or os . path . exists ( weights )): raise ValueError ( 'The `weights` argument should be either ' '`None` (random initialization), `imagenet` ' '(pre-training on ImageNet), ' 'or the path to the weights file to be loaded.' ) if input_tensor is None : img_input = layers . Input ( shape = input_shape ) else : if not backend . is_keras_tensor ( input_tensor ): img_input = layers . Input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 x = layers . Lambda ( preprocessing_func , name = \"preprocess_input\" )( img_input ) x = layers . Lambda ( padd_for_aligning_pixels , name = \"padd_for_aligning_pixels\" )( x ) x = layers . Conv2D ( 64 , 7 , strides = 2 , use_bias = use_bias , name = 'conv1_conv' )( x ) if not preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'conv1_bn' )( x ) x = layers . Activation ( 'relu' , name = 'conv1_relu' )( x ) x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = 'pool1_pad' )( x ) x = layers . MaxPooling2D ( 3 , strides = 2 , name = 'pool1_pool' )( x ) outputs = stack_fn ( x ) if preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'post_bn' )( outputs [ - 1 ]) outputs [ - 1 ] = layers . Activation ( 'relu' , name = 'post_relu' )( x ) # Ensure that the model takes into account # any potential predecessors of `input_tensor`. if input_tensor is not None : inputs = layer_utils . get_source_inputs ( input_tensor ) else : inputs = img_input # Create model. model = training . Model ( inputs , outputs , name = model_name ) # Load weights. if ( weights == 'imagenet' ) and ( model_name in WEIGHTS_HASHES ): file_hash , base_weight_path , file_name = WEIGHTS_HASHES [ model_name ] if file_name is None : file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5' weights_path = data_utils . get_file ( file_name , base_weight_path + file_name , cache_subdir = 'models' , file_hash = file_hash ) model . load_weights ( weights_path ) elif weights is not None : model . load_weights ( weights ) return model def ResNet50 ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 architecture.\"\"\" # Or set to None resnet . layers = tf . keras . layers def stack_fn ( x ): c2 = resnet . stack1 ( x , 64 , 3 , stride1 = 1 , name = 'conv2' ) c3 = resnet . stack1 ( c2 , 128 , 4 , name = 'conv3' ) c4 = resnet . stack1 ( c3 , 256 , 6 , name = 'conv4' ) c5 = resnet . stack1 ( c4 , 512 , 3 , name = 'conv5' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , resnet . preprocess_input , False , True , 'resnet50' , weights , input_tensor , input_shape , ** kwargs ) def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. **Warning**: Do not forget to use `bgr` instead of `rgb`. ```python import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", shuffle_files=True, with_info=True) ds_train = ds_train.map(functools.partial(preprocess, bgr=True), num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` \"\"\" def stack_fn ( x ): c2 = Group ( x , 64 , 3 , strides = 1 , name = 'resnet50/group0' ) c3 = Group ( c2 , 128 , 4 , strides = 2 , name = 'resnet50/group1' ) c4 = Group ( c3 , 256 , 6 , strides = 2 , name = 'resnet50/group2' ) c5 = Group ( c4 , 512 , 3 , strides = 2 , name = 'resnet50/group3' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , preprocess_input_pytorch , False , False , 'resnet50_pytorch' , weights , input_tensor , input_shape , ** kwargs ) def preprocess_input_pytorch ( images : tf . Tensor ): \"\"\"Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. \"\"\" mean = tf . constant ([ 103.53 , 116.28 , 123.675 ], dtype = images . dtype ) std = tf . constant ([ 57.375 , 57.12 , 58.395 ], dtype = images . dtype ) images = ( images - mean ) / std return images def Group ( inputs : tf . Tensor , filters : int , blocks : int , strides : int , name = None ): \"\"\"A set of stacked residual blocks with the pytorch style Arguments: filters: integer, filters of the bottleneck layer in a block. blocks: number of blocks in the stacked blocks. strides: Stride of the second conv layer in the first block. \"\"\" x = Block ( inputs , filters , strides , use_conv_shortcut = True , name = f ' { name } /block0' ) for i in range ( 1 , blocks ): x = Block ( x , filters , 1 , use_conv_shortcut = False , name = f ' { name } /block { i } ' ) return x def Block ( inputs : tf . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ): \"\"\"A residual block with the pytorch_style Arguments: inputs: The inputs tensor filters: integer, filters of the bottleneck layer. strides: default 1, stride of the second convolution layer. In the official Keras implementation the stride is performed on the first convolution. This is different in the pytorch implementation. use_conv_shortcut: Use convolution shortcut if True, otherwise identity shortcut. \"\"\" bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 if use_conv_shortcut : shortcut = layers . Conv2D ( 4 * filters , 1 , strides = strides , use_bias = False , padding = 'same' , name = f ' { name } /convshortcut' )( inputs ) shortcut = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /convshortcut/bn' )( shortcut ) else : shortcut = inputs x = layers . Conv2D ( filters , 1 , strides = 1 , use_bias = False , padding = 'same' , name = f ' { name } /conv1' )( inputs ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv1/bn' )( x ) x = layers . Activation ( 'relu' , name = f ' { name } /conv1/relu' )( x ) if strides == 2 : x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = f ' { name } /pad2' )( x ) x = layers . Conv2D ( filters , 3 , padding = 'valid' , use_bias = False , strides = strides , name = f ' { name } /conv2' )( x ) else : x = layers . Conv2D ( filters , 3 , use_bias = False , padding = 'same' , strides = strides , name = f ' { name } /conv2' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv2/bn' )( x ) x = layers . Activation ( 'relu' , name = f ' { name } /conv2/relu' )( x ) x = layers . Conv2D ( filters * 4 , 1 , use_bias = False , padding = 'same' , name = f ' { name } /conv3' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' { name } /conv3/bn' )( x ) x = layers . Add ()([ shortcut , x ]) return layers . Activation ( 'relu' , name = f ' { name } /last_relu' )( x )","title":"Module kerod.model.backbone.resnet"},{"location":"reference/kerod/model/backbone/resnet/#variables","text":"CUSTOM_WEIGHTS_PATH OFFICIAL_WEIGHTS_PATH WEIGHTS_HASHES","title":"Variables"},{"location":"reference/kerod/model/backbone/resnet/#functions","text":"","title":"Functions"},{"location":"reference/kerod/model/backbone/resnet/#block","text":"def Block ( inputs : tensorflow . python . framework . ops . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ) A residual block with the pytorch_style Parameters: Name Description inputs The inputs tensor filters integer, filters of the bottleneck layer. strides default 1, stride of the second convolution layer. In the official Keras implementation the stride is performed on the first convolution. This is different in the pytorch implementation. use_conv_shortcut Use convolution shortcut if True, otherwise identity shortcut. View Source def Block ( inputs : tf . Tensor , filters : int , strides : int = 1 , use_conv_shortcut = True , name = None ) : \"\"\" A residual block with the pytorch_style Arguments : inputs : The inputs tensor filters : integer , filters of the bottleneck layer . strides : default 1 , stride of the second convolution layer . In the official Keras implementation the stride is performed on the first convolution . This is different in the pytorch implementation . use_conv_shortcut : Use convolution shortcut if True , otherwise identity shortcut . \"\"\" bn_axis = 3 if backend . image_data_format () == ' channels_last ' else 1 if use_conv_shortcut : shortcut = layers . Conv2D ( 4 * filters , 1 , strides = strides , use_bias = False , padding = ' same ' , name = f ' {name}/convshortcut ' )( inputs ) shortcut = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/convshortcut/bn ' )( shortcut ) else : shortcut = inputs x = layers . Conv2D ( filters , 1 , strides = 1 , use_bias = False , padding = ' same ' , name = f ' {name}/conv1 ' )( inputs ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv1/bn ' )( x ) x = layers . Activation ( ' relu ' , name = f ' {name}/conv1/relu ' )( x ) if strides == 2 : x = layers . ZeroPadding2D ( padding = (( 1 , 0 ) , ( 1 , 0 )) , name = f ' {name}/pad2 ' )( x ) x = layers . Conv2D ( filters , 3 , padding = ' valid ' , use_bias = False , strides = strides , name = f ' {name}/conv2 ' )( x ) else : x = layers . Conv2D ( filters , 3 , use_bias = False , padding = ' same ' , strides = strides , name = f ' {name}/conv2 ' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv2/bn ' )( x ) x = layers . Activation ( ' relu ' , name = f ' {name}/conv2/relu ' )( x ) x = layers . Conv2D ( filters * 4 , 1 , use_bias = False , padding = ' same ' , name = f ' {name}/conv3 ' )( x ) x = layers . BatchNormalization ( axis = bn_axis , name = f ' {name}/conv3/bn ' )( x ) x = layers . Add ()( [ shortcut , x ] ) return layers . Activation ( ' relu ' , name = f ' {name}/last_relu ' )( x )","title":"Block"},{"location":"reference/kerod/model/backbone/resnet/#group","text":"def Group ( inputs : tensorflow . python . framework . ops . Tensor , filters : int , blocks : int , strides : int , name = None ) A set of stacked residual blocks with the pytorch style Parameters: Name Description filters integer, filters of the bottleneck layer in a block. blocks number of blocks in the stacked blocks. strides Stride of the second conv layer in the first block. View Source def Group ( inputs : tf . Tensor , filters : int , blocks : int , strides : int , name = None ) : \"\"\" A set of stacked residual blocks with the pytorch style Arguments : filters : integer , filters of the bottleneck layer in a block . blocks : number of blocks in the stacked blocks . strides : Stride of the second conv layer in the first block . \"\"\" x = Block ( inputs , filters , strides , use_conv_shortcut = True , name = f ' {name}/block0 ' ) for i in range ( 1 , blocks ) : x = Block ( x , filters , 1 , use_conv_shortcut = False , name = f ' {name}/block{i} ' ) return x","title":"Group"},{"location":"reference/kerod/model/backbone/resnet/#resnet","text":"def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tensorflow . python . keras . engine . training . Model Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at ~/.keras/keras.json . Caution: Be sure to properly pre-process your inputs to the application. Please see applications.resnet.preprocess_input for an example. Parameters: Name Description stack_fn a function that returns output tensor for the stacked residual blocks. preprocessing_func a function that returns the corresponding preprocessing of the network. preact whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name string, model name. include_top whether to include the fully-connected layer at the top of the network. weights one of None (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor optional Keras tensor (i.e. output of layers.Input() ) to use as image input for the model. input_shape optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (224, 224, 3) (with channels_last data format) or (3, 224, 224) (with channels_first data format). It should have exactly 3 inputs channels. kwargs For backwards compatibility only. Returns: Type Description None A keras.Model instance. Raises: Type Description ValueError in case of invalid argument for weights , or invalid input shape. View Source def ResNet ( stack_fn : Callable , preprocessing_func : Callable , preact : bool , use_bias : bool , model_name = 'resnet' , weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) -> tf . keras . Model : \" \"\" Instantiates the ResNet, ResNetV2, and ResNeXt architecture. Reference paper: [Deep Residual Learning for Image Recognition] (https://arxiv.org/abs/1512.03385) (CVPR 2015) Optionally loads weights pre-trained on ImageNet. Note that the data format convention used by the model is the one specified in your Keras config at `~/.keras/keras.json`. Caution: Be sure to properly pre-process your inputs to the application. Please see `applications.resnet.preprocess_input` for an example. Arguments: stack_fn: a function that returns output tensor for the stacked residual blocks. preprocessing_func: a function that returns the corresponding preprocessing of the network. preact: whether to use pre-activation or not (True for ResNetV2, False for ResNet and ResNeXt). use_bias: whether to use biases for convolutional layers or not (True for ResNet and ResNetV2, False for ResNeXt). model_name: string, model name. include_top: whether to include the fully-connected layer at the top of the network. weights: one of `None` (random initialization), 'imagenet' (pre-training on ImageNet), or the path to the weights file to be loaded. input_tensor: optional Keras tensor (i.e. output of `layers.Input()`) to use as image input for the model. input_shape: optional shape tuple, only to be specified if `include_top` is False (otherwise the input shape has to be `(224, 224, 3)` (with `channels_last` data format) or `(3, 224, 224)` (with `channels_first` data format). It should have exactly 3 inputs channels. kwargs: For backwards compatibility only. Returns: A `keras.Model` instance. Raises: *ValueError*: in case of invalid argument for `weights`, or invalid input shape. \"\" \" if kwargs : raise ValueError ( 'Unknown argument(s): %s' % ( kwargs ,)) if not ( weights in { 'imagenet' , None } or os . path . exists ( weights )) : raise ValueError ( 'The `weights` argument should be either ' '`None` (random initialization), `imagenet` ' '(pre-training on ImageNet), ' 'or the path to the weights file to be loaded.' ) if input_tensor is None : img_input = layers . Input ( shape = input_shape ) else : if not backend . is_keras_tensor ( input_tensor ) : img_input = layers . Input ( tensor = input_tensor , shape = input_shape ) else : img_input = input_tensor bn_axis = 3 if backend . image_data_format () == 'channels_last' else 1 x = layers . Lambda ( preprocessing_func , name = \"preprocess_input\" )( img_input ) x = layers . Lambda ( padd_for_aligning_pixels , name = \"padd_for_aligning_pixels\" )( x ) x = layers . Conv2D ( 64 , 7 , strides = 2 , use_bias = use_bias , name = 'conv1_conv' )( x ) if not preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'conv1_bn' )( x ) x = layers . Activation ( 'relu' , name = 'conv1_relu' )( x ) x = layers . ZeroPadding2D ( padding = (( 1 , 0 ), ( 1 , 0 )), name = 'pool1_pad' )( x ) x = layers . MaxPooling2D ( 3 , strides = 2 , name = 'pool1_pool' )( x ) outputs = stack_fn ( x ) if preact : x = layers . BatchNormalization ( axis = bn_axis , epsilon = 1.001e-5 , name = 'post_bn' )( outputs [ - 1 ] ) outputs [ - 1 ] = layers . Activation ( 'relu' , name = 'post_relu' )( x ) # Ensure that the model takes into account # any potential predecessors of `input_tensor`. if input_tensor is not None : inputs = layer_utils . get_source_inputs ( input_tensor ) else : inputs = img_input # Create model. model = training . Model ( inputs , outputs , name = model_name ) # Load weights. if ( weights == 'imagenet' ) and ( model_name in WEIGHTS_HASHES ) : file_hash , base_weight_path , file_name = WEIGHTS_HASHES [ model_name ] if file_name is None : file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_notop.h5' weights_path = data_utils . get_file ( file_name , base_weight_path + file_name , cache_subdir = 'models' , file_hash = file_hash ) model . load_weights ( weights_path ) elif weights is not None : model . load_weights ( weights ) return model","title":"ResNet"},{"location":"reference/kerod/model/backbone/resnet/#resnet50","text":"def ResNet50 ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) Instantiates the ResNet50 architecture. View Source def ResNet50 ( weights = ' imagenet ' , input_tensor = None , input_shape = None , ** kwargs ) : \"\"\" Instantiates the ResNet50 architecture. \"\"\" # Or set to None resnet . layers = tf . keras . layers def stack_fn ( x ) : c2 = resnet . stack1 ( x , 64 , 3 , stride1 = 1 , name = ' conv2 ' ) c3 = resnet . stack1 ( c2 , 128 , 4 , name = ' conv3 ' ) c4 = resnet . stack1 ( c3 , 256 , 6 , name = ' conv4 ' ) c5 = resnet . stack1 ( c4 , 512 , 3 , name = ' conv5 ' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , resnet . preprocess_input , False , True , ' resnet50 ' , weights , input_tensor , input_shape , ** kwargs )","title":"ResNet50"},{"location":"reference/kerod/model/backbone/resnet/#resnet50pytorchstyle","text":"def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ) Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. Warning : Do not forget to use bgr instead of rgb . import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train , ds_info = tfds . load ( name = \"coco/2017\" , split = \"train\" , shuffle_files = True , with_info = True ) ds_train = ds_train . map ( functools . partial ( preprocess , bgr = True ), num_parallel_calls = tf . data . experimental . AUTOTUNE ) View Source def ResNet50PytorchStyle ( weights = 'imagenet' , input_tensor = None , input_shape = None , ** kwargs ): \"\"\"Instantiates the ResNet50 with the pytorch style architecture. In the bottleneck residual block, pytorch-style ResNet uses a 1x1 stride-1 convolutional layer followed by a 3x3 stride-2 convolutional layer. **Warning**: Do not forget to use `bgr` instead of `rgb`. ```python import functools import tensorflow_datasets as tfds from kerod.preprocessing import preprocess ds_train, ds_info = tfds.load(name=\"coco/2017\", split=\"train\", shuffle_files=True, with_info=True) ds_train = ds_train.map(functools.partial(preprocess, bgr=True), num_parallel_calls=tf.data.experimental.AUTOTUNE) ``` \"\"\" def stack_fn ( x ): c2 = Group ( x , 64 , 3 , strides = 1 , name = 'resnet50/group0' ) c3 = Group ( c2 , 128 , 4 , strides = 2 , name = 'resnet50/group1' ) c4 = Group ( c3 , 256 , 6 , strides = 2 , name = 'resnet50/group2' ) c5 = Group ( c4 , 512 , 3 , strides = 2 , name = 'resnet50/group3' ) return [ c2 , c3 , c4 , c5 ] return ResNet ( stack_fn , preprocess_input_pytorch , False , False , 'resnet50_pytorch' , weights , input_tensor , input_shape , ** kwargs )","title":"ResNet50PytorchStyle"},{"location":"reference/kerod/model/backbone/resnet/#padd_for_aligning_pixels","text":"def padd_for_aligning_pixels ( inputs : tensorflow . python . framework . ops . Tensor ) This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32. View Source def padd_for_aligning_pixels ( inputs : tf . Tensor ) : \"\"\" This padding operation is here to make the pixels of the output perfectly aligned. It will make the output perfectly aligned at stride 32 . \"\"\" chan = inputs . shape [ 3 ] b4_stride = 32 . 0 shape2d = tf . shape ( inputs ) [ 1 : 3 ] new_shape2d = tf . cast ( tf . math . ceil ( tf . cast ( shape2d , tf . float32 ) / b4_stride ) * b4_stride , tf . int32 ) pad_shape2d = new_shape2d - shape2d inputs = tf . pad ( inputs , tf . stack ( [[ 0 , 0 ], [ 3 , 2 + pad_shape2d [ 0 ]], [ 3 , 2 + pad_shape2d [ 1 ]], [ 0 , 0 ]] ) , name = ' conv1_pad ' ) # yapf : disable inputs . set_shape ( [ None , None , None , chan ] ) return inputs","title":"padd_for_aligning_pixels"},{"location":"reference/kerod/model/backbone/resnet/#preprocess_input_pytorch","text":"def preprocess_input_pytorch ( images : tensorflow . python . framework . ops . Tensor ) Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. View Source def preprocess_input_pytorch ( images : tf . Tensor ): \"\"\"Will preprocess the images for the resnet trained on Tensorpack. The network has been trained using BGR. \"\"\" mean = tf . constant ([ 103.53 , 116.28 , 123.675 ], dtype = images . dtype ) std = tf . constant ([ 57.375 , 57.12 , 58.395 ], dtype = images . dtype ) images = ( images - mean ) / std return images","title":"preprocess_input_pytorch"},{"location":"reference/kerod/utils/","text":"Module kerod.utils None None View Source from kerod.utils.ops import item_assignment , get_full_indices Sub-modules kerod.utils.documentation kerod.utils.drawing kerod.utils.ops kerod.utils.training","title":"Index"},{"location":"reference/kerod/utils/#module-kerodutils","text":"None None View Source from kerod.utils.ops import item_assignment , get_full_indices","title":"Module kerod.utils"},{"location":"reference/kerod/utils/#sub-modules","text":"kerod.utils.documentation kerod.utils.drawing kerod.utils.ops kerod.utils.training","title":"Sub-modules"},{"location":"reference/kerod/utils/documentation/","text":"Module kerod.utils.documentation None None View Source import tensorflow as tf from tensorflow.python.keras.engine.training import Model def remove_unwanted_doc ( _class , pdoc : dict ): \"\"\"Remove unwanted documentation from tensorflow keras inheritance.\"\"\" if issubclass ( _class , Model ): doc_class_to_remove = Model elif issubclass ( _class , tf . keras . layers . Layer ): doc_class_to_remove = tf . keras . layers . Layer for k in doc_class_to_remove . __dict__ : if k not in [ 'call' , '__doc__' , '__module__' , '__init__' , '__name__' ]: pdoc [ f ' { _class . __name__ } . { k } ' ] = None pdoc [ f ' { _class . __name__ } .with_name_scope' ] = None Functions remove_unwanted_doc def remove_unwanted_doc ( _class , pdoc : dict ) Remove unwanted documentation from tensorflow keras inheritance. View Source def remove_unwanted_doc ( _class , pdoc : dict ) : \"\"\" Remove unwanted documentation from tensorflow keras inheritance. \"\"\" if issubclass ( _class , Model ) : doc_class_to_remove = Model elif issubclass ( _class , tf . keras . layers . Layer ) : doc_class_to_remove = tf . keras . layers . Layer for k in doc_class_to_remove . __dict__ : if k not in [ ' call ' , ' __doc__ ' , ' __module__ ' , ' __init__ ' , ' __name__ ' ]: pdoc [ f ' {_class.__name__}.{k} ' ] = None pdoc [ f ' {_class.__name__}.with_name_scope ' ] = None","title":"Documentation"},{"location":"reference/kerod/utils/documentation/#module-kerodutilsdocumentation","text":"None None View Source import tensorflow as tf from tensorflow.python.keras.engine.training import Model def remove_unwanted_doc ( _class , pdoc : dict ): \"\"\"Remove unwanted documentation from tensorflow keras inheritance.\"\"\" if issubclass ( _class , Model ): doc_class_to_remove = Model elif issubclass ( _class , tf . keras . layers . Layer ): doc_class_to_remove = tf . keras . layers . Layer for k in doc_class_to_remove . __dict__ : if k not in [ 'call' , '__doc__' , '__module__' , '__init__' , '__name__' ]: pdoc [ f ' { _class . __name__ } . { k } ' ] = None pdoc [ f ' { _class . __name__ } .with_name_scope' ] = None","title":"Module kerod.utils.documentation"},{"location":"reference/kerod/utils/documentation/#functions","text":"","title":"Functions"},{"location":"reference/kerod/utils/documentation/#remove_unwanted_doc","text":"def remove_unwanted_doc ( _class , pdoc : dict ) Remove unwanted documentation from tensorflow keras inheritance. View Source def remove_unwanted_doc ( _class , pdoc : dict ) : \"\"\" Remove unwanted documentation from tensorflow keras inheritance. \"\"\" if issubclass ( _class , Model ) : doc_class_to_remove = Model elif issubclass ( _class , tf . keras . layers . Layer ) : doc_class_to_remove = tf . keras . layers . Layer for k in doc_class_to_remove . __dict__ : if k not in [ ' call ' , ' __doc__ ' , ' __module__ ' , ' __init__ ' , ' __name__ ' ]: pdoc [ f ' {_class.__name__}.{k} ' ] = None pdoc [ f ' {_class.__name__}.with_name_scope ' ] = None","title":"remove_unwanted_doc"},{"location":"reference/kerod/utils/drawing/","text":"Module kerod.utils.drawing None None View Source from typing import List , Union import matplotlib.colors as pltc import matplotlib.patches as patches import matplotlib.pyplot as plt import numpy as np import tensorflow as tf class BoxDrawer : \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *classes*: The ordered list of classes that we could display \"\"\" def __init__ ( self , classes : List [ str ]): self . _classes = classes all_colors = [ color for color in pltc . cnames . keys ()] self . _colors = [ all_colors [ i ] for i in np . random . randint ( 0 , len ( all_colors ), size = len ( classes )) ] def __call__ ( self , images , images_information , boxes , labels : Union [ np . array , List [ List [ int ]], tf . Tensor ], scores : Union [ np . array , List [ List [ float ]], tf . Tensor ], num_valid_detections : Union [ np . array , List [ int ], tf . Tensor ], resize = True ): \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *images*: An image (tf.Tensor or numpy array) with shape [batch, height, width, 3] - *images_information*: A 2D (tf.Tensor or numpy array) and shape [batch, (height, width)]. It contains the shape of the image without any padding. It allows to remove padding of the image - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [batch, num_boxes, (y_min, x_min, y_max, x_max)] are as described by `1`. If set to false the boxes won't be resized. - *labels*: Label of the predicted boxes. - *scores*: Score of the predicted boxes. - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs \"\"\" if isinstance ( images_information , np . ndarray ): images_information = images_information . astype ( np . int32 ) elif tf . is_tensor ( images_information ): images_information = tf . cast ( images_information , tf . int32 ) if isinstance ( labels , np . ndarray ): labels = labels . tolist () elif tf . is_tensor ( labels ): labels = labels . numpy () . tolist () if isinstance ( scores , np . ndarray ): scores = scores . tolist () elif tf . is_tensor ( scores ): scores = scores . numpy () . tolist () if isinstance ( num_valid_detections , np . ndarray ): num_valid_detections = num_valid_detections . tolist () elif tf . is_tensor ( num_valid_detections ): num_valid_detections = num_valid_detections . numpy () . tolist () for im , im_info , bb , cls , scr , nvd in zip ( images , images_information , boxes , labels , scores , num_valid_detections ): labels = [ self . _classes [ int ( ind )] for ind in cls ] colors = [ self . _colors [ int ( ind )] for ind in cls ] im = im [: im_info [ 0 ], : im_info [ 1 ]] draw_bounding_boxes ( im , bb , scores = scr , labels = labels , num_valid_detections = nvd , resize = resize , colors = colors ) def draw_bounding_boxes ( image , boxes , labels : list = None , scores : np . array = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ): \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *image*: An image (tf.Tensor or numpy array)with shape [height, width, 3] - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] - *labels*: A list of string corresponding to the predicted label. - *scores*: A list of scores predicted - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *colors*: A list of matplotlib colors of length = num_boxes - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by `1`. If set to false the boxes won't be resized. \"\"\" if isinstance ( image , tf . Tensor ): image = image . numpy () if isinstance ( boxes , tf . Tensor ): boxes = boxes . numpy () plt . figure ( figsize = ( 15 , 15 )) plt . imshow ( image . astype ( np . uint8 )) axes = plt . gca () if isinstance ( boxes , tf . Tensor ): boxes = boxes . numpy () if resize : boxes [:, 0 :: 2 ] *= image . shape [ 0 ] boxes [:, 1 :: 2 ] *= image . shape [ 1 ] if num_valid_detections is not None : boxes = boxes [: num_valid_detections ] for i , box in enumerate ( boxes ): x_y = ( box [ 1 ], box [ 0 ]) width = box [ 3 ] - box [ 1 ] height = box [ 2 ] - box [ 0 ] color = 'r' if colors is None else colors [ i ] # Draw the boxes r = patches . Rectangle ( x_y , width , height , fill = False , edgecolor = color ) axes . add_patch ( r ) if labels is not None : score = scores [ i ] if scores is not None else 1 axes . annotate ( f ' { labels [ i ] } _ { str ( round ( score , 4 )) } ' , x_y , color = color , weight = 'bold' , fontsize = 10 , ha = 'center' , va = 'center' ) plt . show () Functions draw_bounding_boxes def draw_bounding_boxes ( image , boxes , labels : list = None , scores : < built - in function array > = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ) Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: floats in [0.0, 1.0] relative to the width and height of the underlying image already resized to the corresponding size Arguments: image : An image (tf.Tensor or numpy array)with shape [height, width, 3] boxes : An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] labels : A list of string corresponding to the predicted label. scores : A list of scores predicted num_valid_detections : Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. colors : A list of matplotlib colors of length = num_boxes resize : Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by 1 . If set to false the boxes won't be resized. View Source def draw_bounding_boxes ( image , boxes , labels : list = None , scores : np . array = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ) : \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *image*: An image (tf.Tensor or numpy array)with shape [height, width, 3] - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] - *labels*: A list of string corresponding to the predicted label. - *scores*: A list of scores predicted - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *colors*: A list of matplotlib colors of length = num_boxes - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by `1`. If set to false the boxes won't be resized. \"\"\" if isinstance ( image , tf . Tensor ) : image = image . numpy () if isinstance ( boxes , tf . Tensor ) : boxes = boxes . numpy () plt . figure ( figsize = ( 15 , 15 )) plt . imshow ( image . astype ( np . uint8 )) axes = plt . gca () if isinstance ( boxes , tf . Tensor ) : boxes = boxes . numpy () if resize : boxes [ :, 0::2 ] *= image . shape [ 0 ] boxes [ :, 1::2 ] *= image . shape [ 1 ] if num_valid_detections is not None : boxes = boxes [ :num_valid_detections ] for i , box in enumerate ( boxes ) : x_y = ( box [ 1 ] , box [ 0 ] ) width = box [ 3 ] - box [ 1 ] height = box [ 2 ] - box [ 0 ] color = 'r' if colors is None else colors [ i ] # Draw the boxes r = patches . Rectangle ( x_y , width , height , fill = False , edgecolor = color ) axes . add_patch ( r ) if labels is not None : score = scores [ i ] if scores is not None else 1 axes . annotate ( f '{labels[i]}_{str(round(score, 4))}' , x_y , color = color , weight = 'bold' , fontsize = 10 , ha = 'center' , va = 'center' ) plt . show () Classes BoxDrawer class BoxDrawer ( classes : List [ str ] ) by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: floats in [0.0, 1.0] relative to the width and height of the underlying image already resized to the corresponding size Arguments: classes : The ordered list of classes that we could display","title":"Drawing"},{"location":"reference/kerod/utils/drawing/#module-kerodutilsdrawing","text":"None None View Source from typing import List , Union import matplotlib.colors as pltc import matplotlib.patches as patches import matplotlib.pyplot as plt import numpy as np import tensorflow as tf class BoxDrawer : \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *classes*: The ordered list of classes that we could display \"\"\" def __init__ ( self , classes : List [ str ]): self . _classes = classes all_colors = [ color for color in pltc . cnames . keys ()] self . _colors = [ all_colors [ i ] for i in np . random . randint ( 0 , len ( all_colors ), size = len ( classes )) ] def __call__ ( self , images , images_information , boxes , labels : Union [ np . array , List [ List [ int ]], tf . Tensor ], scores : Union [ np . array , List [ List [ float ]], tf . Tensor ], num_valid_detections : Union [ np . array , List [ int ], tf . Tensor ], resize = True ): \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *images*: An image (tf.Tensor or numpy array) with shape [batch, height, width, 3] - *images_information*: A 2D (tf.Tensor or numpy array) and shape [batch, (height, width)]. It contains the shape of the image without any padding. It allows to remove padding of the image - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [batch, num_boxes, (y_min, x_min, y_max, x_max)] are as described by `1`. If set to false the boxes won't be resized. - *labels*: Label of the predicted boxes. - *scores*: Score of the predicted boxes. - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs \"\"\" if isinstance ( images_information , np . ndarray ): images_information = images_information . astype ( np . int32 ) elif tf . is_tensor ( images_information ): images_information = tf . cast ( images_information , tf . int32 ) if isinstance ( labels , np . ndarray ): labels = labels . tolist () elif tf . is_tensor ( labels ): labels = labels . numpy () . tolist () if isinstance ( scores , np . ndarray ): scores = scores . tolist () elif tf . is_tensor ( scores ): scores = scores . numpy () . tolist () if isinstance ( num_valid_detections , np . ndarray ): num_valid_detections = num_valid_detections . tolist () elif tf . is_tensor ( num_valid_detections ): num_valid_detections = num_valid_detections . numpy () . tolist () for im , im_info , bb , cls , scr , nvd in zip ( images , images_information , boxes , labels , scores , num_valid_detections ): labels = [ self . _classes [ int ( ind )] for ind in cls ] colors = [ self . _colors [ int ( ind )] for ind in cls ] im = im [: im_info [ 0 ], : im_info [ 1 ]] draw_bounding_boxes ( im , bb , scores = scr , labels = labels , num_valid_detections = nvd , resize = resize , colors = colors ) def draw_bounding_boxes ( image , boxes , labels : list = None , scores : np . array = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ): \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *image*: An image (tf.Tensor or numpy array)with shape [height, width, 3] - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] - *labels*: A list of string corresponding to the predicted label. - *scores*: A list of scores predicted - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *colors*: A list of matplotlib colors of length = num_boxes - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by `1`. If set to false the boxes won't be resized. \"\"\" if isinstance ( image , tf . Tensor ): image = image . numpy () if isinstance ( boxes , tf . Tensor ): boxes = boxes . numpy () plt . figure ( figsize = ( 15 , 15 )) plt . imshow ( image . astype ( np . uint8 )) axes = plt . gca () if isinstance ( boxes , tf . Tensor ): boxes = boxes . numpy () if resize : boxes [:, 0 :: 2 ] *= image . shape [ 0 ] boxes [:, 1 :: 2 ] *= image . shape [ 1 ] if num_valid_detections is not None : boxes = boxes [: num_valid_detections ] for i , box in enumerate ( boxes ): x_y = ( box [ 1 ], box [ 0 ]) width = box [ 3 ] - box [ 1 ] height = box [ 2 ] - box [ 0 ] color = 'r' if colors is None else colors [ i ] # Draw the boxes r = patches . Rectangle ( x_y , width , height , fill = False , edgecolor = color ) axes . add_patch ( r ) if labels is not None : score = scores [ i ] if scores is not None else 1 axes . annotate ( f ' { labels [ i ] } _ { str ( round ( score , 4 )) } ' , x_y , color = color , weight = 'bold' , fontsize = 10 , ha = 'center' , va = 'center' ) plt . show ()","title":"Module kerod.utils.drawing"},{"location":"reference/kerod/utils/drawing/#functions","text":"","title":"Functions"},{"location":"reference/kerod/utils/drawing/#draw_bounding_boxes","text":"def draw_bounding_boxes ( image , boxes , labels : list = None , scores : < built - in function array > = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ) Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: floats in [0.0, 1.0] relative to the width and height of the underlying image already resized to the corresponding size Arguments: image : An image (tf.Tensor or numpy array)with shape [height, width, 3] boxes : An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] labels : A list of string corresponding to the predicted label. scores : A list of scores predicted num_valid_detections : Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. colors : A list of matplotlib colors of length = num_boxes resize : Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by 1 . If set to false the boxes won't be resized. View Source def draw_bounding_boxes ( image , boxes , labels : list = None , scores : np . array = None , num_valid_detections : int = None , colors : List [ str ] = None , resize = True ) : \"\"\"Outputs a copy of images but draws on top of the pixels zero or more bounding boxes specified by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: 1. floats in [0.0, 1.0] relative to the width and height of the underlying image 2. already resized to the corresponding size Arguments: - *image*: An image (tf.Tensor or numpy array)with shape [height, width, 3] - *boxes*: An array (tf.Tensor, or Numpy array) of boxes with the following shape [num_boxes, (y_min, x_min, y_max, x_max)] - *labels*: A list of string corresponding to the predicted label. - *scores*: A list of scores predicted - *num_valid_detections*: Number of boxes that we need to display. By default it will display all the boxes. This argument is useful whenever we are inference mode. The network perform padding operation and num_valid_detections is the value which allow to know which boxes were padded. - *colors*: A list of matplotlib colors of length = num_boxes - *resize*: Allow to resize the bounding boxes to the proper size. If set to true the inputs are as described by `1`. If set to false the boxes won't be resized. \"\"\" if isinstance ( image , tf . Tensor ) : image = image . numpy () if isinstance ( boxes , tf . Tensor ) : boxes = boxes . numpy () plt . figure ( figsize = ( 15 , 15 )) plt . imshow ( image . astype ( np . uint8 )) axes = plt . gca () if isinstance ( boxes , tf . Tensor ) : boxes = boxes . numpy () if resize : boxes [ :, 0::2 ] *= image . shape [ 0 ] boxes [ :, 1::2 ] *= image . shape [ 1 ] if num_valid_detections is not None : boxes = boxes [ :num_valid_detections ] for i , box in enumerate ( boxes ) : x_y = ( box [ 1 ] , box [ 0 ] ) width = box [ 3 ] - box [ 1 ] height = box [ 2 ] - box [ 0 ] color = 'r' if colors is None else colors [ i ] # Draw the boxes r = patches . Rectangle ( x_y , width , height , fill = False , edgecolor = color ) axes . add_patch ( r ) if labels is not None : score = scores [ i ] if scores is not None else 1 axes . annotate ( f '{labels[i]}_{str(round(score, 4))}' , x_y , color = color , weight = 'bold' , fontsize = 10 , ha = 'center' , va = 'center' ) plt . show ()","title":"draw_bounding_boxes"},{"location":"reference/kerod/utils/drawing/#classes","text":"","title":"Classes"},{"location":"reference/kerod/utils/drawing/#boxdrawer","text":"class BoxDrawer ( classes : List [ str ] ) by the locations in boxes. The coordinates of the each bounding box in boxes are encoded as [y_min, x_min, y_max, x_max]. The bounding box coordinates can be: floats in [0.0, 1.0] relative to the width and height of the underlying image already resized to the corresponding size Arguments: classes : The ordered list of classes that we could display","title":"BoxDrawer"},{"location":"reference/kerod/utils/ops/","text":"Module kerod.utils.ops None None View Source import tensorflow as tf def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = tf . float32 ): \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: - *indices*: 1d Tensor with integer indices which are to be set to indices_values. - *size*: scalar with size (integer) of output Tensor. - *indices_value*: values of elements specified by indices in the output vector - *default_value*: values of other elements in the output vector. - *dtype*: data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" size = tf . cast ( size , dtype = tf . int32 ) zeros = tf . ones ([ size ], dtype = dtype ) * default_value values = tf . ones_like ( indices , dtype = dtype ) * indices_value return tf . dynamic_stitch ([ tf . range ( size ), tf . cast ( indices , dtype = tf . int32 )], [ zeros , values ]) def item_assignment ( tensor : tf . Tensor , indicator : tf . Tensor , val ): \"\"\"Set the indicated fields of tensor to val. ```python tensor = tf.constant([1, 2, 3, 4]) # won't work in tensorflow tensor[tensor == 2] = 1 tensor = item_assignment(tensor, tensor == 2, 1) ``` Arguments: - *tensor*: A tensor without shape constraint. - *indicator*: boolean tensor with same shape as `tensor`. - *val*: scalar with value to set. \"\"\" indicator = tf . cast ( indicator , tensor . dtype ) return tensor * ( 1 - indicator ) + val * indicator def get_full_indices ( indices ): \"\"\" This operation allows to extract full indices from indices. These full-indices have the proper format for gather_nd operations. Example: ```python indices = [[0, 1], [2, 3]] full_indices = [[[0, 0], [0, 1]], [[1, 2], [1, 3]]] ``` Arguments: - *indices*: Indices without their assorciated batch format [batch_size, k]. Returns: Full-indices tensor [batch_size, k, 2] \"\"\" batch_size = tf . shape ( indices )[ 0 ] num_elements = tf . shape ( indices )[ 1 ] batch_ids = tf . range ( 0 , batch_size ) # Repeat the batch indice for every indices per batch # [0, 1, ..., n] => [[0, ..., 0], [1, ..., 1], ..., [n, ..., n]] batch_ids = tf . tile ( batch_ids [:, None ], [ 1 , num_elements ]) # [[a1, ..., au], ...,[n1, ..., nu]] => # [[[0, a1], ..., [0, au]], ..., [[n, n1], ..., [n, nu]]] full_indices = tf . concat ( [ batch_ids [ ... , None ], indices [ ... , None ]], axis =- 1 ) return full_indices Functions get_full_indices def get_full_indices ( indices ) This operation allows to extract full indices from indices. These full-indices have the proper format for gather_nd operations. Example: indices = [[ 0 , 1 ], [ 2 , 3 ]] full_indices = [[[ 0 , 0 ], [ 0 , 1 ]], [[ 1 , 2 ], [ 1 , 3 ]]] Arguments: indices : Indices without their assorciated batch format [batch_size, k]. Returns: Full-indices tensor [batch_size, k, 2] View Source def get_full_indices ( indices ) : \"\"\" This operation allows to extract full indices from indices. These full - indices have the proper format for gather_nd operations . Example : ``` python indices = [[ 0 , 1 ], [ 2 , 3 ]] full_indices = [[[ 0 , 0 ], [ 0 , 1 ]], [[ 1 , 2 ], [ 1 , 3 ]]] ``` Arguments : - * indices * : Indices without their assorciated batch format [ batch_size , k ]. Returns : Full - indices tensor [ batch_size , k , 2 ] \"\"\" batch_size = tf . shape ( indices ) [ 0 ] num_elements = tf . shape ( indices ) [ 1 ] batch_ids = tf . range ( 0 , batch_size ) # Repeat the batch indice for every indices per batch # [ 0 , 1 , ..., n ] => [[ 0 , ..., 0 ], [ 1 , ..., 1 ], ..., [ n , ..., n ]] batch_ids = tf . tile ( batch_ids [:, None ], [ 1 , num_elements ] ) # [[ a1 , ..., au ], ...,[ n1 , ..., nu ]] => # [[[ 0 , a1 ], ..., [ 0 , au ]], ..., [[ n , n1 ], ..., [ n , nu ]]] full_indices = tf . concat ( [ batch_ids [..., None ], indices [..., None ]], axis =- 1 ) return full_indices indices_to_dense_vector def indices_to_dense_vector ( indices , size , indices_value = 1.0 , default_value = 0 , dtype = tf . float32 ) Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: indices : 1d Tensor with integer indices which are to be set to indices_values. size : scalar with size (integer) of output Tensor. indices_value : values of elements specified by indices in the output vector default_value : values of other elements in the output vector. dtype : data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. View Source def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = tf . float32 ) : \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: - *indices*: 1d Tensor with integer indices which are to be set to indices_values. - *size*: scalar with size (integer) of output Tensor. - *indices_value*: values of elements specified by indices in the output vector - *default_value*: values of other elements in the output vector. - *dtype*: data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" size = tf . cast ( size , dtype = tf . int32 ) zeros = tf . ones ( [ size ] , dtype = dtype ) * default_value values = tf . ones_like ( indices , dtype = dtype ) * indices_value return tf . dynamic_stitch ( [ tf.range(size), tf.cast(indices, dtype=tf.int32) ] , [ zeros, values ] ) item_assignment def item_assignment ( tensor : tensorflow . python . framework . ops . Tensor , indicator : tensorflow . python . framework . ops . Tensor , val ) Set the indicated fields of tensor to val. tensor = tf . constant ([ 1 , 2 , 3 , 4 ]) # won't work in tensorflow tensor [ tensor == 2 ] = 1 tensor = item_assignment ( tensor , tensor == 2 , 1 ) Arguments: tensor : A tensor without shape constraint. indicator : boolean tensor with same shape as tensor . val : scalar with value to set. View Source def item_assignment ( tensor : tf . Tensor , indicator : tf . Tensor , val ) : \" \"\" Set the indicated fields of tensor to val. ```python tensor = tf.constant([1, 2, 3, 4]) # won't work in tensorflow tensor[tensor == 2] = 1 tensor = item_assignment(tensor, tensor == 2, 1) ``` Arguments: - *tensor*: A tensor without shape constraint. - *indicator*: boolean tensor with same shape as `tensor`. - *val*: scalar with value to set. \"\" \" indicator = tf . cast ( indicator , tensor . dtype ) return tensor * ( 1 - indicator ) + val * indicator","title":"Ops"},{"location":"reference/kerod/utils/ops/#module-kerodutilsops","text":"None None View Source import tensorflow as tf def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = tf . float32 ): \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: - *indices*: 1d Tensor with integer indices which are to be set to indices_values. - *size*: scalar with size (integer) of output Tensor. - *indices_value*: values of elements specified by indices in the output vector - *default_value*: values of other elements in the output vector. - *dtype*: data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" size = tf . cast ( size , dtype = tf . int32 ) zeros = tf . ones ([ size ], dtype = dtype ) * default_value values = tf . ones_like ( indices , dtype = dtype ) * indices_value return tf . dynamic_stitch ([ tf . range ( size ), tf . cast ( indices , dtype = tf . int32 )], [ zeros , values ]) def item_assignment ( tensor : tf . Tensor , indicator : tf . Tensor , val ): \"\"\"Set the indicated fields of tensor to val. ```python tensor = tf.constant([1, 2, 3, 4]) # won't work in tensorflow tensor[tensor == 2] = 1 tensor = item_assignment(tensor, tensor == 2, 1) ``` Arguments: - *tensor*: A tensor without shape constraint. - *indicator*: boolean tensor with same shape as `tensor`. - *val*: scalar with value to set. \"\"\" indicator = tf . cast ( indicator , tensor . dtype ) return tensor * ( 1 - indicator ) + val * indicator def get_full_indices ( indices ): \"\"\" This operation allows to extract full indices from indices. These full-indices have the proper format for gather_nd operations. Example: ```python indices = [[0, 1], [2, 3]] full_indices = [[[0, 0], [0, 1]], [[1, 2], [1, 3]]] ``` Arguments: - *indices*: Indices without their assorciated batch format [batch_size, k]. Returns: Full-indices tensor [batch_size, k, 2] \"\"\" batch_size = tf . shape ( indices )[ 0 ] num_elements = tf . shape ( indices )[ 1 ] batch_ids = tf . range ( 0 , batch_size ) # Repeat the batch indice for every indices per batch # [0, 1, ..., n] => [[0, ..., 0], [1, ..., 1], ..., [n, ..., n]] batch_ids = tf . tile ( batch_ids [:, None ], [ 1 , num_elements ]) # [[a1, ..., au], ...,[n1, ..., nu]] => # [[[0, a1], ..., [0, au]], ..., [[n, n1], ..., [n, nu]]] full_indices = tf . concat ( [ batch_ids [ ... , None ], indices [ ... , None ]], axis =- 1 ) return full_indices","title":"Module kerod.utils.ops"},{"location":"reference/kerod/utils/ops/#functions","text":"","title":"Functions"},{"location":"reference/kerod/utils/ops/#get_full_indices","text":"def get_full_indices ( indices ) This operation allows to extract full indices from indices. These full-indices have the proper format for gather_nd operations. Example: indices = [[ 0 , 1 ], [ 2 , 3 ]] full_indices = [[[ 0 , 0 ], [ 0 , 1 ]], [[ 1 , 2 ], [ 1 , 3 ]]] Arguments: indices : Indices without their assorciated batch format [batch_size, k]. Returns: Full-indices tensor [batch_size, k, 2] View Source def get_full_indices ( indices ) : \"\"\" This operation allows to extract full indices from indices. These full - indices have the proper format for gather_nd operations . Example : ``` python indices = [[ 0 , 1 ], [ 2 , 3 ]] full_indices = [[[ 0 , 0 ], [ 0 , 1 ]], [[ 1 , 2 ], [ 1 , 3 ]]] ``` Arguments : - * indices * : Indices without their assorciated batch format [ batch_size , k ]. Returns : Full - indices tensor [ batch_size , k , 2 ] \"\"\" batch_size = tf . shape ( indices ) [ 0 ] num_elements = tf . shape ( indices ) [ 1 ] batch_ids = tf . range ( 0 , batch_size ) # Repeat the batch indice for every indices per batch # [ 0 , 1 , ..., n ] => [[ 0 , ..., 0 ], [ 1 , ..., 1 ], ..., [ n , ..., n ]] batch_ids = tf . tile ( batch_ids [:, None ], [ 1 , num_elements ] ) # [[ a1 , ..., au ], ...,[ n1 , ..., nu ]] => # [[[ 0 , a1 ], ..., [ 0 , au ]], ..., [[ n , n1 ], ..., [ n , nu ]]] full_indices = tf . concat ( [ batch_ids [..., None ], indices [..., None ]], axis =- 1 ) return full_indices","title":"get_full_indices"},{"location":"reference/kerod/utils/ops/#indices_to_dense_vector","text":"def indices_to_dense_vector ( indices , size , indices_value = 1.0 , default_value = 0 , dtype = tf . float32 ) Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: indices : 1d Tensor with integer indices which are to be set to indices_values. size : scalar with size (integer) of output Tensor. indices_value : values of elements specified by indices in the output vector default_value : values of other elements in the output vector. dtype : data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. View Source def indices_to_dense_vector ( indices , size , indices_value = 1. , default_value = 0 , dtype = tf . float32 ) : \"\"\"Creates dense vector with indices set to specific value and rest to zeros. This function exists because it is unclear if it is safe to use tf.sparse_to_dense(indices, [size], 1, validate_indices=False) with indices which are not ordered. This function accepts a dynamic size (e.g. tf.shape(tensor)[0]) Arguments: - *indices*: 1d Tensor with integer indices which are to be set to indices_values. - *size*: scalar with size (integer) of output Tensor. - *indices_value*: values of elements specified by indices in the output vector - *default_value*: values of other elements in the output vector. - *dtype*: data type. Returns: A dense 1D Tensor of shape [size] with indices set to indices_values and the rest set to default_value. \"\"\" size = tf . cast ( size , dtype = tf . int32 ) zeros = tf . ones ( [ size ] , dtype = dtype ) * default_value values = tf . ones_like ( indices , dtype = dtype ) * indices_value return tf . dynamic_stitch ( [ tf.range(size), tf.cast(indices, dtype=tf.int32) ] , [ zeros, values ] )","title":"indices_to_dense_vector"},{"location":"reference/kerod/utils/ops/#item_assignment","text":"def item_assignment ( tensor : tensorflow . python . framework . ops . Tensor , indicator : tensorflow . python . framework . ops . Tensor , val ) Set the indicated fields of tensor to val. tensor = tf . constant ([ 1 , 2 , 3 , 4 ]) # won't work in tensorflow tensor [ tensor == 2 ] = 1 tensor = item_assignment ( tensor , tensor == 2 , 1 ) Arguments: tensor : A tensor without shape constraint. indicator : boolean tensor with same shape as tensor . val : scalar with value to set. View Source def item_assignment ( tensor : tf . Tensor , indicator : tf . Tensor , val ) : \" \"\" Set the indicated fields of tensor to val. ```python tensor = tf.constant([1, 2, 3, 4]) # won't work in tensorflow tensor[tensor == 2] = 1 tensor = item_assignment(tensor, tensor == 2, 1) ``` Arguments: - *tensor*: A tensor without shape constraint. - *indicator*: boolean tensor with same shape as `tensor`. - *val*: scalar with value to set. \"\" \" indicator = tf . cast ( indicator , tensor . dtype ) return tensor * ( 1 - indicator ) + val * indicator","title":"item_assignment"},{"location":"reference/kerod/utils/training/","text":"Module kerod.utils.training None None View Source import tensorflow as tf from typing import Callable def freeze_layers_before ( model : tf . keras . Model , layer_name : str ): \"\"\"Freezes layers of a Keras `model` before a given `layer_name` (excluded).\"\"\" freeze_before = model . get_layer ( layer_name ) index_freeze_before = model . layers . index ( freeze_before ) for layer in model . layers [: index_freeze_before ]: layer . trainable = False def freeze_batch_normalization ( model : tf . keras . Model ): \"\"\"In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen. \"\"\" for layer in model . layers : if isinstance ( layer , tf . keras . layers . BatchNormalization ): layer . trainable = False def apply_kernel_regularization ( func : Callable , model : tf . keras . Model ): \"\"\"Apply kernel regularization on all the trainable layers of a Layer or a Model\"\"\" for layer in model . layers : if hasattr ( layer , 'kernel' ) and layer . trainable : model . add_loss ( func ( layer . kernel )) Functions apply_kernel_regularization def apply_kernel_regularization ( func : Callable , model : tensorflow . python . keras . engine . training . Model ) Apply kernel regularization on all the trainable layers of a Layer or a Model View Source def apply_kernel_regularization ( func: Callable , model: tf . keras . Model ) : \"\"\"Apply kernel regularization on all the trainable layers of a Layer or a Model\"\"\" for layer in model . layers: if hasattr ( layer , ' kernel ') and layer . trainable: model . add_loss ( func ( layer . kernel )) freeze_batch_normalization def freeze_batch_normalization ( model : tensorflow . python . keras . engine . training . Model ) In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen. View Source def freeze_batch_normalization ( model : tf . keras . Model ) : \"\"\" In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen . \"\"\" for layer in model . layers : if isinstance ( layer , tf . keras . layers . BatchNormalization ) : layer . trainable = False freeze_layers_before def freeze_layers_before ( model : tensorflow . python . keras . engine . training . Model , layer_name : str ) Freezes layers of a Keras model before a given layer_name (excluded). View Source def freeze_layers_before ( model : tf . keras . Model , layer_name : str ) : \" \"\" Freezes layers of a Keras `model` before a given `layer_name` (excluded). \"\" \" freeze_before = model . get_layer ( layer_name ) index_freeze_before = model . layers . index ( freeze_before ) for layer in model . layers [ : index_freeze_before ] : layer . trainable = False","title":"Training"},{"location":"reference/kerod/utils/training/#module-kerodutilstraining","text":"None None View Source import tensorflow as tf from typing import Callable def freeze_layers_before ( model : tf . keras . Model , layer_name : str ): \"\"\"Freezes layers of a Keras `model` before a given `layer_name` (excluded).\"\"\" freeze_before = model . get_layer ( layer_name ) index_freeze_before = model . layers . index ( freeze_before ) for layer in model . layers [: index_freeze_before ]: layer . trainable = False def freeze_batch_normalization ( model : tf . keras . Model ): \"\"\"In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen. \"\"\" for layer in model . layers : if isinstance ( layer , tf . keras . layers . BatchNormalization ): layer . trainable = False def apply_kernel_regularization ( func : Callable , model : tf . keras . Model ): \"\"\"Apply kernel regularization on all the trainable layers of a Layer or a Model\"\"\" for layer in model . layers : if hasattr ( layer , 'kernel' ) and layer . trainable : model . add_loss ( func ( layer . kernel ))","title":"Module kerod.utils.training"},{"location":"reference/kerod/utils/training/#functions","text":"","title":"Functions"},{"location":"reference/kerod/utils/training/#apply_kernel_regularization","text":"def apply_kernel_regularization ( func : Callable , model : tensorflow . python . keras . engine . training . Model ) Apply kernel regularization on all the trainable layers of a Layer or a Model View Source def apply_kernel_regularization ( func: Callable , model: tf . keras . Model ) : \"\"\"Apply kernel regularization on all the trainable layers of a Layer or a Model\"\"\" for layer in model . layers: if hasattr ( layer , ' kernel ') and layer . trainable: model . add_loss ( func ( layer . kernel ))","title":"apply_kernel_regularization"},{"location":"reference/kerod/utils/training/#freeze_batch_normalization","text":"def freeze_batch_normalization ( model : tensorflow . python . keras . engine . training . Model ) In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen. View Source def freeze_batch_normalization ( model : tf . keras . Model ) : \"\"\" In Object detection we usually do not train on big batch. The BatchNormalization is not useful and should be frozen . \"\"\" for layer in model . layers : if isinstance ( layer , tf . keras . layers . BatchNormalization ) : layer . trainable = False","title":"freeze_batch_normalization"},{"location":"reference/kerod/utils/training/#freeze_layers_before","text":"def freeze_layers_before ( model : tensorflow . python . keras . engine . training . Model , layer_name : str ) Freezes layers of a Keras model before a given layer_name (excluded). View Source def freeze_layers_before ( model : tf . keras . Model , layer_name : str ) : \" \"\" Freezes layers of a Keras `model` before a given `layer_name` (excluded). \"\" \" freeze_before = model . get_layer ( layer_name ) index_freeze_before = model . layers . index ( freeze_before ) for layer in model . layers [ : index_freeze_before ] : layer . trainable = False","title":"freeze_layers_before"}]}