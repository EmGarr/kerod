
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-7.3.0">
    
    
      
        <title>Faster Rcnn - kerod</title>
      
    
    
      <link rel="stylesheet" href="../../../../assets/stylesheets/main.8b42a75e.min.css">
      
        
        <link rel="stylesheet" href="../../../../assets/stylesheets/palette.3f5d1f46.min.css">
        
          
          
          <meta name="theme-color" content="#e92063">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      


    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="slate" data-md-color-primary="pink" data-md-color-accent="amber">
  
    
    <script>function __prefix(e){return new URL("../../../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#module-kerodmodelfaster_rcnn" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../.." title="kerod" class="md-header__button md-logo" aria-label="kerod" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            kerod
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Faster Rcnn
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/EmGarr/kerod/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    kerod
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../.." title="kerod" class="md-nav__button md-logo" aria-label="kerod" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    kerod
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/EmGarr/kerod/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    kerod
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../../CONTRIBUTING/" class="md-nav__link">
        Contributing
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../../../docs/BENCHMARKS/" class="md-nav__link">
        Benchmarks
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Reference" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Reference
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1" type="checkbox" id="__nav_4_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1">
          Kerod
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Kerod" data-md-level="2">
        <label class="md-nav__title" for="__nav_4_1">
          <span class="md-nav__icon md-icon"></span>
          Kerod
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1_2" type="checkbox" id="__nav_4_1_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1_2">
          Core
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Core" data-md-level="3">
        <label class="md-nav__title" for="__nav_4_1_2">
          <span class="md-nav__icon md-icon"></span>
          Core
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/box_coder/" class="md-nav__link">
        Box Coder
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/box_ops/" class="md-nav__link">
        Box Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/constants/" class="md-nav__link">
        Constants
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/learning_rate_schedule/" class="md-nav__link">
        Learning Rate Schedule
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/losses/" class="md-nav__link">
        Losses
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/matcher/" class="md-nav__link">
        Matcher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/sampling_ops/" class="md-nav__link">
        Sampling Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/similarity/" class="md-nav__link">
        Similarity
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/standard_fields/" class="md-nav__link">
        Standard Fields
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../core/target_assigner/" class="md-nav__link">
        Target Assigner
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1_3" type="checkbox" id="__nav_4_1_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1_3">
          Dataset
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Dataset" data-md-level="3">
        <label class="md-nav__title" for="__nav_4_1_3">
          <span class="md-nav__icon md-icon"></span>
          Dataset
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/augmentation/" class="md-nav__link">
        Augmentation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/preprocessing/" class="md-nav__link">
        Preprocessing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../dataset/utils/" class="md-nav__link">
        Utils
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1_4" type="checkbox" id="__nav_4_1_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4">
          Layers
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Layers" data-md-level="3">
        <label class="md-nav__title" for="__nav_4_1_4">
          <span class="md-nav__icon md-icon"></span>
          Layers
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/anchors/" class="md-nav__link">
        Anchors
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/attentions/" class="md-nav__link">
        Attentions
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/patches/" class="md-nav__link">
        Patches
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/positional_encoding/" class="md-nav__link">
        Positional Encoding
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/transformer/" class="md-nav__link">
        Transformer
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1_4_7" type="checkbox" id="__nav_4_1_4_7" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4_7">
          Detection
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Detection" data-md-level="4">
        <label class="md-nav__title" for="__nav_4_1_4_7">
          <span class="md-nav__icon md-icon"></span>
          Detection
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/detection/abstract_detection_head/" class="md-nav__link">
        Abstract Detection Head
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/detection/fast_rcnn/" class="md-nav__link">
        Fast Rcnn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/detection/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/detection/pooling_ops/" class="md-nav__link">
        Pooling Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/detection/rpn/" class="md-nav__link">
        Rpn
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1_4_8" type="checkbox" id="__nav_4_1_4_8" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4_8">
          Post Processing
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Post Processing" data-md-level="4">
        <label class="md-nav__title" for="__nav_4_1_4_8">
          <span class="md-nav__icon md-icon"></span>
          Post Processing
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/post_processing/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/post_processing/non_maximum_suppression/" class="md-nav__link">
        Non Maximum Suppression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/post_processing/post_processing_detr/" class="md-nav__link">
        Post Processing Detr
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1_4_9" type="checkbox" id="__nav_4_1_4_9" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1_4_9">
          Smca
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Smca" data-md-level="4">
        <label class="md-nav__title" for="__nav_4_1_4_9">
          <span class="md-nav__icon md-icon"></span>
          Smca
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/smca/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/smca/reference_points/" class="md-nav__link">
        Reference Points
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../layers/smca/weight_map/" class="md-nav__link">
        Weight Map
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1_5" type="checkbox" id="__nav_4_1_5" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1_5">
          Model
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Model" data-md-level="3">
        <label class="md-nav__title" for="__nav_4_1_5">
          <span class="md-nav__icon md-icon"></span>
          Model
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../detr/" class="md-nav__link">
        Detr
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../factory/" class="md-nav__link">
        Factory
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Faster Rcnn
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Faster Rcnn
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fasterrcnnfpn" class="md-nav__link">
    FasterRcnnFPN
  </a>
  
    <nav class="md-nav" aria-label="FasterRcnnFPN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arguments" class="md-nav__link">
    Arguments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply" class="md-nav__link">
    apply
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#export_for_serving" class="md-nav__link">
    export_for_serving
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_losses_for" class="md-nav__link">
    get_losses_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_updates_for" class="md-nav__link">
    get_updates_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#serving_step" class="md-nav__link">
    serving_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fasterrcnnfpnresnet50caffe" class="md-nav__link">
    FasterRcnnFPNResnet50Caffe
  </a>
  
    <nav class="md-nav" aria-label="FasterRcnnFPNResnet50Caffe">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arguments_1" class="md-nav__link">
    Arguments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss_1" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric_1" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update_1" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable_1" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight_1" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply_1" class="md-nav__link">
    apply
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call_1" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask_1" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape_1" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature_1" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params_1" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#export_for_serving_1" class="md-nav__link">
    export_for_serving
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at_1" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at_1" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at_1" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_losses_for_1" class="md-nav__link">
    get_losses_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at_1" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at_1" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at_1" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_updates_for_1" class="md-nav__link">
    get_updates_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#serving_step_1" class="md-nav__link">
    serving_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights_1" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fasterrcnnfpnresnet50pytorch" class="md-nav__link">
    FasterRcnnFPNResnet50Pytorch
  </a>
  
    <nav class="md-nav" aria-label="FasterRcnnFPNResnet50Pytorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arguments_2" class="md-nav__link">
    Arguments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss_2" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric_2" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update_2" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable_2" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight_2" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply_2" class="md-nav__link">
    apply
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call_2" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask_2" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape_2" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature_2" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params_2" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#export_for_serving_2" class="md-nav__link">
    export_for_serving
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at_2" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at_2" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at_2" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_losses_for_2" class="md-nav__link">
    get_losses_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at_2" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at_2" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at_2" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_updates_for_2" class="md-nav__link">
    get_updates_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#serving_step_2" class="md-nav__link">
    serving_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights_2" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../smca_detr/" class="md-nav__link">
        Smca Detr
      </a>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1_5_6" type="checkbox" id="__nav_4_1_5_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1_5_6">
          Backbone
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Backbone" data-md-level="4">
        <label class="md-nav__title" for="__nav_4_1_5_6">
          <span class="md-nav__icon md-icon"></span>
          Backbone
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../backbone/fpn/" class="md-nav__link">
        Fpn
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../backbone/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../backbone/resnet/" class="md-nav__link">
        Resnet
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4_1_6" type="checkbox" id="__nav_4_1_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4_1_6">
          Utils
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Utils" data-md-level="3">
        <label class="md-nav__title" for="__nav_4_1_6">
          <span class="md-nav__icon md-icon"></span>
          Utils
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/documentation/" class="md-nav__link">
        Documentation
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/drawing/" class="md-nav__link">
        Drawing
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/" class="md-nav__link">
        Index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/ops/" class="md-nav__link">
        Ops
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../utils/training/" class="md-nav__link">
        Training
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#classes" class="md-nav__link">
    Classes
  </a>
  
    <nav class="md-nav" aria-label="Classes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#fasterrcnnfpn" class="md-nav__link">
    FasterRcnnFPN
  </a>
  
    <nav class="md-nav" aria-label="FasterRcnnFPN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arguments" class="md-nav__link">
    Arguments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#descendants" class="md-nav__link">
    Descendants
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply" class="md-nav__link">
    apply
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#export_for_serving" class="md-nav__link">
    export_for_serving
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_losses_for" class="md-nav__link">
    get_losses_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_updates_for" class="md-nav__link">
    get_updates_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#serving_step" class="md-nav__link">
    serving_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fasterrcnnfpnresnet50caffe" class="md-nav__link">
    FasterRcnnFPNResnet50Caffe
  </a>
  
    <nav class="md-nav" aria-label="FasterRcnnFPNResnet50Caffe">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arguments_1" class="md-nav__link">
    Arguments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_1" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_1" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss_1" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric_1" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update_1" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable_1" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight_1" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply_1" class="md-nav__link">
    apply
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call_1" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask_1" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape_1" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature_1" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params_1" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#export_for_serving_1" class="md-nav__link">
    export_for_serving
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at_1" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at_1" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at_1" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_losses_for_1" class="md-nav__link">
    get_losses_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at_1" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at_1" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at_1" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_updates_for_1" class="md-nav__link">
    get_updates_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#serving_step_1" class="md-nav__link">
    serving_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights_1" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fasterrcnnfpnresnet50pytorch" class="md-nav__link">
    FasterRcnnFPNResnet50Pytorch
  </a>
  
    <nav class="md-nav" aria-label="FasterRcnnFPNResnet50Pytorch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#arguments_2" class="md-nav__link">
    Arguments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#ancestors-in-mro_2" class="md-nav__link">
    Ancestors (in MRO)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods_2" class="md-nav__link">
    Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_loss_2" class="md-nav__link">
    add_loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_metric_2" class="md-nav__link">
    add_metric
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_update_2" class="md-nav__link">
    add_update
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_variable_2" class="md-nav__link">
    add_variable
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#add_weight_2" class="md-nav__link">
    add_weight
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#apply_2" class="md-nav__link">
    apply
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#call_2" class="md-nav__link">
    call
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_mask_2" class="md-nav__link">
    compute_mask
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_shape_2" class="md-nav__link">
    compute_output_shape
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#compute_output_signature_2" class="md-nav__link">
    compute_output_signature
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#count_params_2" class="md-nav__link">
    count_params
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#export_for_serving_2" class="md-nav__link">
    export_for_serving
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_at_2" class="md-nav__link">
    get_input_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_mask_at_2" class="md-nav__link">
    get_input_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_input_shape_at_2" class="md-nav__link">
    get_input_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_losses_for_2" class="md-nav__link">
    get_losses_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_at_2" class="md-nav__link">
    get_output_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_mask_at_2" class="md-nav__link">
    get_output_mask_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_output_shape_at_2" class="md-nav__link">
    get_output_shape_at
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#get_updates_for_2" class="md-nav__link">
    get_updates_for
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#serving_step_2" class="md-nav__link">
    serving_step
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#set_weights_2" class="md-nav__link">
    set_weights
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/EmGarr/kerod/edit/main/reference/kerod/model/faster_rcnn.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="module-kerodmodelfaster_rcnn">Module kerod.model.faster_rcnn</h1>
<p>None</p>
<p>None</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="kn">from</span> <span class="nn">kerod.core.standard_fields</span> <span class="kn">import</span> <span class="n">BoxField</span><span class="p">,</span> <span class="n">DatasetField</span>

<span class="kn">from</span> <span class="nn">kerod.model.backbone.fpn</span> <span class="kn">import</span> <span class="n">FPN</span>

<span class="kn">from</span> <span class="nn">kerod.model.backbone.resnet</span> <span class="kn">import</span> <span class="n">ResNet50</span><span class="p">,</span> <span class="n">ResNet50PytorchStyle</span>

<span class="kn">from</span> <span class="nn">kerod.layers</span> <span class="kn">import</span> <span class="n">FastRCNN</span><span class="p">,</span> <span class="n">RegionProposalNetwork</span>

<span class="kn">from</span> <span class="nn">kerod.layers.post_processing</span> <span class="kn">import</span> <span class="p">(</span><span class="n">post_process_fast_rcnn_boxes</span><span class="p">,</span> <span class="n">post_process_rpn</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">kerod.utils.documentation</span> <span class="kn">import</span> <span class="n">remove_unwanted_doc</span>

<span class="kn">from</span> <span class="nn">kerod.utils.training</span> <span class="kn">import</span> <span class="n">apply_kernel_regularization</span>

<span class="kn">from</span> <span class="nn">tensorflow.python.keras.engine</span> <span class="kn">import</span> <span class="n">data_adapter</span>

<span class="n">__pdoc__</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">class</span> <span class="nc">FasterRcnnFPN</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">):</span>

    <span class="sd">&quot;&quot;&quot;Build a FPN Resnet 50 Faster RCNN network ready to use for training.</span>

<span class="sd">    You can use it as follow:</span>

<span class="sd">    ```python</span>

<span class="sd">    model_faster_rcnn = FasterRcnnFPNResnet50(80)</span>

<span class="sd">    base_lr = 0.1</span>

<span class="sd">    optimizer = tf.keras.optimizers.SGD(learning_rate=base_lr)</span>

<span class="sd">    model_faster_rcnn.compile(optimizer=optimizer, loss=None)</span>

<span class="sd">    model_faster_rcnn.fit(ds_train, validation_data=ds_test, epochs=11,)</span>

<span class="sd">    ```</span>

<span class="sd">    Arguments:</span>

<span class="sd">        num_classes: The number of classes of your dataset</span>

<span class="sd">            (**do not include the background class** it is handle for you)</span>

<span class="sd">        backbone: A tensorflow Model.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">backbone</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span> <span class="o">=</span> <span class="n">backbone</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fpn</span> <span class="o">=</span> <span class="n">FPN</span><span class="p">(</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span> <span class="o">=</span> <span class="n">RegionProposalNetwork</span><span class="p">(</span><span class="n">kernel_regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">fast_rcnn</span> <span class="o">=</span> <span class="n">FastRCNN</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">kernel_regularizer</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">)</span>

        <span class="c1"># See docstring self.export_for_serving for usage</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_serving</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Perform an inference in training.</span>

<span class="sd">        Arguments:</span>

<span class="sd">            inputs: A dict with the following schema:</span>

<span class="sd">                `images`: A Tensor of shape [batch_size, height, width, 3]</span>

<span class="sd">                `image_informations`: A float32 Tensor of shape [batch_size, 2] where</span>

<span class="sd">                    the last dimension represents the original height and</span>

<span class="sd">                    width of the images (without the padding).</span>

<span class="sd">                `ground_truths`: A dict</span>

<span class="sd">                    - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes],</span>

<span class="sd">                    - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)]</span>

<span class="sd">                    - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt]</span>

<span class="sd">                    - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt]</span>

<span class="sd">                    - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1]</span>

<span class="sd">                        which allows to remove the padding created by tf.Data.</span>

<span class="sd">                        Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32)</span>

<span class="sd">                        then my second box has a padding of 1</span>

<span class="sd">            training: Is automatically set to `True` in train and test mode</span>

<span class="sd">                (normally test should be at false). Why? Through the call we the losses and the metrics</span>

<span class="sd">                of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`.</span>

<span class="sd">                In test we want to benefit from those and therefore we compute them. It is an inheritance</span>

<span class="sd">                from tensorflow 2.0 and 2.1 and I&#39;ll think to move them in a more traditional way inside the</span>

<span class="sd">                train_step and test_step. However for now this method benefit of the encapsulation of</span>

<span class="sd">                the `self.compiled_loss` method.</span>

<span class="sd">        Returns:</span>

<span class="sd">            Tuple:</span>

<span class="sd">                - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes]</span>

<span class="sd">                    representing the class probability.</span>

<span class="sd">                - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)]</span>

<span class="sd">                - `anchors`: A Tensor of shape [batch_size, num_boxes, 4]</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">images</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">DatasetField</span><span class="o">.</span><span class="n">IMAGES</span><span class="p">]</span>

        <span class="n">images_information</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">DatasetField</span><span class="o">.</span><span class="n">IMAGES_INFO</span><span class="p">]</span>

        <span class="c1"># The preprocessing dedicated to the backbone is done inside the model.</span>

        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>

        <span class="n">pyramid</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fpn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

        <span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span> <span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span> <span class="n">anchors_per_lvl</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="p">(</span><span class="n">pyramid</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">training</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_serving</span><span class="p">:</span>

            <span class="n">apply_kernel_regularization</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">backbone</span><span class="p">)</span>

            <span class="c1"># add_loss stores the rpn losses computation in self.losses</span>

            <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rpn</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span> <span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span> <span class="n">anchors_per_lvl</span><span class="p">,</span>

                                      <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="p">])</span>

        <span class="n">num_boxes</span> <span class="o">=</span> <span class="mi">2000</span> <span class="k">if</span> <span class="n">training</span> <span class="k">else</span> <span class="mi">1000</span>

        <span class="n">rois</span> <span class="o">=</span> <span class="n">post_process_rpn</span><span class="p">(</span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span>

                                <span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span>

                                <span class="n">anchors_per_lvl</span><span class="p">,</span>

                                <span class="n">images_information</span><span class="p">,</span>

                                <span class="n">pre_nms_topk_per_lvl</span><span class="o">=</span><span class="n">num_boxes</span><span class="p">,</span>

                                <span class="n">post_nms_topk</span><span class="o">=</span><span class="n">num_boxes</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">training</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_serving</span><span class="p">:</span>

            <span class="n">ground_truths</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="p">]</span>

            <span class="c1"># Include the ground_truths as RoIs for the training</span>

            <span class="n">rois</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_dtype</span><span class="p">),</span> <span class="n">ground_truths</span><span class="p">[</span><span class="n">BoxField</span><span class="o">.</span><span class="n">BOXES</span><span class="p">]],</span>

                             <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Sample the boxes needed for inference</span>

            <span class="n">y_true</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">rois</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fast_rcnn</span><span class="o">.</span><span class="n">sample_boxes</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span> <span class="n">ground_truths</span><span class="p">)</span>

        <span class="n">classification_pred</span><span class="p">,</span> <span class="n">localization_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fast_rcnn</span><span class="p">([</span><span class="n">pyramid</span><span class="p">,</span> <span class="n">rois</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">training</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">_serving</span><span class="p">:</span>

            <span class="c1"># add_loss stores the fast_rcnn losses computation in self.losses</span>

            <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fast_rcnn</span><span class="o">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">classification_pred</span><span class="p">,</span> <span class="n">localization_pred</span><span class="p">)</span>

        <span class="n">classification_pred</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">classification_pred</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">classification_pred</span><span class="p">,</span> <span class="n">localization_pred</span><span class="p">,</span> <span class="n">rois</span>

    <span class="k">def</span> <span class="nf">train_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>

        <span class="c1"># These are the only transformations `Model.fit` applies to user-input</span>

        <span class="c1"># data when a `tf.data.Dataset` is provided. These utilities will be exposed</span>

        <span class="c1"># publicly.</span>

        <span class="n">data</span> <span class="o">=</span> <span class="n">data_adapter</span><span class="o">.</span><span class="n">expand_1d</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">data_adapter</span><span class="o">.</span><span class="n">unpack_x_y_sample_weight</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">GradientTape</span><span class="p">()</span> <span class="k">as</span> <span class="n">tape</span><span class="p">:</span>

            <span class="n">x</span><span class="p">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>

            <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># All the losses are computed in the call. It can seems weird but it those</span>

            <span class="c1"># the job in a clean way. They are automatically added to self.losses</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compiled_loss</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">regularization_losses</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainable_variables</span><span class="p">,</span> <span class="n">tape</span><span class="o">=</span><span class="n">tape</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="n">m</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">result</span><span class="p">()</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>

        <span class="n">data</span> <span class="o">=</span> <span class="n">data_adapter</span><span class="o">.</span><span class="n">expand_1d</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">data_adapter</span><span class="o">.</span><span class="n">unpack_x_y_sample_weight</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">x</span><span class="p">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">y</span>

        <span class="c1"># In our graph all the metrics are computed inside the call method</span>

        <span class="c1"># So we set training to True to benefit from those metrics</span>

        <span class="c1"># Of course there is no backpropagation at the test step</span>

        <span class="n">y_pred</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compiled_loss</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">regularization_losses</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">losses</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span><span class="n">m</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">m</span><span class="o">.</span><span class="n">result</span><span class="p">()</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">metrics</span><span class="p">}</span>

    <span class="k">def</span> <span class="nf">predict_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>

        <span class="n">data</span> <span class="o">=</span> <span class="n">data_adapter</span><span class="o">.</span><span class="n">expand_1d</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">x</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">data_adapter</span><span class="o">.</span><span class="n">unpack_x_y_sample_weight</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>

        <span class="n">classification_pred</span><span class="p">,</span> <span class="n">localization_pred</span><span class="p">,</span> <span class="n">rois</span> <span class="o">=</span> <span class="bp">self</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="c1"># Remove the background classes</span>

        <span class="n">classification_pred</span> <span class="o">=</span> <span class="n">classification_pred</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:]</span>

        <span class="k">return</span> <span class="n">post_process_fast_rcnn_boxes</span><span class="p">(</span><span class="n">classification_pred</span><span class="p">,</span> <span class="n">localization_pred</span><span class="p">,</span> <span class="n">rois</span><span class="p">,</span>

                                            <span class="n">x</span><span class="p">[</span><span class="n">DatasetField</span><span class="o">.</span><span class="n">IMAGES_INFO</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>

    <span class="nd">@tf</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="p">[</span>

        <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">DatasetField</span><span class="o">.</span><span class="n">IMAGES</span><span class="p">),</span>

        <span class="n">tf</span><span class="o">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">DatasetField</span><span class="o">.</span><span class="n">IMAGES_INFO</span><span class="p">)</span>

    <span class="p">])</span>

    <span class="k">def</span> <span class="nf">serving_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">images</span><span class="p">,</span> <span class="n">images_info</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Allow to bypass the save_model behavior the graph in serving mode.</span>

<span class="sd">        Currently, the issue is that in training the ground_truths are passed to the call method but</span>

<span class="sd">        not in inference. For the serving only the `images` and `images_information` are defined.</span>

<span class="sd">        It means the inputs link to the ground_truths won&#39;t be defined in serving. However, tensorflow</span>

<span class="sd">        absolutely want it and will return an exception if the ground_truth isn&#39;t provided.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict_step</span><span class="p">({</span>

            <span class="n">DatasetField</span><span class="o">.</span><span class="n">IMAGES</span><span class="p">:</span> <span class="n">images</span><span class="p">,</span>

            <span class="n">DatasetField</span><span class="o">.</span><span class="n">IMAGES_INFO</span><span class="p">:</span> <span class="n">images_info</span>

        <span class="p">})</span>

    <span class="k">def</span> <span class="nf">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>

             <span class="n">filepath</span><span class="p">,</span>

             <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

             <span class="n">include_optimizer</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>

             <span class="n">save_format</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>

             <span class="n">signatures</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>

             <span class="n">options</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

        <span class="k">try</span><span class="p">:</span>

            <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span>

                         <span class="n">overwrite</span><span class="o">=</span><span class="n">overwrite</span><span class="p">,</span>

                         <span class="n">include_optimizer</span><span class="o">=</span><span class="n">include_optimizer</span><span class="p">,</span>

                         <span class="n">save_format</span><span class="o">=</span><span class="n">save_format</span><span class="p">,</span>

                         <span class="n">signatures</span><span class="o">=</span><span class="n">signatures</span><span class="p">,</span>

                         <span class="n">options</span><span class="o">=</span><span class="n">options</span><span class="p">)</span>

        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>

            <span class="k">raise</span> <span class="ne">Exception</span><span class="p">(</span>

                <span class="s1">&#39;Saving does not work with dynamic inputs the ground_truths are injected in the inputs. &#39;</span>

                <span class="s1">&#39;Please use export_model method instead to bypass this error.&#39;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">export_for_serving</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">):</span>

        <span class="sd">&quot;&quot;&quot;Allow to bypass the save_model behavior the graph in serving mode.</span>

<span class="sd">        Currently, the issue is that in training the ground_truths are passed to the call method but</span>

<span class="sd">        not in inference. For the serving only the `images` and `images_information` are defined.</span>

<span class="sd">        It means the inputs link to the ground_truths won&#39;t be defined in serving. However, in tensorflow</span>

<span class="sd">        when the `training` arguments is defined int the method `call`, `tf.save_model.save` method</span>

<span class="sd">        performs a check on the graph for training=False and training=True.</span>

<span class="sd">        However, we don&#39;t want this check to be perform because our ground_truths inputs aren&#39;t defined.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_serving</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">call_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">serving_step</span><span class="o">.</span><span class="n">get_concrete_function</span><span class="p">()</span>

        <span class="n">tf</span><span class="o">.</span><span class="n">saved_model</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">filepath</span><span class="p">,</span> <span class="n">signatures</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;serving_default&#39;</span><span class="p">:</span> <span class="n">call_output</span><span class="p">})</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_serving</span> <span class="o">=</span> <span class="kc">False</span>

<span class="k">class</span> <span class="nc">FasterRcnnFPNResnet50Caffe</span><span class="p">(</span><span class="n">FasterRcnnFPN</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="n">resnet</span> <span class="o">=</span> <span class="n">ResNet50</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">resnet</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">FasterRcnnFPNResnet50Pytorch</span><span class="p">(</span><span class="n">FasterRcnnFPN</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="n">resnet</span> <span class="o">=</span> <span class="n">ResNet50PytorchStyle</span><span class="p">(</span><span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">weights</span><span class="o">=</span><span class="s1">&#39;imagenet&#39;</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">resnet</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

<span class="n">remove_unwanted_doc</span><span class="p">(</span><span class="n">FasterRcnnFPN</span><span class="p">,</span> <span class="n">__pdoc__</span><span class="p">)</span>

<span class="n">remove_unwanted_doc</span><span class="p">(</span><span class="n">FasterRcnnFPNResnet50Caffe</span><span class="p">,</span> <span class="n">__pdoc__</span><span class="p">)</span>

<span class="n">remove_unwanted_doc</span><span class="p">(</span><span class="n">FasterRcnnFPNResnet50Pytorch</span><span class="p">,</span> <span class="n">__pdoc__</span><span class="p">)</span>
</code></pre></div>

</details>
<h2 id="classes">Classes</h2>
<h3 id="fasterrcnnfpn">FasterRcnnFPN</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">FasterRcnnFPN</span><span class="p">(</span>
    <span class="n">num_classes</span><span class="p">,</span>
    <span class="n">backbone</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>You can use it as follow:</p>
<div class="codehilite"><pre><span></span><code><span class="n">model_faster_rcnn</span> <span class="o">=</span> <span class="n">FasterRcnnFPNResnet50</span><span class="p">(</span><span class="mi">80</span><span class="p">)</span>
<span class="n">base_lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">base_lr</span><span class="p">)</span>
<span class="n">model_faster_rcnn</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">model_faster_rcnn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">ds_test</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">11</span><span class="p">,)</span>
</code></pre></div>

<h4 id="arguments">Arguments</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>num_classes</td>
<td>The number of classes of your dataset<br>(<strong>do not include the background class</strong> it is handle for you)</td>
</tr>
<tr>
<td>backbone</td>
<td>A tensorflow Model.</td>
</tr>
</tbody>
</table>
<h4 id="ancestors-in-mro">Ancestors (in MRO)</h4>
<ul>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h4 id="descendants">Descendants</h4>
<ul>
<li>kerod.model.faster_rcnn.FasterRcnnFPNResnet50Caffe</li>
<li>kerod.model.faster_rcnn.FasterRcnnFPNResnet50Pytorch</li>
</ul>
<h4 id="methods">Methods</h4>
<h4 id="add_loss">add_loss</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">losses</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be dependent
on the inputs passed when calling a layer. Hence, when reusing the same
layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may
be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>This method can be used inside a subclassed layer or model's <code>call</code>
function, in which case <code>losses</code> should be a Tensor or list of Tensors.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MyLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div>

<p>This method can also be called directly on a Functional Model during
construction. In this case, any loss Tensors passed to this Model must
be symbolic and be able to be traced back to the model's <code>Input</code>s. These
losses become part of the model's topology and are tracked in <code>get_config</code>.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="c1"># Activity regularization.</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</code></pre></div>

<p>If this is not the case for your loss (if, for example, your loss references
a <code>Variable</code> of one of the model's layers), you can wrap your loss in a
zero-argument lambda. These losses are not tracked as part of the model's
topology since they can't be serialized.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">d</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="c1"># Weight regularization.</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">kernel</span><span class="p">))</span>
</code></pre></div>

<p>Arguments:
  losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses
    may also be zero-argument callables which create a loss tensor.
  **kwargs: Additional keyword arguments for backward compatibility.
    Accepted values:
      inputs - Deprecated, will be automatically inferred.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>  <span class="n">def</span> <span class="n">add_loss</span>(<span class="nb">self</span>, <span class="n">losses</span>, **<span class="n">kwargs</span>):

    <span class="s">&quot;&quot;&quot;Add loss tensor(s), potentially dependent on layer inputs.</span>

<span class="s">    Some losses (for instance, activity regularization losses) may be dependent</span>

<span class="s">    on the inputs passed when calling a layer. Hence, when reusing the same</span>

<span class="s">    layer on different inputs `a` and `b`, some entries in `layer.losses` may</span>

<span class="s">    be dependent on `a` and some on `b`. This method automatically keeps track</span>

<span class="s">    of dependencies.</span>

<span class="s">    This method can be used inside a subclassed layer or model&#39;s `call`</span>

<span class="s">    function, in which case `losses` should be a Tensor or list of Tensors.</span>

<span class="s">    Example:</span>

<span class="s">    ```python</span>

<span class="s">    class MyLayer(tf.keras.layers.Layer):</span>

<span class="s">      def call(self, inputs):</span>

<span class="s">        self.add_loss(tf.abs(tf.reduce_mean(inputs)))</span>

<span class="s">        return inputs</span>

<span class="s">    ```</span>

<span class="s">    This method can also be called directly on a Functional Model during</span>

<span class="s">    construction. In this case, any loss Tensors passed to this Model must</span>

<span class="s">    be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s">    losses become part of the model&#39;s topology and are tracked in `get_config`.</span>

<span class="s">    Example:</span>

<span class="s">    ```python</span>

<span class="s">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">    x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s">    # Activity regularization.</span>

<span class="s">    model.add_loss(tf.abs(tf.reduce_mean(x)))</span>

<span class="s">    ```</span>

<span class="s">    If this is not the case for your loss (if, for example, your loss references</span>

<span class="s">    a `Variable` of one of the model&#39;s layers), you can wrap your loss in a</span>

<span class="s">    zero-argument lambda. These losses are not tracked as part of the model&#39;s</span>

<span class="s">    topology since they can&#39;t be serialized.</span>

<span class="s">    Example:</span>

<span class="s">    ```python</span>

<span class="s">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">    d = tf.keras.layers.Dense(10)</span>

<span class="s">    x = d(inputs)</span>

<span class="s">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s">    # Weight regularization.</span>

<span class="s">    model.add_loss(lambda: tf.reduce_mean(d.kernel))</span>

<span class="s">    ```</span>

<span class="s">    Arguments:</span>

<span class="s">      losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses</span>

<span class="s">        may also be zero-argument callables which create a loss tensor.</span>

<span class="s">      **kwargs: Additional keyword arguments for backward compatibility.</span>

<span class="s">        Accepted values:</span>

<span class="s">          inputs - Deprecated, will be automatically inferred.</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span>.<span class="nb">pop</span>(<span class="s">&#39;inputs&#39;</span>, <span class="n">None</span>)

    <span class="k">if</span> <span class="n">kwargs:</span>

      <span class="n">raise</span> <span class="n">TypeError</span>(<span class="s">&#39;Unknown keyword arguments: %s&#39;</span> % (<span class="n">kwargs</span>.<span class="nb">keys</span>(),))

    <span class="n">def</span> <span class="n">_tag_callable</span>(<span class="n">loss</span>):

      <span class="s">&quot;&quot;&quot;Tags callable loss tensor as `_unconditional_loss`.&quot;&quot;&quot;</span>

      <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

        <span class="c1"># We run the loss without autocasting, as regularizers are often</span>

        <span class="c1"># numerically unstable in float16.</span>

        <span class="k">with</span> <span class="n">autocast_variable</span>.<span class="n">enable_auto_cast_variables</span>(<span class="n">None</span>):

          <span class="n">loss</span> = <span class="n">loss</span>()

      <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

        <span class="k">return</span> <span class="n">None</span>  <span class="c1"># Will be filtered out when computing the .losses property</span>

      <span class="k">if</span> <span class="nb">not</span> <span class="n">tensor_util</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

        <span class="n">loss</span> = <span class="n">ops</span>.<span class="n">convert_to_tensor_v2_with_dispatch</span>(

            <span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

      <span class="n">loss</span>.<span class="n">_unconditional_loss</span> = <span class="nb">True</span>  <span class="c1"># pylint: disable=protected-access</span>

      <span class="k">return</span> <span class="n">loss</span>

    <span class="n">losses</span> = <span class="n">nest</span>.<span class="n">flatten</span>(<span class="n">losses</span>)

    <span class="n">callable_losses</span> = []

    <span class="n">eager_losses</span> = []

    <span class="n">symbolic_losses</span> = []

    <span class="k">for</span> <span class="n">loss</span> <span class="nb">in</span> <span class="n">losses:</span>

      <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

        <span class="n">callable_losses</span>.<span class="nb">append</span>(<span class="n">functools</span>.<span class="n">partial</span>(<span class="n">_tag_callable</span>, <span class="n">loss</span>))

        <span class="n">continue</span>

      <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

        <span class="n">continue</span>

      <span class="k">if</span> <span class="nb">not</span> <span class="n">tensor_util</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>) <span class="o">and</span> <span class="nb">not</span> <span class="n">isinstance</span>(

          <span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>):

        <span class="n">loss</span> = <span class="n">ops</span>.<span class="n">convert_to_tensor_v2_with_dispatch</span>(

            <span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

      <span class="c1"># TF Functions should take the eager path.</span>

      <span class="k">if</span> ((<span class="n">tf_utils</span>.<span class="n">is_symbolic_tensor</span>(<span class="n">loss</span>) <span class="o">or</span>

           <span class="n">isinstance</span>(<span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>)) <span class="o">and</span>

          <span class="nb">not</span> <span class="n">base_layer_utils</span>.<span class="n">is_in_tf_function</span>()):

        <span class="n">symbolic_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

      <span class="n">elif</span> <span class="n">tensor_util</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

        <span class="n">eager_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

    <span class="nb">self</span>.<span class="n">_callable_losses</span>.<span class="n">extend</span>(<span class="n">callable_losses</span>)

    <span class="n">in_call_context</span> = <span class="n">base_layer_utils</span>.<span class="n">call_context</span>().<span class="n">in_call</span>

    <span class="k">if</span> <span class="n">eager_losses</span> <span class="o">and</span> <span class="nb">not</span> <span class="n">in_call_context:</span>

      <span class="n">raise</span> <span class="n">ValueError</span>(

          <span class="s">&#39;Expected a symbolic Tensors or a callable for the loss value. &#39;</span>

          <span class="s">&#39;Please wrap your loss computation in a zero argument `lambda`.&#39;</span>)

    <span class="nb">self</span>.<span class="n">_eager_losses</span>.<span class="n">extend</span>(<span class="n">eager_losses</span>)

    <span class="k">if</span> <span class="n">in_call_context</span> <span class="o">and</span> <span class="nb">not</span> <span class="n">keras_tensor</span>.<span class="n">keras_tensors_enabled</span>():

      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="nb">in</span> <span class="n">symbolic_losses:</span>

        <span class="nb">self</span>.<span class="n">_losses</span>.<span class="nb">append</span>(<span class="n">symbolic_loss</span>)

    <span class="n">else:</span>

      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="nb">in</span> <span class="n">symbolic_losses:</span>

        <span class="k">if</span> <span class="n">getattr</span>(<span class="nb">self</span>, <span class="s">&#39;_is_graph_network&#39;</span>, <span class="nb">False</span>):

          <span class="nb">self</span>.<span class="n">_graph_network_add_loss</span>(<span class="n">symbolic_loss</span>)

        <span class="n">else:</span>

          <span class="c1"># Possible a loss was added in a Layer&#39;s `build`.</span>

          <span class="nb">self</span>.<span class="n">_losses</span>.<span class="nb">append</span>(<span class="n">symbolic_loss</span>)
</code></pre></div>

</details>
<h4 id="add_metric">add_metric</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">value</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds metric tensor to the layer.</p>
<p>This method can be used inside the <code>call()</code> method of a subclassed layer
or model.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MyMetricLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyMetricLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;my_metric_layer&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_2&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div>

<p>This method can also be called directly on a Functional Model during
construction. In this case, any tensor passed to this Model must
be symbolic and be able to be traced back to the model's <code>Input</code>s. These
metrics become part of the model's topology and are tracked when you
save the model via <code>save()</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p>Note: Calling <code>add_metric()</code> with the result of a metric object on a
Functional Model, as shown in the example below, is not supported. This is
because we cannot trace the metric result tensor back to the model's inputs.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>value</td>
<td>Metric tensor.</td>
</tr>
<tr>
<td>name</td>
<td>String metric name.</td>
</tr>
<tr>
<td>**kwargs</td>
<td>Additional keyword arguments for backward compatibility.<br>Accepted values:<br><code>aggregation</code> - When the <code>value</code> tensor provided is not the result of<br>calling a <code>keras.Metric</code> instance, it will be aggregated by default<br>using a <code>keras.Metric.Mean</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_metric</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Adds metric tensor to the layer.</span>

<span class="s2">    This method can be used inside the `call()` method of a subclassed layer</span>

<span class="s2">    or model.</span>

<span class="s2">    ```python</span>

<span class="s2">    class MyMetricLayer(tf.keras.layers.Layer):</span>

<span class="s2">      def __init__(self):</span>

<span class="s2">        super(MyMetricLayer, self).__init__(name=&#39;my_metric_layer&#39;)</span>

<span class="s2">        self.mean = tf.keras.metrics.Mean(name=&#39;metric_1&#39;)</span>

<span class="s2">      def call(self, inputs):</span>

<span class="s2">        self.add_metric(self.mean(x))</span>

<span class="s2">        self.add_metric(tf.reduce_sum(x), name=&#39;metric_2&#39;)</span>

<span class="s2">        return inputs</span>

<span class="s2">    ```</span>

<span class="s2">    This method can also be called directly on a Functional Model during</span>

<span class="s2">    construction. In this case, any tensor passed to this Model must</span>

<span class="s2">    be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s2">    metrics become part of the model&#39;s topology and are tracked when you</span>

<span class="s2">    save the model via `save()`.</span>

<span class="s2">    ```python</span>

<span class="s2">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">    x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">    model.add_metric(math_ops.reduce_sum(x), name=&#39;metric_1&#39;)</span>

<span class="s2">    ```</span>

<span class="s2">    Note: Calling `add_metric()` with the result of a metric object on a</span>

<span class="s2">    Functional Model, as shown in the example below, is not supported. This is</span>

<span class="s2">    because we cannot trace the metric result tensor back to the model&#39;s inputs.</span>

<span class="s2">    ```python</span>

<span class="s2">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">    x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">    model.add_metric(tf.keras.metrics.Mean()(x), name=&#39;metric_1&#39;)</span>

<span class="s2">    ```</span>

<span class="s2">    Args:</span>

<span class="s2">      value: Metric tensor.</span>

<span class="s2">      name: String metric name.</span>

<span class="s2">      **kwargs: Additional keyword arguments for backward compatibility.</span>

<span class="s2">        Accepted values:</span>

<span class="s2">        `aggregation` - When the `value` tensor provided is not the result of</span>

<span class="s2">        calling a `keras.Metric` instance, it will be aggregated by default</span>

<span class="s2">        using a `keras.Metric.Mean`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">kwargs_keys</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">list</span><span class="p">(</span><span class="n">kwargs</span><span class="p">.</span><span class="k">keys</span><span class="p">())</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">or</span><span class="w"></span>

<span class="w">        </span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">kwargs_keys</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s1">&#39;aggregation&#39;</span><span class="p">))</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword arguments: &#39;</span><span class="p">,</span><span class="w"> </span><span class="n">str</span><span class="p">(</span><span class="n">kwargs</span><span class="p">.</span><span class="k">keys</span><span class="p">()))</span><span class="w"></span>

<span class="w">    </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hasattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_metric_obj&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">keras_tensor</span><span class="p">.</span><span class="n">keras_tensors_enabled</span><span class="p">()</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">is_symbolic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">keras_tensor</span><span class="p">.</span><span class="n">KerasTensor</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">is_symbolic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf_utils</span><span class="p">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="k">value</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">in_call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">().</span><span class="n">in_call</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">name</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Eg. `self.add_metric(math_ops.reduce_sum(x))`</span><span class="w"></span>

<span class="w">      </span><span class="c1"># In eager mode, we use metric name to lookup a metric. Without a name,</span><span class="w"></span>

<span class="w">      </span><span class="c1"># a new Mean metric wrapper will be created on every model/layer call.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># So, we raise an error when no name is provided.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># We will do the same for symbolic mode for consistency although a name</span><span class="w"></span>

<span class="w">      </span><span class="c1"># will be generated if no name is provided.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># We will not raise this error in the foll use case for the sake of</span><span class="w"></span>

<span class="w">      </span><span class="c1"># consistency as name in provided in the metric constructor.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># mean = metrics.Mean(name=&#39;my_metric&#39;)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># model.add_metric(mean(outputs))</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;Please provide a name for your metric like &#39;</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;`self.add_metric(tf.reduce_sum(inputs), &#39;</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;name=</span><span class="se">\&#39;</span><span class="s1">mean_activation</span><span class="se">\&#39;</span><span class="s1">)`&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">elif</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">value</span><span class="p">.</span><span class="n">_metric_obj</span><span class="p">.</span><span class="k">name</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">is_symbolic</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected a symbolic Tensor for the metric value, &#39;</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;received: &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">str</span><span class="p">(</span><span class="k">value</span><span class="p">))</span><span class="w"></span>

<span class="w">    </span><span class="c1"># If a metric was added in a Layer&#39;s `call` or `build`.</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># TF Function path should take the eager path.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If the given metric is available in `metrics` list we just update state</span><span class="w"></span>

<span class="w">      </span><span class="c1"># on it, otherwise we create a new metric instance and</span><span class="w"></span>

<span class="w">      </span><span class="c1"># add it to the `metrics` list.</span><span class="w"></span>

<span class="w">      </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_metric_obj&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Tensors that come from a Metric object already updated the Metric state.</span><span class="w"></span>

<span class="w">      </span><span class="n">should_update_state</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">metric_obj</span><span class="w"></span>

<span class="w">      </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric_obj</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">name</span><span class="w"></span>

<span class="w">      </span><span class="k">with</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics_lock</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="k">match</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_existing_metric</span><span class="p">(</span><span class="k">name</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">match</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">match</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="c1"># Build the metric object with the value&#39;s dtype if it defines one</span><span class="w"></span>

<span class="w">          </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metrics_mod</span><span class="p">.</span><span class="n">Mean</span><span class="p">(</span><span class="w"></span>

<span class="w">              </span><span class="k">name</span><span class="o">=</span><span class="k">name</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">))</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">should_update_state</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">metric_obj</span><span class="p">(</span><span class="k">value</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;Using the result of calling a `Metric` object &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;when calling `add_metric` on a Functional &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;Model is not supported. Please pass the &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;Tensor to monitor directly.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Insert layers into the Keras Graph Network.</span><span class="w"></span>

<span class="w">      </span><span class="n">aggregation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;mean&#39;</span><span class="w"></span>

<span class="w">      </span><span class="n">self</span><span class="p">.</span><span class="n">_graph_network_add_metric</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">aggregation</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="add_update">add_update</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_update</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">updates</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and variance
in a BatchNormalization layer) may be dependent on the inputs passed
when calling a layer. Hence, when reusing the same layer on
different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be
dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>This call is ignored when eager execution is enabled (in that case, variable
updates are run on the fly and thus do not need to be tracked for later
execution).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>updates</td>
<td>Update op, or list/tuple of update ops, or zero-arg callable<br>that returns an update op. A zero-arg callable should be passed in<br>order to disable running the updates by setting <code>trainable=False</code><br>on this Layer, when executing in Eager mode.</td>
</tr>
<tr>
<td>inputs</td>
<td>Deprecated, will be automatically inferred.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">updates</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Add update op(s), potentially dependent on layer inputs.</span>

<span class="s2">    Weight updates (for instance, the updates of the moving mean and variance</span>

<span class="s2">    in a BatchNormalization layer) may be dependent on the inputs passed</span>

<span class="s2">    when calling a layer. Hence, when reusing the same layer on</span>

<span class="s2">    different inputs `a` and `b`, some entries in `layer.updates` may be</span>

<span class="s2">    dependent on `a` and some on `b`. This method automatically keeps track</span>

<span class="s2">    of dependencies.</span>

<span class="s2">    This call is ignored when eager execution is enabled (in that case, variable</span>

<span class="s2">    updates are run on the fly and thus do not need to be tracked for later</span>

<span class="s2">    execution).</span>

<span class="s2">    Arguments:</span>

<span class="s2">      updates: Update op, or list/tuple of update ops, or zero-arg callable</span>

<span class="s2">        that returns an update op. A zero-arg callable should be passed in</span>

<span class="s2">        order to disable running the updates by setting `trainable=False`</span>

<span class="s2">        on this Layer, when executing in Eager mode.</span>

<span class="s2">      inputs: Deprecated, will be automatically inferred.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">tf_logging</span><span class="p">.</span><span class="n">warning</span><span class="p">(</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;`add_update` `inputs` kwarg has been deprecated. You no longer need &#39;</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;to pass a value to `inputs` as it is being automatically inferred.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="c1"># No need to run updates during Functional API construction.</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">in_keras_graph</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Callable updates are disabled by setting `trainable=False`.</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">frozen</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="k">update</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">updates</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">callable</span><span class="p">(</span><span class="k">update</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="k">update</span><span class="p">()</span><span class="w">  </span><span class="c1"># pylint: disable=not-callable</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="add_variable">add_variable</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_variable</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use! Alias for <code>add_weight</code>.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_variable</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use! Alias for `add_weight`.</span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.add_variable` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.add_weight` method instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="add_weight">add_weight</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synchronization</span><span class="o">=&lt;</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="o">=&lt;</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds a new variable to the layer.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>Variable name.</td>
</tr>
<tr>
<td>shape</td>
<td>Variable shape. Defaults to scalar if unspecified.</td>
</tr>
<tr>
<td>dtype</td>
<td>The type of the variable. Defaults to <code>self.dtype</code>.</td>
</tr>
<tr>
<td>initializer</td>
<td>Initializer instance (callable).</td>
</tr>
<tr>
<td>regularizer</td>
<td>Regularizer instance (callable).</td>
</tr>
<tr>
<td>trainable</td>
<td>Boolean, whether the variable should be part of the layer's<br>"trainable_variables" (e.g. variables, biases)<br>or "non_trainable_variables" (e.g. BatchNorm mean and variance).<br>Note that <code>trainable</code> cannot be <code>True</code> if <code>synchronization</code><br>is set to <code>ON_READ</code>.</td>
</tr>
<tr>
<td>constraint</td>
<td>Constraint instance (callable).</td>
</tr>
<tr>
<td>use_resource</td>
<td>Whether to use <code>ResourceVariable</code>.</td>
</tr>
<tr>
<td>synchronization</td>
<td>Indicates when a distributed a variable will be<br>aggregated. Accepted values are constants defined in the class<br><code>tf.VariableSynchronization</code>. By default the synchronization is set to<br><code>AUTO</code> and the current <code>DistributionStrategy</code> chooses<br>when to synchronize. If <code>synchronization</code> is set to <code>ON_READ</code>,<br><code>trainable</code> must not be set to <code>True</code>.</td>
</tr>
<tr>
<td>aggregation</td>
<td>Indicates how a distributed variable will be aggregated.<br>Accepted values are constants defined in the class<br><code>tf.VariableAggregation</code>.</td>
</tr>
<tr>
<td>**kwargs</td>
<td>Additional keyword arguments. Accepted values are <code>getter</code>,<br><code>collections</code>, <code>experimental_autocast</code> and <code>caching_device</code>.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>The variable created.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>When giving unsupported dtype and no initializer or when<br>trainable has been set to True with synchronization set as <code>ON_READ</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.for_subclass_implementers</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_weight</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="k">name</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">shape</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">dtype</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">initializer</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">regularizer</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">trainable</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="k">constraint</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">use_resource</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">synchronization</span><span class="o">=</span><span class="n">tf_variables</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">AUTO</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">aggregation</span><span class="o">=</span><span class="n">tf_variables</span><span class="p">.</span><span class="n">VariableAggregation</span><span class="p">.</span><span class="k">NONE</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Adds a new variable to the layer.</span>

<span class="s2">    Arguments:</span>

<span class="s2">      name: Variable name.</span>

<span class="s2">      shape: Variable shape. Defaults to scalar if unspecified.</span>

<span class="s2">      dtype: The type of the variable. Defaults to `self.dtype`.</span>

<span class="s2">      initializer: Initializer instance (callable).</span>

<span class="s2">      regularizer: Regularizer instance (callable).</span>

<span class="s2">      trainable: Boolean, whether the variable should be part of the layer&#39;s</span>

<span class="s2">        &quot;</span><span class="n">trainable_variables</span><span class="s2">&quot; (e.g. variables, biases)</span>

<span class="s2">        or &quot;</span><span class="n">non_trainable_variables</span><span class="s2">&quot; (e.g. BatchNorm mean and variance).</span>

<span class="s2">        Note that `trainable` cannot be `True` if `synchronization`</span>

<span class="s2">        is set to `ON_READ`.</span>

<span class="s2">      constraint: Constraint instance (callable).</span>

<span class="s2">      use_resource: Whether to use `ResourceVariable`.</span>

<span class="s2">      synchronization: Indicates when a distributed a variable will be</span>

<span class="s2">        aggregated. Accepted values are constants defined in the class</span>

<span class="s2">        `tf.VariableSynchronization`. By default the synchronization is set to</span>

<span class="s2">        `AUTO` and the current `DistributionStrategy` chooses</span>

<span class="s2">        when to synchronize. If `synchronization` is set to `ON_READ`,</span>

<span class="s2">        `trainable` must not be set to `True`.</span>

<span class="s2">      aggregation: Indicates how a distributed variable will be aggregated.</span>

<span class="s2">        Accepted values are constants defined in the class</span>

<span class="s2">        `tf.VariableAggregation`.</span>

<span class="s2">      **kwargs: Additional keyword arguments. Accepted values are `getter`,</span>

<span class="s2">        `collections`, `experimental_autocast` and `caching_device`.</span>

<span class="s2">    Returns:</span>

<span class="s2">      The variable created.</span>

<span class="s2">    Raises:</span>

<span class="s2">      ValueError: When giving unsupported dtype and no initializer or when</span>

<span class="s2">        trainable has been set to True with synchronization set as `ON_READ`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;partitioner&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w">  </span><span class="c1"># Ignored.</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Validate optional keyword arguments.</span><span class="w"></span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">kwargs</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">[</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;experimental_autocast&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;caching_device&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;getter&#39;</span><span class="err">]</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword argument:&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">kwarg</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">collections_arg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="c1"># &#39;experimental_autocast&#39; can be set to False by the caller to indicate an</span><span class="w"></span>

<span class="w">    </span><span class="c1"># AutoCastVariable should never be created.</span><span class="w"></span>

<span class="w">    </span><span class="n">autocast</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;experimental_autocast&#39;</span><span class="p">,</span><span class="w"> </span><span class="no">True</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="c1"># See the docstring for tf.Variable about the details for caching_device.</span><span class="w"></span>

<span class="w">    </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;caching_device&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">backend</span><span class="p">.</span><span class="n">floatx</span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dtypes</span><span class="p">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># The policy is &quot;_infer&quot;, so we infer the policy from the variable dtype.</span><span class="w"></span>

<span class="w">      </span><span class="n">self</span><span class="p">.</span><span class="n">_set_dtype_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">Policy</span><span class="p">(</span><span class="n">dtype</span><span class="p">.</span><span class="n">base_dtype</span><span class="p">.</span><span class="k">name</span><span class="p">))</span><span class="w"></span>

<span class="w">    </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">regularizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">regularizers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="n">regularizer</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">constraint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">constraints</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="k">constraint</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">synchronization</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">tf_variables</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">ON_READ</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;Synchronization value can be set to &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;VariableSynchronization.ON_READ only for non-trainable variables. &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;You have specified trainable=True and &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;synchronization=VariableSynchronization.ON_READ.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="c1"># Set trainable to be false when variable is to be synced on read.</span><span class="w"></span>

<span class="w">        </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">False</span><span class="w"></span>

<span class="w">    </span><span class="n">elif</span><span class="w"> </span><span class="n">trainable</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">True</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Initialize variable when no initializer provided</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">initializer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If dtype is DT_FLOAT, provide a uniform unit scaling initializer</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If dtype is DT_INT/DT_UINT, provide a default value `zero`</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If dtype is DT_BOOL, provide a default value `FALSE`</span><span class="w"></span>

<span class="w">      </span><span class="n">elif</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_integer</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_unsigned</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_bool</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;An initializer for variable %s of type %s is required&#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39; for layer %s&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="k">name</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">base_dtype</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">name</span><span class="p">))</span><span class="w"></span>

<span class="w">    </span><span class="n">getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;getter&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">make_variable</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">autocast</span><span class="w"> </span><span class="k">and</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">compute_dtype</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span><span class="w"></span>

<span class="w">        </span><span class="k">and</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">old_getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getter</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Wrap variable constructor to return an AutoCastVariable.</span><span class="w"></span>

<span class="w">      </span><span class="n">def</span><span class="w"> </span><span class="n">getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w">  </span><span class="c1"># pylint: disable=function-redefined</span><span class="w"></span>

<span class="w">        </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">autocast_variable</span><span class="p">.</span><span class="n">create_autocast_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Also the caching_device does not work with the mixed precision API,</span><span class="w"></span>

<span class="w">      </span><span class="c1"># disable it if it is specified.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># TODO(b/142020079): Reenable it once the bug is fixed.</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">caching_device</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">tf_logging</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`caching_device` does not work with mixed precision &#39;</span><span class="w"></span>

<span class="w">                        </span><span class="s1">&#39;API. Ignoring user specified `caching_device`.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="w"></span>

<span class="w">    </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_add_variable_with_custom_getter</span><span class="p">(</span><span class="w"></span>

<span class="w">        </span><span class="k">name</span><span class="o">=</span><span class="k">name</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="c1"># TODO(allenl): a `make_variable` equivalent should be added as a</span><span class="w"></span>

<span class="w">        </span><span class="c1"># `Trackable` method.</span><span class="w"></span>

<span class="w">        </span><span class="n">getter</span><span class="o">=</span><span class="n">getter</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="c1"># Manage errors in Layer rather than Trackable.</span><span class="w"></span>

<span class="w">        </span><span class="n">overwrite</span><span class="o">=</span><span class="no">True</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="k">constraint</span><span class="o">=</span><span class="k">constraint</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">collections</span><span class="o">=</span><span class="n">collections_arg</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">regularizer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># TODO(fchollet): in the future, this should be handled at the</span><span class="w"></span>

<span class="w">      </span><span class="c1"># level of variable creation, and weight regularization losses</span><span class="w"></span>

<span class="w">      </span><span class="c1"># should be variable attributes.</span><span class="w"></span>

<span class="w">      </span><span class="n">name_in_scope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="k">name</span><span class="err">[</span><span class="o">:</span><span class="n">variable</span><span class="p">.</span><span class="k">name</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span><span class="err">]</span><span class="w"></span>

<span class="w">      </span><span class="n">self</span><span class="p">.</span><span class="n">_handle_weight_regularization</span><span class="p">(</span><span class="n">name_in_scope</span><span class="p">,</span><span class="w"></span>

<span class="w">                                         </span><span class="n">variable</span><span class="p">,</span><span class="w"></span>

<span class="w">                                         </span><span class="n">regularizer</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">is_split_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">variable</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">variable</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="apply">apply</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use!</p>
<p>This is an alias of <code>self.__call__</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Input tensor(s).</td>
</tr>
<tr>
<td>*args</td>
<td>additional positional arguments to be passed to <code>self.call</code>.</td>
</tr>
<tr>
<td>**kwargs</td>
<td>additional keyword arguments to be passed to <code>self.call</code>.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Output tensor(s).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Deprecated, do NOT use!</span>

<span class="ss">    This is an alias of `self.__call__`.</span>

<span class="ss">    Arguments:</span>

<span class="ss">      inputs: Input tensor(s).</span>

<span class="ss">      *args: additional positional arguments to be passed to `self.call`.</span>

<span class="ss">      **kwargs: additional keyword arguments to be passed to `self.call`.</span>

<span class="ss">    Returns:</span>

<span class="ss">      Output tensor(s).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.apply` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.__call__` method instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">__call__</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="call">call</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">training</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Perform an inference in training.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>A dict with the following schema:<br><code>images</code>: A Tensor of shape [batch_size, height, width, 3]<br><code>image_informations</code>: A float32 Tensor of shape [batch_size, 2] where<br>    the last dimension represents the original height and<br>    width of the images (without the padding).<br><br><code>ground_truths</code>: A dict<br>    - <code>BoxField.LABELS</code>: A 3-D tensor of shape [batch_size, num_gt, num_classes],<br>    - <code>BoxField.BOXES</code>: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)]<br>    - <code>BoxField.LABELS</code>: A 3-D tensor of int32 and shape [batch_size, num_gt]<br>    - <code>BoxField.WEIGHTS</code>: A 3-D tensor of float and shape [batch_size, num_gt]<br>    - <code>BoxField.NUM_BOXES</code>: A 2-D tensor of int32 and shape [batch_size, 1]<br>        which allows to remove the padding created by tf.Data.<br>        Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32)<br>        then my second box has a padding of 1</td>
</tr>
<tr>
<td>training</td>
<td>Is automatically set to <code>True</code> in train and test mode<br>(normally test should be at false). Why? Through the call we the losses and the metrics<br>of the rpn and fast_rcnn. They are automatically added with <code>add_loss</code> and <code>add_metrics</code>.<br>In test we want to benefit from those and therefore we compute them. It is an inheritance<br>from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the<br>train_step and test_step. However for now this method benefit of the encapsulation of<br>the <code>self.compiled_loss</code> method.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tuple</td>
<td>- <code>classification_pred</code>: A Tensor of shape [batch_size, num_boxes, num_classes]<br>    representing the class probability.<br>- <code>localization_pred</code>: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)]<br>- <code>anchors</code>: A Tensor of shape [batch_size, num_boxes, 4]</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">training</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Perform an inference in training.</span>

<span class="s2">        Arguments:</span>

<span class="s2">            inputs: A dict with the following schema:</span>

<span class="s2">                `images`: A Tensor of shape [batch_size, height, width, 3]</span>

<span class="s2">                `image_informations`: A float32 Tensor of shape [batch_size, 2] where</span>

<span class="s2">                    the last dimension represents the original height and</span>

<span class="s2">                    width of the images (without the padding).</span>

<span class="s2">                `ground_truths`: A dict</span>

<span class="s2">                    - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes],</span>

<span class="s2">                    - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)]</span>

<span class="s2">                    - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt]</span>

<span class="s2">                    - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt]</span>

<span class="s2">                    - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1]</span>

<span class="s2">                        which allows to remove the padding created by tf.Data.</span>

<span class="s2">                        Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32)</span>

<span class="s2">                        then my second box has a padding of 1</span>

<span class="s2">            training: Is automatically set to `True` in train and test mode</span>

<span class="s2">                (normally test should be at false). Why? Through the call we the losses and the metrics</span>

<span class="s2">                of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`.</span>

<span class="s2">                In test we want to benefit from those and therefore we compute them. It is an inheritance</span>

<span class="s2">                from tensorflow 2.0 and 2.1 and I&#39;ll think to move them in a more traditional way inside the</span>

<span class="s2">                train_step and test_step. However for now this method benefit of the encapsulation of</span>

<span class="s2">                the `self.compiled_loss` method.</span>

<span class="s2">        Returns:</span>

<span class="s2">            Tuple:</span>

<span class="s2">                - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes]</span>

<span class="s2">                    representing the class probability.</span>

<span class="s2">                - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)]</span>

<span class="s2">                - `anchors`: A Tensor of shape [batch_size, num_boxes, 4]</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="err">[</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES</span><span class="err">]</span><span class="w"></span>

<span class="w">        </span><span class="n">images_information</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="err">[</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES_INFO</span><span class="err">]</span><span class="w"></span>

<span class="w">        </span><span class="c1"># The preprocessing dedicated to the backbone is done inside the model.</span><span class="w"></span>

<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">images</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">pyramid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fpn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">anchors_per_lvl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">rpn</span><span class="p">(</span><span class="n">pyramid</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="o">:</span><span class="w"></span>

<span class="w">            </span><span class="n">apply_kernel_regularization</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">l2</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="c1"># add_loss stores the rpn losses computation in self.losses</span><span class="w"></span>

<span class="w">            </span><span class="n">_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">rpn</span><span class="p">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">anchors_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                      </span><span class="n">inputs</span><span class="err">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="err">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">num_boxes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2000</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="mi">1000</span><span class="w"></span>

<span class="w">        </span><span class="n">rois</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">post_process_rpn</span><span class="p">(</span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">anchors_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">images_information</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">pre_nms_topk_per_lvl</span><span class="o">=</span><span class="n">num_boxes</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">post_nms_topk</span><span class="o">=</span><span class="n">num_boxes</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="o">:</span><span class="w"></span>

<span class="w">            </span><span class="n">ground_truths</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="err">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="err">]</span><span class="w"></span>

<span class="w">            </span><span class="c1"># Include the ground_truths as RoIs for the training</span><span class="w"></span>

<span class="w">            </span><span class="n">rois</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="nf">concat</span><span class="p">(</span><span class="err">[</span><span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_compute_dtype</span><span class="p">),</span><span class="w"> </span><span class="n">ground_truths</span><span class="err">[</span><span class="n">BoxField</span><span class="p">.</span><span class="n">BOXES</span><span class="err">]]</span><span class="p">,</span><span class="w"></span>

<span class="w">                             </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="c1"># Sample the boxes needed for inference</span><span class="w"></span>

<span class="w">            </span><span class="n">y_true</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">rois</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fast_rcnn</span><span class="p">.</span><span class="n">sample_boxes</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span><span class="w"> </span><span class="n">ground_truths</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">classification_pred</span><span class="p">,</span><span class="w"> </span><span class="n">localization_pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fast_rcnn</span><span class="p">(</span><span class="err">[</span><span class="n">pyramid</span><span class="p">,</span><span class="w"> </span><span class="n">rois</span><span class="err">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="o">:</span><span class="w"></span>

<span class="w">            </span><span class="c1"># add_loss stores the fast_rcnn losses computation in self.losses</span><span class="w"></span>

<span class="w">            </span><span class="n">_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fast_rcnn</span><span class="p">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">classification_pred</span><span class="p">,</span><span class="w"> </span><span class="n">localization_pred</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">classification_pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">classification_pred</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">classification_pred</span><span class="p">,</span><span class="w"> </span><span class="n">localization_pred</span><span class="p">,</span><span class="w"> </span><span class="n">rois</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="compute_mask">compute_mask</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes an output mask tensor.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Tensor or list of tensors.</td>
</tr>
<tr>
<td>mask</td>
<td>Tensor or list of tensors.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None or a tensor (or list of tensors,<br>one per output tensor of the layer).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@generic_utils</span><span class="p">.</span><span class="k">default</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">compute_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">mask</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="err">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="nl">pylint</span><span class="p">:</span><span class="w"> </span><span class="n">disable</span><span class="o">=</span><span class="n">unused</span><span class="o">-</span><span class="n">argument</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Computes an output mask tensor.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        inputs: Tensor or list of tensors.</span>

<span class="ss">        mask: Tensor or list of tensors.</span>

<span class="ss">    Returns:</span>

<span class="ss">        None or a tensor (or list of tensors,</span>

<span class="ss">            one per output tensor of the layer).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">_supports_masking</span><span class="p">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="p">(</span><span class="n">m</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s1">&#39; does not support masking, &#39;</span><span class="w"></span>

<span class="w">                        </span><span class="s1">&#39;but was passed an input_mask: &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">str</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="w"></span>

<span class="w">      </span><span class="err">#</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="nl">supported</span><span class="p">:</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">mask</span><span class="p">.</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="k">None</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="n">supported</span><span class="p">,</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="k">default</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="k">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="n">mask</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">mask</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="compute_output_shape">compute_output_shape</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_shape</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes the output shape of the layer.</p>
<p>If the layer has not been built, this method will call <code>build</code> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_shape</td>
<td>Shape tuple (tuple of integers)<br>or list of shape tuples (one per output tensor of the layer).<br>Shape tuples can include None for free dimensions,<br>instead of an integer.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>An input shape tuple.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>  <span class="n">def</span> <span class="n">compute_output_shape</span>(<span class="nb">self</span>, <span class="n">input_shape</span>):

    <span class="s">&quot;&quot;&quot;Computes the output shape of the layer.</span>

<span class="s">    If the layer has not been built, this method will call `build` on the</span>

<span class="s">    layer. This assumes that the layer will later be used with inputs that</span>

<span class="s">    match the input shape provided here.</span>

<span class="s">    Arguments:</span>

<span class="s">        input_shape: Shape tuple (tuple of integers)</span>

<span class="s">            or list of shape tuples (one per output tensor of the layer).</span>

<span class="s">            Shape tuples can include None for free dimensions,</span>

<span class="s">            instead of an integer.</span>

<span class="s">    Returns:</span>

<span class="s">        An input shape tuple.</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">context</span>.<span class="n">executing_eagerly</span>():

      <span class="c1"># In this case we build the model first in order to do shape inference.</span>

      <span class="c1"># This is acceptable because the framework only calls</span>

      <span class="c1"># `compute_output_shape` on shape values that the layer would later be</span>

      <span class="c1"># built for. It would however cause issues in case a user attempts to</span>

      <span class="c1"># use `compute_output_shape` manually with shapes that are incompatible</span>

      <span class="c1"># with the shape the Layer will be called on (these users will have to</span>

      <span class="c1"># implement `compute_output_shape` themselves).</span>

      <span class="nb">self</span>.<span class="n">_maybe_build</span>(<span class="n">input_shape</span>)

      <span class="k">with</span> <span class="n">func_graph</span>.<span class="n">FuncGraph</span>(<span class="n">str</span>(<span class="nb">self</span>.<span class="nb">name</span>) + <span class="s">&#39;_scratch_graph&#39;</span>).<span class="n">as_default</span>():

        <span class="n">input_shape</span> = <span class="n">tf_utils</span>.<span class="n">convert_shapes</span>(<span class="n">input_shape</span>, <span class="n">to_tuples</span>=<span class="nb">False</span>)

        <span class="n">def</span> <span class="n">_make_placeholder_like</span>(<span class="nb">shape</span>):

          <span class="n">ph</span> = <span class="n">backend</span>.<span class="nb">placeholder</span>(<span class="nb">shape</span>=<span class="nb">shape</span>, <span class="n">dtype</span>=<span class="nb">self</span>.<span class="n">dtype</span>)

          <span class="n">ph</span>.<span class="n">_keras_mask</span> = <span class="n">None</span>

          <span class="k">return</span> <span class="n">ph</span>

        <span class="n">inputs</span> = <span class="n">nest</span>.<span class="n">map_structure</span>(<span class="n">_make_placeholder_like</span>, <span class="n">input_shape</span>)

        <span class="n">try:</span>

          <span class="n">outputs</span> = <span class="nb">self</span>(<span class="n">inputs</span>, <span class="n">training</span>=<span class="nb">False</span>)

        <span class="n">except</span> <span class="n">TypeError</span> <span class="n">as</span> <span class="n">e:</span>

          <span class="n">six</span>.<span class="n">raise_from</span>(

              <span class="n">NotImplementedError</span>(

                  <span class="s">&#39;We could not automatically infer the static shape of the &#39;</span>

                  <span class="s">&#39;layer\&#39;s output. Please implement the &#39;</span>

                  <span class="s">&#39;`compute_output_shape` method on your layer (%s).&#39;</span> %

                  <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>), <span class="nb">e</span>)

      <span class="k">return</span> <span class="n">nest</span>.<span class="n">map_structure</span>(<span class="n">lambda</span> <span class="n">t:</span> <span class="nb">t</span>.<span class="nb">shape</span>, <span class="n">outputs</span>)

    <span class="n">raise</span> <span class="n">NotImplementedError</span>(

        <span class="s">&#39;Please run in eager mode or implement the `compute_output_shape` &#39;</span>

        <span class="s">&#39;method on your layer (%s).&#39;</span> % <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>)
</code></pre></div>

</details>
<h4 id="compute_output_signature">compute_output_signature</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_signature</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_signature</span>
<span class="p">)</span>
</code></pre></div>

<p>Compute the output tensor signature of the layer based on the inputs.</p>
<p>Unlike a TensorShape object, a TensorSpec object contains both shape
and dtype information for a tensor. This method allows layers to provide
output dtype information if it is different from the input dtype.
For any layer that doesn't implement this function,
the framework will fall back to use <code>compute_output_shape</code>, and will
assume that the output dtype matches the input dtype.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_signature</td>
<td>Single TensorSpec or nested structure of TensorSpec<br>objects, describing a candidate input for the layer.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Single TensorSpec or nested structure of TensorSpec objects, describing<br>how the layer would transform the provided input.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>TypeError</td>
<td>If input_signature contains a non-TensorSpec object.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.for_subclass_implementers</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">compute_output_signature</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Compute the output tensor signature of the layer based on the inputs.</span>

<span class="s2">    Unlike a TensorShape object, a TensorSpec object contains both shape</span>

<span class="s2">    and dtype information for a tensor. This method allows layers to provide</span>

<span class="s2">    output dtype information if it is different from the input dtype.</span>

<span class="s2">    For any layer that doesn&#39;t implement this function,</span>

<span class="s2">    the framework will fall back to use `compute_output_shape`, and will</span>

<span class="s2">    assume that the output dtype matches the input dtype.</span>

<span class="s2">    Args:</span>

<span class="s2">      input_signature: Single TensorSpec or nested structure of TensorSpec</span>

<span class="s2">        objects, describing a candidate input for the layer.</span>

<span class="s2">    Returns:</span>

<span class="s2">      Single TensorSpec or nested structure of TensorSpec objects, describing</span>

<span class="s2">        how the layer would transform the provided input.</span>

<span class="s2">    Raises:</span>

<span class="s2">      TypeError: If input_signature contains a non-TensorSpec object.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">check_type_return_shape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_spec</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;Only TensorSpec signature types are supported, &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;but saw signature signature entry: {}.&#39;</span><span class="p">.</span><span class="k">format</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">shape</span><span class="w"></span>

<span class="w">    </span><span class="n">input_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">check_type_return_shape</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">output_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_compute_dtype</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">input_dtypes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[</span><span class="n">s</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_signature</span><span class="p">)</span><span class="err">]</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Default behavior when self.dtype is None, is to use the first input&#39;s</span><span class="w"></span>

<span class="w">      </span><span class="c1"># dtype.</span><span class="w"></span>

<span class="w">      </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_dtypes</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span><span class="w"></span>

<span class="w">        </span><span class="n">lambda</span><span class="w"> </span><span class="n">s</span><span class="o">:</span><span class="w"> </span><span class="n">tensor_spec</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="o">=</span><span class="n">s</span><span class="p">),</span><span class="w"></span>

<span class="w">        </span><span class="n">output_shape</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="count_params">count_params</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Count the total number of scalars composing the weights.</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>An integer count.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>if the layer isn't yet built<br>(in which case its weights aren't yet defined).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Count the total number of scalars composing the weights.</span>

<span class="s2">    Returns:</span>

<span class="s2">        An integer count.</span>

<span class="s2">    Raises:</span>

<span class="s2">        ValueError: if the layer isn&#39;t yet built</span>

<span class="s2">          (in which case its weights aren&#39;t yet defined).</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">built</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="n">tf_utils</span><span class="p">.</span><span class="n">maybe_init_scope</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inputs</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;You tried to call `count_params` on &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="o">+</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;, but the layer isn</span><span class="se">\&#39;</span><span class="s1">t built. &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;You can build it manually via: `&#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="o">+</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;.build(batch_input_shape)`.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">layer_utils</span><span class="p">.</span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="export_for_serving">export_for_serving</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">export_for_serving</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">filepath</span>
<span class="p">)</span>
</code></pre></div>

<p>Allow to bypass the save_model behavior the graph in serving mode.</p>
<p>Currently, the issue is that in training the ground_truths are passed to the call method but
not in inference. For the serving only the <code>images</code> and <code>images_information</code> are defined.
It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow
when the <code>training</code> arguments is defined int the method <code>call</code>, <code>tf.save_model.save</code> method
performs a check on the graph for training=False and training=True.
However, we don't want this check to be perform because our ground_truths inputs aren't defined.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">export_for_serving</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">filepath</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Allow to bypass the save_model behavior the graph in serving mode.</span>

<span class="s2">        Currently, the issue is that in training the ground_truths are passed to the call method but</span>

<span class="s2">        not in inference. For the serving only the `images` and `images_information` are defined.</span>

<span class="s2">        It means the inputs link to the ground_truths won&#39;t be defined in serving. However, in tensorflow</span>

<span class="s2">        when the `training` arguments is defined int the method `call`, `tf.save_model.save` method</span>

<span class="s2">        performs a check on the graph for training=False and training=True.</span>

<span class="s2">        However, we don&#39;t want this check to be perform because our ground_truths inputs aren&#39;t defined.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">True</span><span class="w"></span>

<span class="w">        </span><span class="n">call_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">serving_step</span><span class="p">.</span><span class="n">get_concrete_function</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">tf</span><span class="p">.</span><span class="n">saved_model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">filepath</span><span class="p">,</span><span class="w"> </span><span class="n">signatures</span><span class="o">=</span><span class="err">{</span><span class="s1">&#39;serving_default&#39;</span><span class="o">:</span><span class="w"> </span><span class="n">call_output</span><span class="err">}</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">False</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_input_at">get_input_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A tensor (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;input_tensors&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;input&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_input_mask_at">get_input_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A mask tensor</span>

<span class="ss">        (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_input_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &#39;_keras_mask&#39;, None) for x in inputs</span><span class="o">]</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_input_shape_at">get_input_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the input shape(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A shape tuple</span>

<span class="ss">        (or list of shape tuples if the layer has multiple inputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;input_shapes&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;input shape&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_losses_for">get_losses_for</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_losses_for</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use!</p>
<p>Retrieves losses relevant to a specific set of inputs.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Input tensor or list/tuple of input tensors.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>List of loss tensors of the layer that depend on <code>inputs</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_generate_docs</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_losses_for</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use!</span>

<span class="s2">    Retrieves losses relevant to a specific set of inputs.</span>

<span class="s2">    Arguments:</span>

<span class="s2">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="s2">    Returns:</span>

<span class="s2">      List of loss tensors of the layer that depend on `inputs`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.get_losses_for` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.losses` instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">losses</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_output_at">get_output_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A tensor (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;output_tensors&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;output&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_output_mask_at">get_output_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A mask tensor</span>

<span class="ss">        (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_output_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &#39;_keras_mask&#39;, None) for x in output</span><span class="o">]</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_output_shape_at">get_output_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the output shape(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A shape tuple</span>

<span class="ss">        (or list of shape tuples if the layer has multiple outputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;output_shapes&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;output shape&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_updates_for">get_updates_for</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_updates_for</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use!</p>
<p>Retrieves updates relevant to a specific set of inputs.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Input tensor or list/tuple of input tensors.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>List of update ops of the layer that depend on <code>inputs</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_generate_docs</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_updates_for</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use!</span>

<span class="s2">    Retrieves updates relevant to a specific set of inputs.</span>

<span class="s2">    Arguments:</span>

<span class="s2">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="s2">    Returns:</span>

<span class="s2">      List of update ops of the layer that depend on `inputs`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.get_updates_for` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.updates` method instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">updates</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="serving_step">serving_step</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">serving_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">images</span><span class="p">,</span>
    <span class="n">images_info</span>
<span class="p">)</span>
</code></pre></div>

<p>Allow to bypass the save_model behavior the graph in serving mode.</p>
<p>Currently, the issue is that in training the ground_truths are passed to the call method but
not in inference. For the serving only the <code>images</code> and <code>images_information</code> are defined.
It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow
absolutely want it and will return an exception if the ground_truth isn't provided.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@tf.function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="err">[</span><span class="w"></span>

<span class="w">        </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES</span><span class="p">),</span><span class="w"></span>

<span class="w">        </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES_INFO</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="err">]</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">serving_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">images</span><span class="p">,</span><span class="w"> </span><span class="n">images_info</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Allow to bypass the save_model behavior the graph in serving mode.</span>

<span class="s2">        Currently, the issue is that in training the ground_truths are passed to the call method but</span>

<span class="s2">        not in inference. For the serving only the `images` and `images_information` are defined.</span>

<span class="s2">        It means the inputs link to the ground_truths won&#39;t be defined in serving. However, tensorflow</span>

<span class="s2">        absolutely want it and will return an exception if the ground_truth isn&#39;t provided.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">predict_step</span><span class="p">(</span><span class="err">{</span><span class="w"></span>

<span class="w">            </span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES</span><span class="o">:</span><span class="w"> </span><span class="n">images</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES_INFO</span><span class="o">:</span><span class="w"> </span><span class="n">images_info</span><span class="w"></span>

<span class="w">        </span><span class="err">}</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="set_weights">set_weights</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">weights</span>
<span class="p">)</span>
</code></pre></div>

<p>Sets the weights of the layer, from Numpy arrays.</p>
<p>The weights of a layer represent the state of the layer. This function
sets the weight values from numpy arrays. The weight values should be
passed in the order they are created by the layer. Note that the layer's
weights must be instantiated before calling this function by calling
the layer.</p>
<p>For example, a Dense layer returns a list of two values-- per-output
weights and the bias value. These can be used to set the weights of another
Dense layer:</p>
<blockquote>
<blockquote>
<blockquote>
<p>a = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(1.))
a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))
a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
b = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(2.))
b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))
b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
b.set_weights(a.get_weights())
b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>weights</td>
<td>a list of Numpy arrays. The number<br>of arrays and their shape must match<br>number of the dimensions of the weights<br>of the layer (i.e. it should match the<br>output of <code>get_weights</code>).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>If the provided weights list does not match the<br>layer's specifications.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">):</span><span class="w"></span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets the weights of the layer, from Numpy arrays.</span>

<span class="sd">    The weights of a layer represent the state of the layer. This function</span>

<span class="sd">    sets the weight values from numpy arrays. The weight values should be</span>

<span class="sd">    passed in the order they are created by the layer. Note that the layer&#39;s</span>

<span class="sd">    weights must be instantiated before calling this function by calling</span>

<span class="sd">    the layer.</span>

<span class="sd">    For example, a Dense layer returns a list of two values-- per-output</span>

<span class="sd">    weights and the bias value. These can be used to set the weights of another</span>

<span class="sd">    Dense layer:</span>

<span class="sd">    &gt;&gt;&gt; a = tf.keras.layers.Dense(1,</span>

<span class="sd">    ...   kernel_initializer=tf.constant_initializer(1.))</span>

<span class="sd">    &gt;&gt;&gt; a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))</span>

<span class="sd">    &gt;&gt;&gt; a.get_weights()</span>

<span class="sd">    [array([[1.],</span>

<span class="sd">           [1.],</span>

<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    &gt;&gt;&gt; b = tf.keras.layers.Dense(1,</span>

<span class="sd">    ...   kernel_initializer=tf.constant_initializer(2.))</span>

<span class="sd">    &gt;&gt;&gt; b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))</span>

<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>

<span class="sd">    [array([[2.],</span>

<span class="sd">           [2.],</span>

<span class="sd">           [2.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    &gt;&gt;&gt; b.set_weights(a.get_weights())</span>

<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>

<span class="sd">    [array([[1.],</span>

<span class="sd">           [1.],</span>

<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    Arguments:</span>

<span class="sd">        weights: a list of Numpy arrays. The number</span>

<span class="sd">            of arrays and their shape must match</span>

<span class="sd">            number of the dimensions of the weights</span>

<span class="sd">            of the layer (i.e. it should match the</span>

<span class="sd">            output of `get_weights`).</span>

<span class="sd">    Raises:</span>

<span class="sd">        ValueError: If the provided weights list does not match the</span>

<span class="sd">            layer&#39;s specifications.</span>

<span class="sd">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="w"></span>

<span class="w">    </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"></span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">params</span><span class="p">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span><span class="w"></span>

<span class="w">        </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">param</span><span class="o">.</span><span class="n">num_tensors</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="p">:</span><span class="w"></span>

<span class="w">        </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;You called `set_weights(weights)` on layer &quot;</span><span class="si">%s</span><span class="s1">&quot; &#39;</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;with a weight list of length </span><span class="si">%s</span><span class="s1">, but the layer was &#39;</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;expecting </span><span class="si">%s</span><span class="s1"> weights. Provided weights: </span><span class="si">%s</span><span class="s1">...&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"></span>

<span class="w">          </span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span><span class="w"> </span><span class="n">expected_num_weights</span><span class="p">,</span><span class="w"> </span><span class="nb">str</span><span class="p">(</span><span class="n">weights</span><span class="p">)[:</span><span class="mi">50</span><span class="p">]))</span><span class="w"></span>

<span class="w">    </span><span class="n">weight_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"></span>

<span class="w">    </span><span class="n">weight_value_tuples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]</span><span class="w"></span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">params</span><span class="p">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span><span class="w"></span>

<span class="w">        </span><span class="n">num_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="o">.</span><span class="n">num_tensors</span><span class="w"></span>

<span class="w">        </span><span class="n">tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">:</span><span class="n">weight_index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">num_tensors</span><span class="p">]</span><span class="w"></span>

<span class="w">        </span><span class="n">param</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">num_tensors</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="p">:</span><span class="w"></span>

<span class="w">        </span><span class="n">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">]</span><span class="w"></span>

<span class="w">        </span><span class="n">ref_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">ref_shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span><span class="w"></span>

<span class="w">          </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">              </span><span class="s1">&#39;Layer weight shape </span><span class="si">%s</span><span class="s1"> not compatible with provided weight &#39;</span><span class="w"></span>

<span class="w">              </span><span class="s1">&#39;shape </span><span class="si">%s</span><span class="s1">&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="n">ref_shape</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">weight_value_tuples</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>

<span class="w">    </span><span class="n">backend</span><span class="o">.</span><span class="n">batch_set_value</span><span class="p">(</span><span class="n">weight_value_tuples</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h3 id="fasterrcnnfpnresnet50caffe">FasterRcnnFPNResnet50Caffe</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">FasterRcnnFPNResnet50Caffe</span><span class="p">(</span>
    <span class="n">num_classes</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>You can use it as follow:</p>
<div class="codehilite"><pre><span></span><code><span class="n">model_faster_rcnn</span> <span class="o">=</span> <span class="n">FasterRcnnFPNResnet50</span><span class="p">(</span><span class="mi">80</span><span class="p">)</span>
<span class="n">base_lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">base_lr</span><span class="p">)</span>
<span class="n">model_faster_rcnn</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">model_faster_rcnn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">ds_test</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">11</span><span class="p">,)</span>
</code></pre></div>

<h4 id="arguments_1">Arguments</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>num_classes</td>
<td>The number of classes of your dataset<br>(<strong>do not include the background class</strong> it is handle for you)</td>
</tr>
<tr>
<td>backbone</td>
<td>A tensorflow Model.</td>
</tr>
</tbody>
</table>
<h4 id="ancestors-in-mro_1">Ancestors (in MRO)</h4>
<ul>
<li>kerod.model.faster_rcnn.FasterRcnnFPN</li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h4 id="methods_1">Methods</h4>
<h4 id="add_loss_1">add_loss</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">losses</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be dependent
on the inputs passed when calling a layer. Hence, when reusing the same
layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may
be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>This method can be used inside a subclassed layer or model's <code>call</code>
function, in which case <code>losses</code> should be a Tensor or list of Tensors.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MyLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div>

<p>This method can also be called directly on a Functional Model during
construction. In this case, any loss Tensors passed to this Model must
be symbolic and be able to be traced back to the model's <code>Input</code>s. These
losses become part of the model's topology and are tracked in <code>get_config</code>.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="c1"># Activity regularization.</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</code></pre></div>

<p>If this is not the case for your loss (if, for example, your loss references
a <code>Variable</code> of one of the model's layers), you can wrap your loss in a
zero-argument lambda. These losses are not tracked as part of the model's
topology since they can't be serialized.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">d</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="c1"># Weight regularization.</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">kernel</span><span class="p">))</span>
</code></pre></div>

<p>Arguments:
  losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses
    may also be zero-argument callables which create a loss tensor.
  **kwargs: Additional keyword arguments for backward compatibility.
    Accepted values:
      inputs - Deprecated, will be automatically inferred.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>  <span class="n">def</span> <span class="n">add_loss</span>(<span class="nb">self</span>, <span class="n">losses</span>, **<span class="n">kwargs</span>):

    <span class="s">&quot;&quot;&quot;Add loss tensor(s), potentially dependent on layer inputs.</span>

<span class="s">    Some losses (for instance, activity regularization losses) may be dependent</span>

<span class="s">    on the inputs passed when calling a layer. Hence, when reusing the same</span>

<span class="s">    layer on different inputs `a` and `b`, some entries in `layer.losses` may</span>

<span class="s">    be dependent on `a` and some on `b`. This method automatically keeps track</span>

<span class="s">    of dependencies.</span>

<span class="s">    This method can be used inside a subclassed layer or model&#39;s `call`</span>

<span class="s">    function, in which case `losses` should be a Tensor or list of Tensors.</span>

<span class="s">    Example:</span>

<span class="s">    ```python</span>

<span class="s">    class MyLayer(tf.keras.layers.Layer):</span>

<span class="s">      def call(self, inputs):</span>

<span class="s">        self.add_loss(tf.abs(tf.reduce_mean(inputs)))</span>

<span class="s">        return inputs</span>

<span class="s">    ```</span>

<span class="s">    This method can also be called directly on a Functional Model during</span>

<span class="s">    construction. In this case, any loss Tensors passed to this Model must</span>

<span class="s">    be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s">    losses become part of the model&#39;s topology and are tracked in `get_config`.</span>

<span class="s">    Example:</span>

<span class="s">    ```python</span>

<span class="s">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">    x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s">    # Activity regularization.</span>

<span class="s">    model.add_loss(tf.abs(tf.reduce_mean(x)))</span>

<span class="s">    ```</span>

<span class="s">    If this is not the case for your loss (if, for example, your loss references</span>

<span class="s">    a `Variable` of one of the model&#39;s layers), you can wrap your loss in a</span>

<span class="s">    zero-argument lambda. These losses are not tracked as part of the model&#39;s</span>

<span class="s">    topology since they can&#39;t be serialized.</span>

<span class="s">    Example:</span>

<span class="s">    ```python</span>

<span class="s">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">    d = tf.keras.layers.Dense(10)</span>

<span class="s">    x = d(inputs)</span>

<span class="s">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s">    # Weight regularization.</span>

<span class="s">    model.add_loss(lambda: tf.reduce_mean(d.kernel))</span>

<span class="s">    ```</span>

<span class="s">    Arguments:</span>

<span class="s">      losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses</span>

<span class="s">        may also be zero-argument callables which create a loss tensor.</span>

<span class="s">      **kwargs: Additional keyword arguments for backward compatibility.</span>

<span class="s">        Accepted values:</span>

<span class="s">          inputs - Deprecated, will be automatically inferred.</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span>.<span class="nb">pop</span>(<span class="s">&#39;inputs&#39;</span>, <span class="n">None</span>)

    <span class="k">if</span> <span class="n">kwargs:</span>

      <span class="n">raise</span> <span class="n">TypeError</span>(<span class="s">&#39;Unknown keyword arguments: %s&#39;</span> % (<span class="n">kwargs</span>.<span class="nb">keys</span>(),))

    <span class="n">def</span> <span class="n">_tag_callable</span>(<span class="n">loss</span>):

      <span class="s">&quot;&quot;&quot;Tags callable loss tensor as `_unconditional_loss`.&quot;&quot;&quot;</span>

      <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

        <span class="c1"># We run the loss without autocasting, as regularizers are often</span>

        <span class="c1"># numerically unstable in float16.</span>

        <span class="k">with</span> <span class="n">autocast_variable</span>.<span class="n">enable_auto_cast_variables</span>(<span class="n">None</span>):

          <span class="n">loss</span> = <span class="n">loss</span>()

      <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

        <span class="k">return</span> <span class="n">None</span>  <span class="c1"># Will be filtered out when computing the .losses property</span>

      <span class="k">if</span> <span class="nb">not</span> <span class="n">tensor_util</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

        <span class="n">loss</span> = <span class="n">ops</span>.<span class="n">convert_to_tensor_v2_with_dispatch</span>(

            <span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

      <span class="n">loss</span>.<span class="n">_unconditional_loss</span> = <span class="nb">True</span>  <span class="c1"># pylint: disable=protected-access</span>

      <span class="k">return</span> <span class="n">loss</span>

    <span class="n">losses</span> = <span class="n">nest</span>.<span class="n">flatten</span>(<span class="n">losses</span>)

    <span class="n">callable_losses</span> = []

    <span class="n">eager_losses</span> = []

    <span class="n">symbolic_losses</span> = []

    <span class="k">for</span> <span class="n">loss</span> <span class="nb">in</span> <span class="n">losses:</span>

      <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

        <span class="n">callable_losses</span>.<span class="nb">append</span>(<span class="n">functools</span>.<span class="n">partial</span>(<span class="n">_tag_callable</span>, <span class="n">loss</span>))

        <span class="n">continue</span>

      <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

        <span class="n">continue</span>

      <span class="k">if</span> <span class="nb">not</span> <span class="n">tensor_util</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>) <span class="o">and</span> <span class="nb">not</span> <span class="n">isinstance</span>(

          <span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>):

        <span class="n">loss</span> = <span class="n">ops</span>.<span class="n">convert_to_tensor_v2_with_dispatch</span>(

            <span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

      <span class="c1"># TF Functions should take the eager path.</span>

      <span class="k">if</span> ((<span class="n">tf_utils</span>.<span class="n">is_symbolic_tensor</span>(<span class="n">loss</span>) <span class="o">or</span>

           <span class="n">isinstance</span>(<span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>)) <span class="o">and</span>

          <span class="nb">not</span> <span class="n">base_layer_utils</span>.<span class="n">is_in_tf_function</span>()):

        <span class="n">symbolic_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

      <span class="n">elif</span> <span class="n">tensor_util</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

        <span class="n">eager_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

    <span class="nb">self</span>.<span class="n">_callable_losses</span>.<span class="n">extend</span>(<span class="n">callable_losses</span>)

    <span class="n">in_call_context</span> = <span class="n">base_layer_utils</span>.<span class="n">call_context</span>().<span class="n">in_call</span>

    <span class="k">if</span> <span class="n">eager_losses</span> <span class="o">and</span> <span class="nb">not</span> <span class="n">in_call_context:</span>

      <span class="n">raise</span> <span class="n">ValueError</span>(

          <span class="s">&#39;Expected a symbolic Tensors or a callable for the loss value. &#39;</span>

          <span class="s">&#39;Please wrap your loss computation in a zero argument `lambda`.&#39;</span>)

    <span class="nb">self</span>.<span class="n">_eager_losses</span>.<span class="n">extend</span>(<span class="n">eager_losses</span>)

    <span class="k">if</span> <span class="n">in_call_context</span> <span class="o">and</span> <span class="nb">not</span> <span class="n">keras_tensor</span>.<span class="n">keras_tensors_enabled</span>():

      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="nb">in</span> <span class="n">symbolic_losses:</span>

        <span class="nb">self</span>.<span class="n">_losses</span>.<span class="nb">append</span>(<span class="n">symbolic_loss</span>)

    <span class="n">else:</span>

      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="nb">in</span> <span class="n">symbolic_losses:</span>

        <span class="k">if</span> <span class="n">getattr</span>(<span class="nb">self</span>, <span class="s">&#39;_is_graph_network&#39;</span>, <span class="nb">False</span>):

          <span class="nb">self</span>.<span class="n">_graph_network_add_loss</span>(<span class="n">symbolic_loss</span>)

        <span class="n">else:</span>

          <span class="c1"># Possible a loss was added in a Layer&#39;s `build`.</span>

          <span class="nb">self</span>.<span class="n">_losses</span>.<span class="nb">append</span>(<span class="n">symbolic_loss</span>)
</code></pre></div>

</details>
<h4 id="add_metric_1">add_metric</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">value</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds metric tensor to the layer.</p>
<p>This method can be used inside the <code>call()</code> method of a subclassed layer
or model.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MyMetricLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyMetricLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;my_metric_layer&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_2&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div>

<p>This method can also be called directly on a Functional Model during
construction. In this case, any tensor passed to this Model must
be symbolic and be able to be traced back to the model's <code>Input</code>s. These
metrics become part of the model's topology and are tracked when you
save the model via <code>save()</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p>Note: Calling <code>add_metric()</code> with the result of a metric object on a
Functional Model, as shown in the example below, is not supported. This is
because we cannot trace the metric result tensor back to the model's inputs.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>value</td>
<td>Metric tensor.</td>
</tr>
<tr>
<td>name</td>
<td>String metric name.</td>
</tr>
<tr>
<td>**kwargs</td>
<td>Additional keyword arguments for backward compatibility.<br>Accepted values:<br><code>aggregation</code> - When the <code>value</code> tensor provided is not the result of<br>calling a <code>keras.Metric</code> instance, it will be aggregated by default<br>using a <code>keras.Metric.Mean</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_metric</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Adds metric tensor to the layer.</span>

<span class="s2">    This method can be used inside the `call()` method of a subclassed layer</span>

<span class="s2">    or model.</span>

<span class="s2">    ```python</span>

<span class="s2">    class MyMetricLayer(tf.keras.layers.Layer):</span>

<span class="s2">      def __init__(self):</span>

<span class="s2">        super(MyMetricLayer, self).__init__(name=&#39;my_metric_layer&#39;)</span>

<span class="s2">        self.mean = tf.keras.metrics.Mean(name=&#39;metric_1&#39;)</span>

<span class="s2">      def call(self, inputs):</span>

<span class="s2">        self.add_metric(self.mean(x))</span>

<span class="s2">        self.add_metric(tf.reduce_sum(x), name=&#39;metric_2&#39;)</span>

<span class="s2">        return inputs</span>

<span class="s2">    ```</span>

<span class="s2">    This method can also be called directly on a Functional Model during</span>

<span class="s2">    construction. In this case, any tensor passed to this Model must</span>

<span class="s2">    be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s2">    metrics become part of the model&#39;s topology and are tracked when you</span>

<span class="s2">    save the model via `save()`.</span>

<span class="s2">    ```python</span>

<span class="s2">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">    x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">    model.add_metric(math_ops.reduce_sum(x), name=&#39;metric_1&#39;)</span>

<span class="s2">    ```</span>

<span class="s2">    Note: Calling `add_metric()` with the result of a metric object on a</span>

<span class="s2">    Functional Model, as shown in the example below, is not supported. This is</span>

<span class="s2">    because we cannot trace the metric result tensor back to the model&#39;s inputs.</span>

<span class="s2">    ```python</span>

<span class="s2">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">    x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">    model.add_metric(tf.keras.metrics.Mean()(x), name=&#39;metric_1&#39;)</span>

<span class="s2">    ```</span>

<span class="s2">    Args:</span>

<span class="s2">      value: Metric tensor.</span>

<span class="s2">      name: String metric name.</span>

<span class="s2">      **kwargs: Additional keyword arguments for backward compatibility.</span>

<span class="s2">        Accepted values:</span>

<span class="s2">        `aggregation` - When the `value` tensor provided is not the result of</span>

<span class="s2">        calling a `keras.Metric` instance, it will be aggregated by default</span>

<span class="s2">        using a `keras.Metric.Mean`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">kwargs_keys</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">list</span><span class="p">(</span><span class="n">kwargs</span><span class="p">.</span><span class="k">keys</span><span class="p">())</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">or</span><span class="w"></span>

<span class="w">        </span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">kwargs_keys</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s1">&#39;aggregation&#39;</span><span class="p">))</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword arguments: &#39;</span><span class="p">,</span><span class="w"> </span><span class="n">str</span><span class="p">(</span><span class="n">kwargs</span><span class="p">.</span><span class="k">keys</span><span class="p">()))</span><span class="w"></span>

<span class="w">    </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hasattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_metric_obj&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">keras_tensor</span><span class="p">.</span><span class="n">keras_tensors_enabled</span><span class="p">()</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">is_symbolic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">keras_tensor</span><span class="p">.</span><span class="n">KerasTensor</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">is_symbolic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf_utils</span><span class="p">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="k">value</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">in_call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">().</span><span class="n">in_call</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">name</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Eg. `self.add_metric(math_ops.reduce_sum(x))`</span><span class="w"></span>

<span class="w">      </span><span class="c1"># In eager mode, we use metric name to lookup a metric. Without a name,</span><span class="w"></span>

<span class="w">      </span><span class="c1"># a new Mean metric wrapper will be created on every model/layer call.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># So, we raise an error when no name is provided.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># We will do the same for symbolic mode for consistency although a name</span><span class="w"></span>

<span class="w">      </span><span class="c1"># will be generated if no name is provided.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># We will not raise this error in the foll use case for the sake of</span><span class="w"></span>

<span class="w">      </span><span class="c1"># consistency as name in provided in the metric constructor.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># mean = metrics.Mean(name=&#39;my_metric&#39;)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># model.add_metric(mean(outputs))</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;Please provide a name for your metric like &#39;</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;`self.add_metric(tf.reduce_sum(inputs), &#39;</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;name=</span><span class="se">\&#39;</span><span class="s1">mean_activation</span><span class="se">\&#39;</span><span class="s1">)`&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">elif</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">value</span><span class="p">.</span><span class="n">_metric_obj</span><span class="p">.</span><span class="k">name</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">is_symbolic</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected a symbolic Tensor for the metric value, &#39;</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;received: &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">str</span><span class="p">(</span><span class="k">value</span><span class="p">))</span><span class="w"></span>

<span class="w">    </span><span class="c1"># If a metric was added in a Layer&#39;s `call` or `build`.</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># TF Function path should take the eager path.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If the given metric is available in `metrics` list we just update state</span><span class="w"></span>

<span class="w">      </span><span class="c1"># on it, otherwise we create a new metric instance and</span><span class="w"></span>

<span class="w">      </span><span class="c1"># add it to the `metrics` list.</span><span class="w"></span>

<span class="w">      </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_metric_obj&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Tensors that come from a Metric object already updated the Metric state.</span><span class="w"></span>

<span class="w">      </span><span class="n">should_update_state</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">metric_obj</span><span class="w"></span>

<span class="w">      </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric_obj</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">name</span><span class="w"></span>

<span class="w">      </span><span class="k">with</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics_lock</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="k">match</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_existing_metric</span><span class="p">(</span><span class="k">name</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">match</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">match</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="c1"># Build the metric object with the value&#39;s dtype if it defines one</span><span class="w"></span>

<span class="w">          </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metrics_mod</span><span class="p">.</span><span class="n">Mean</span><span class="p">(</span><span class="w"></span>

<span class="w">              </span><span class="k">name</span><span class="o">=</span><span class="k">name</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">))</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">should_update_state</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">metric_obj</span><span class="p">(</span><span class="k">value</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;Using the result of calling a `Metric` object &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;when calling `add_metric` on a Functional &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;Model is not supported. Please pass the &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;Tensor to monitor directly.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Insert layers into the Keras Graph Network.</span><span class="w"></span>

<span class="w">      </span><span class="n">aggregation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;mean&#39;</span><span class="w"></span>

<span class="w">      </span><span class="n">self</span><span class="p">.</span><span class="n">_graph_network_add_metric</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">aggregation</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="add_update_1">add_update</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_update</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">updates</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and variance
in a BatchNormalization layer) may be dependent on the inputs passed
when calling a layer. Hence, when reusing the same layer on
different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be
dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>This call is ignored when eager execution is enabled (in that case, variable
updates are run on the fly and thus do not need to be tracked for later
execution).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>updates</td>
<td>Update op, or list/tuple of update ops, or zero-arg callable<br>that returns an update op. A zero-arg callable should be passed in<br>order to disable running the updates by setting <code>trainable=False</code><br>on this Layer, when executing in Eager mode.</td>
</tr>
<tr>
<td>inputs</td>
<td>Deprecated, will be automatically inferred.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">updates</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Add update op(s), potentially dependent on layer inputs.</span>

<span class="s2">    Weight updates (for instance, the updates of the moving mean and variance</span>

<span class="s2">    in a BatchNormalization layer) may be dependent on the inputs passed</span>

<span class="s2">    when calling a layer. Hence, when reusing the same layer on</span>

<span class="s2">    different inputs `a` and `b`, some entries in `layer.updates` may be</span>

<span class="s2">    dependent on `a` and some on `b`. This method automatically keeps track</span>

<span class="s2">    of dependencies.</span>

<span class="s2">    This call is ignored when eager execution is enabled (in that case, variable</span>

<span class="s2">    updates are run on the fly and thus do not need to be tracked for later</span>

<span class="s2">    execution).</span>

<span class="s2">    Arguments:</span>

<span class="s2">      updates: Update op, or list/tuple of update ops, or zero-arg callable</span>

<span class="s2">        that returns an update op. A zero-arg callable should be passed in</span>

<span class="s2">        order to disable running the updates by setting `trainable=False`</span>

<span class="s2">        on this Layer, when executing in Eager mode.</span>

<span class="s2">      inputs: Deprecated, will be automatically inferred.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">tf_logging</span><span class="p">.</span><span class="n">warning</span><span class="p">(</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;`add_update` `inputs` kwarg has been deprecated. You no longer need &#39;</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;to pass a value to `inputs` as it is being automatically inferred.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="c1"># No need to run updates during Functional API construction.</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">in_keras_graph</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Callable updates are disabled by setting `trainable=False`.</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">frozen</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="k">update</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">updates</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">callable</span><span class="p">(</span><span class="k">update</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="k">update</span><span class="p">()</span><span class="w">  </span><span class="c1"># pylint: disable=not-callable</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="add_variable_1">add_variable</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_variable</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use! Alias for <code>add_weight</code>.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_variable</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use! Alias for `add_weight`.</span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.add_variable` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.add_weight` method instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="add_weight_1">add_weight</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synchronization</span><span class="o">=&lt;</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="o">=&lt;</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds a new variable to the layer.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>Variable name.</td>
</tr>
<tr>
<td>shape</td>
<td>Variable shape. Defaults to scalar if unspecified.</td>
</tr>
<tr>
<td>dtype</td>
<td>The type of the variable. Defaults to <code>self.dtype</code>.</td>
</tr>
<tr>
<td>initializer</td>
<td>Initializer instance (callable).</td>
</tr>
<tr>
<td>regularizer</td>
<td>Regularizer instance (callable).</td>
</tr>
<tr>
<td>trainable</td>
<td>Boolean, whether the variable should be part of the layer's<br>"trainable_variables" (e.g. variables, biases)<br>or "non_trainable_variables" (e.g. BatchNorm mean and variance).<br>Note that <code>trainable</code> cannot be <code>True</code> if <code>synchronization</code><br>is set to <code>ON_READ</code>.</td>
</tr>
<tr>
<td>constraint</td>
<td>Constraint instance (callable).</td>
</tr>
<tr>
<td>use_resource</td>
<td>Whether to use <code>ResourceVariable</code>.</td>
</tr>
<tr>
<td>synchronization</td>
<td>Indicates when a distributed a variable will be<br>aggregated. Accepted values are constants defined in the class<br><code>tf.VariableSynchronization</code>. By default the synchronization is set to<br><code>AUTO</code> and the current <code>DistributionStrategy</code> chooses<br>when to synchronize. If <code>synchronization</code> is set to <code>ON_READ</code>,<br><code>trainable</code> must not be set to <code>True</code>.</td>
</tr>
<tr>
<td>aggregation</td>
<td>Indicates how a distributed variable will be aggregated.<br>Accepted values are constants defined in the class<br><code>tf.VariableAggregation</code>.</td>
</tr>
<tr>
<td>**kwargs</td>
<td>Additional keyword arguments. Accepted values are <code>getter</code>,<br><code>collections</code>, <code>experimental_autocast</code> and <code>caching_device</code>.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>The variable created.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>When giving unsupported dtype and no initializer or when<br>trainable has been set to True with synchronization set as <code>ON_READ</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.for_subclass_implementers</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_weight</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="k">name</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">shape</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">dtype</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">initializer</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">regularizer</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">trainable</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="k">constraint</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">use_resource</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">synchronization</span><span class="o">=</span><span class="n">tf_variables</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">AUTO</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">aggregation</span><span class="o">=</span><span class="n">tf_variables</span><span class="p">.</span><span class="n">VariableAggregation</span><span class="p">.</span><span class="k">NONE</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Adds a new variable to the layer.</span>

<span class="s2">    Arguments:</span>

<span class="s2">      name: Variable name.</span>

<span class="s2">      shape: Variable shape. Defaults to scalar if unspecified.</span>

<span class="s2">      dtype: The type of the variable. Defaults to `self.dtype`.</span>

<span class="s2">      initializer: Initializer instance (callable).</span>

<span class="s2">      regularizer: Regularizer instance (callable).</span>

<span class="s2">      trainable: Boolean, whether the variable should be part of the layer&#39;s</span>

<span class="s2">        &quot;</span><span class="n">trainable_variables</span><span class="s2">&quot; (e.g. variables, biases)</span>

<span class="s2">        or &quot;</span><span class="n">non_trainable_variables</span><span class="s2">&quot; (e.g. BatchNorm mean and variance).</span>

<span class="s2">        Note that `trainable` cannot be `True` if `synchronization`</span>

<span class="s2">        is set to `ON_READ`.</span>

<span class="s2">      constraint: Constraint instance (callable).</span>

<span class="s2">      use_resource: Whether to use `ResourceVariable`.</span>

<span class="s2">      synchronization: Indicates when a distributed a variable will be</span>

<span class="s2">        aggregated. Accepted values are constants defined in the class</span>

<span class="s2">        `tf.VariableSynchronization`. By default the synchronization is set to</span>

<span class="s2">        `AUTO` and the current `DistributionStrategy` chooses</span>

<span class="s2">        when to synchronize. If `synchronization` is set to `ON_READ`,</span>

<span class="s2">        `trainable` must not be set to `True`.</span>

<span class="s2">      aggregation: Indicates how a distributed variable will be aggregated.</span>

<span class="s2">        Accepted values are constants defined in the class</span>

<span class="s2">        `tf.VariableAggregation`.</span>

<span class="s2">      **kwargs: Additional keyword arguments. Accepted values are `getter`,</span>

<span class="s2">        `collections`, `experimental_autocast` and `caching_device`.</span>

<span class="s2">    Returns:</span>

<span class="s2">      The variable created.</span>

<span class="s2">    Raises:</span>

<span class="s2">      ValueError: When giving unsupported dtype and no initializer or when</span>

<span class="s2">        trainable has been set to True with synchronization set as `ON_READ`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;partitioner&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w">  </span><span class="c1"># Ignored.</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Validate optional keyword arguments.</span><span class="w"></span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">kwargs</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">[</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;experimental_autocast&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;caching_device&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;getter&#39;</span><span class="err">]</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword argument:&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">kwarg</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">collections_arg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="c1"># &#39;experimental_autocast&#39; can be set to False by the caller to indicate an</span><span class="w"></span>

<span class="w">    </span><span class="c1"># AutoCastVariable should never be created.</span><span class="w"></span>

<span class="w">    </span><span class="n">autocast</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;experimental_autocast&#39;</span><span class="p">,</span><span class="w"> </span><span class="no">True</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="c1"># See the docstring for tf.Variable about the details for caching_device.</span><span class="w"></span>

<span class="w">    </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;caching_device&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">backend</span><span class="p">.</span><span class="n">floatx</span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dtypes</span><span class="p">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># The policy is &quot;_infer&quot;, so we infer the policy from the variable dtype.</span><span class="w"></span>

<span class="w">      </span><span class="n">self</span><span class="p">.</span><span class="n">_set_dtype_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">Policy</span><span class="p">(</span><span class="n">dtype</span><span class="p">.</span><span class="n">base_dtype</span><span class="p">.</span><span class="k">name</span><span class="p">))</span><span class="w"></span>

<span class="w">    </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">regularizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">regularizers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="n">regularizer</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">constraint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">constraints</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="k">constraint</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">synchronization</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">tf_variables</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">ON_READ</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;Synchronization value can be set to &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;VariableSynchronization.ON_READ only for non-trainable variables. &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;You have specified trainable=True and &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;synchronization=VariableSynchronization.ON_READ.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="c1"># Set trainable to be false when variable is to be synced on read.</span><span class="w"></span>

<span class="w">        </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">False</span><span class="w"></span>

<span class="w">    </span><span class="n">elif</span><span class="w"> </span><span class="n">trainable</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">True</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Initialize variable when no initializer provided</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">initializer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If dtype is DT_FLOAT, provide a uniform unit scaling initializer</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If dtype is DT_INT/DT_UINT, provide a default value `zero`</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If dtype is DT_BOOL, provide a default value `FALSE`</span><span class="w"></span>

<span class="w">      </span><span class="n">elif</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_integer</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_unsigned</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_bool</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;An initializer for variable %s of type %s is required&#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39; for layer %s&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="k">name</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">base_dtype</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">name</span><span class="p">))</span><span class="w"></span>

<span class="w">    </span><span class="n">getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;getter&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">make_variable</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">autocast</span><span class="w"> </span><span class="k">and</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">compute_dtype</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span><span class="w"></span>

<span class="w">        </span><span class="k">and</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">old_getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getter</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Wrap variable constructor to return an AutoCastVariable.</span><span class="w"></span>

<span class="w">      </span><span class="n">def</span><span class="w"> </span><span class="n">getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w">  </span><span class="c1"># pylint: disable=function-redefined</span><span class="w"></span>

<span class="w">        </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">autocast_variable</span><span class="p">.</span><span class="n">create_autocast_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Also the caching_device does not work with the mixed precision API,</span><span class="w"></span>

<span class="w">      </span><span class="c1"># disable it if it is specified.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># TODO(b/142020079): Reenable it once the bug is fixed.</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">caching_device</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">tf_logging</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`caching_device` does not work with mixed precision &#39;</span><span class="w"></span>

<span class="w">                        </span><span class="s1">&#39;API. Ignoring user specified `caching_device`.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="w"></span>

<span class="w">    </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_add_variable_with_custom_getter</span><span class="p">(</span><span class="w"></span>

<span class="w">        </span><span class="k">name</span><span class="o">=</span><span class="k">name</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="c1"># TODO(allenl): a `make_variable` equivalent should be added as a</span><span class="w"></span>

<span class="w">        </span><span class="c1"># `Trackable` method.</span><span class="w"></span>

<span class="w">        </span><span class="n">getter</span><span class="o">=</span><span class="n">getter</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="c1"># Manage errors in Layer rather than Trackable.</span><span class="w"></span>

<span class="w">        </span><span class="n">overwrite</span><span class="o">=</span><span class="no">True</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="k">constraint</span><span class="o">=</span><span class="k">constraint</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">collections</span><span class="o">=</span><span class="n">collections_arg</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">regularizer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># TODO(fchollet): in the future, this should be handled at the</span><span class="w"></span>

<span class="w">      </span><span class="c1"># level of variable creation, and weight regularization losses</span><span class="w"></span>

<span class="w">      </span><span class="c1"># should be variable attributes.</span><span class="w"></span>

<span class="w">      </span><span class="n">name_in_scope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="k">name</span><span class="err">[</span><span class="o">:</span><span class="n">variable</span><span class="p">.</span><span class="k">name</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span><span class="err">]</span><span class="w"></span>

<span class="w">      </span><span class="n">self</span><span class="p">.</span><span class="n">_handle_weight_regularization</span><span class="p">(</span><span class="n">name_in_scope</span><span class="p">,</span><span class="w"></span>

<span class="w">                                         </span><span class="n">variable</span><span class="p">,</span><span class="w"></span>

<span class="w">                                         </span><span class="n">regularizer</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">is_split_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">variable</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">variable</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="apply_1">apply</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use!</p>
<p>This is an alias of <code>self.__call__</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Input tensor(s).</td>
</tr>
<tr>
<td>*args</td>
<td>additional positional arguments to be passed to <code>self.call</code>.</td>
</tr>
<tr>
<td>**kwargs</td>
<td>additional keyword arguments to be passed to <code>self.call</code>.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Output tensor(s).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Deprecated, do NOT use!</span>

<span class="ss">    This is an alias of `self.__call__`.</span>

<span class="ss">    Arguments:</span>

<span class="ss">      inputs: Input tensor(s).</span>

<span class="ss">      *args: additional positional arguments to be passed to `self.call`.</span>

<span class="ss">      **kwargs: additional keyword arguments to be passed to `self.call`.</span>

<span class="ss">    Returns:</span>

<span class="ss">      Output tensor(s).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.apply` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.__call__` method instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">__call__</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="call_1">call</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">training</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Perform an inference in training.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>A dict with the following schema:<br><code>images</code>: A Tensor of shape [batch_size, height, width, 3]<br><code>image_informations</code>: A float32 Tensor of shape [batch_size, 2] where<br>    the last dimension represents the original height and<br>    width of the images (without the padding).<br><br><code>ground_truths</code>: A dict<br>    - <code>BoxField.LABELS</code>: A 3-D tensor of shape [batch_size, num_gt, num_classes],<br>    - <code>BoxField.BOXES</code>: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)]<br>    - <code>BoxField.LABELS</code>: A 3-D tensor of int32 and shape [batch_size, num_gt]<br>    - <code>BoxField.WEIGHTS</code>: A 3-D tensor of float and shape [batch_size, num_gt]<br>    - <code>BoxField.NUM_BOXES</code>: A 2-D tensor of int32 and shape [batch_size, 1]<br>        which allows to remove the padding created by tf.Data.<br>        Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32)<br>        then my second box has a padding of 1</td>
</tr>
<tr>
<td>training</td>
<td>Is automatically set to <code>True</code> in train and test mode<br>(normally test should be at false). Why? Through the call we the losses and the metrics<br>of the rpn and fast_rcnn. They are automatically added with <code>add_loss</code> and <code>add_metrics</code>.<br>In test we want to benefit from those and therefore we compute them. It is an inheritance<br>from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the<br>train_step and test_step. However for now this method benefit of the encapsulation of<br>the <code>self.compiled_loss</code> method.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tuple</td>
<td>- <code>classification_pred</code>: A Tensor of shape [batch_size, num_boxes, num_classes]<br>    representing the class probability.<br>- <code>localization_pred</code>: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)]<br>- <code>anchors</code>: A Tensor of shape [batch_size, num_boxes, 4]</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">training</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Perform an inference in training.</span>

<span class="s2">        Arguments:</span>

<span class="s2">            inputs: A dict with the following schema:</span>

<span class="s2">                `images`: A Tensor of shape [batch_size, height, width, 3]</span>

<span class="s2">                `image_informations`: A float32 Tensor of shape [batch_size, 2] where</span>

<span class="s2">                    the last dimension represents the original height and</span>

<span class="s2">                    width of the images (without the padding).</span>

<span class="s2">                `ground_truths`: A dict</span>

<span class="s2">                    - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes],</span>

<span class="s2">                    - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)]</span>

<span class="s2">                    - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt]</span>

<span class="s2">                    - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt]</span>

<span class="s2">                    - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1]</span>

<span class="s2">                        which allows to remove the padding created by tf.Data.</span>

<span class="s2">                        Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32)</span>

<span class="s2">                        then my second box has a padding of 1</span>

<span class="s2">            training: Is automatically set to `True` in train and test mode</span>

<span class="s2">                (normally test should be at false). Why? Through the call we the losses and the metrics</span>

<span class="s2">                of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`.</span>

<span class="s2">                In test we want to benefit from those and therefore we compute them. It is an inheritance</span>

<span class="s2">                from tensorflow 2.0 and 2.1 and I&#39;ll think to move them in a more traditional way inside the</span>

<span class="s2">                train_step and test_step. However for now this method benefit of the encapsulation of</span>

<span class="s2">                the `self.compiled_loss` method.</span>

<span class="s2">        Returns:</span>

<span class="s2">            Tuple:</span>

<span class="s2">                - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes]</span>

<span class="s2">                    representing the class probability.</span>

<span class="s2">                - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)]</span>

<span class="s2">                - `anchors`: A Tensor of shape [batch_size, num_boxes, 4]</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="err">[</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES</span><span class="err">]</span><span class="w"></span>

<span class="w">        </span><span class="n">images_information</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="err">[</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES_INFO</span><span class="err">]</span><span class="w"></span>

<span class="w">        </span><span class="c1"># The preprocessing dedicated to the backbone is done inside the model.</span><span class="w"></span>

<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">images</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">pyramid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fpn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">anchors_per_lvl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">rpn</span><span class="p">(</span><span class="n">pyramid</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="o">:</span><span class="w"></span>

<span class="w">            </span><span class="n">apply_kernel_regularization</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">l2</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="c1"># add_loss stores the rpn losses computation in self.losses</span><span class="w"></span>

<span class="w">            </span><span class="n">_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">rpn</span><span class="p">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">anchors_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                      </span><span class="n">inputs</span><span class="err">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="err">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">num_boxes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2000</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="mi">1000</span><span class="w"></span>

<span class="w">        </span><span class="n">rois</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">post_process_rpn</span><span class="p">(</span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">anchors_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">images_information</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">pre_nms_topk_per_lvl</span><span class="o">=</span><span class="n">num_boxes</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">post_nms_topk</span><span class="o">=</span><span class="n">num_boxes</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="o">:</span><span class="w"></span>

<span class="w">            </span><span class="n">ground_truths</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="err">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="err">]</span><span class="w"></span>

<span class="w">            </span><span class="c1"># Include the ground_truths as RoIs for the training</span><span class="w"></span>

<span class="w">            </span><span class="n">rois</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="nf">concat</span><span class="p">(</span><span class="err">[</span><span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_compute_dtype</span><span class="p">),</span><span class="w"> </span><span class="n">ground_truths</span><span class="err">[</span><span class="n">BoxField</span><span class="p">.</span><span class="n">BOXES</span><span class="err">]]</span><span class="p">,</span><span class="w"></span>

<span class="w">                             </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="c1"># Sample the boxes needed for inference</span><span class="w"></span>

<span class="w">            </span><span class="n">y_true</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">rois</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fast_rcnn</span><span class="p">.</span><span class="n">sample_boxes</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span><span class="w"> </span><span class="n">ground_truths</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">classification_pred</span><span class="p">,</span><span class="w"> </span><span class="n">localization_pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fast_rcnn</span><span class="p">(</span><span class="err">[</span><span class="n">pyramid</span><span class="p">,</span><span class="w"> </span><span class="n">rois</span><span class="err">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="o">:</span><span class="w"></span>

<span class="w">            </span><span class="c1"># add_loss stores the fast_rcnn losses computation in self.losses</span><span class="w"></span>

<span class="w">            </span><span class="n">_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fast_rcnn</span><span class="p">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">classification_pred</span><span class="p">,</span><span class="w"> </span><span class="n">localization_pred</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">classification_pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">classification_pred</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">classification_pred</span><span class="p">,</span><span class="w"> </span><span class="n">localization_pred</span><span class="p">,</span><span class="w"> </span><span class="n">rois</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="compute_mask_1">compute_mask</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes an output mask tensor.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Tensor or list of tensors.</td>
</tr>
<tr>
<td>mask</td>
<td>Tensor or list of tensors.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None or a tensor (or list of tensors,<br>one per output tensor of the layer).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@generic_utils</span><span class="p">.</span><span class="k">default</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">compute_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">mask</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="err">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="nl">pylint</span><span class="p">:</span><span class="w"> </span><span class="n">disable</span><span class="o">=</span><span class="n">unused</span><span class="o">-</span><span class="n">argument</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Computes an output mask tensor.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        inputs: Tensor or list of tensors.</span>

<span class="ss">        mask: Tensor or list of tensors.</span>

<span class="ss">    Returns:</span>

<span class="ss">        None or a tensor (or list of tensors,</span>

<span class="ss">            one per output tensor of the layer).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">_supports_masking</span><span class="p">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="p">(</span><span class="n">m</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s1">&#39; does not support masking, &#39;</span><span class="w"></span>

<span class="w">                        </span><span class="s1">&#39;but was passed an input_mask: &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">str</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="w"></span>

<span class="w">      </span><span class="err">#</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="nl">supported</span><span class="p">:</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">mask</span><span class="p">.</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="k">None</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="n">supported</span><span class="p">,</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="k">default</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="k">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="n">mask</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">mask</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="compute_output_shape_1">compute_output_shape</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_shape</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes the output shape of the layer.</p>
<p>If the layer has not been built, this method will call <code>build</code> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_shape</td>
<td>Shape tuple (tuple of integers)<br>or list of shape tuples (one per output tensor of the layer).<br>Shape tuples can include None for free dimensions,<br>instead of an integer.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>An input shape tuple.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>  <span class="n">def</span> <span class="n">compute_output_shape</span>(<span class="nb">self</span>, <span class="n">input_shape</span>):

    <span class="s">&quot;&quot;&quot;Computes the output shape of the layer.</span>

<span class="s">    If the layer has not been built, this method will call `build` on the</span>

<span class="s">    layer. This assumes that the layer will later be used with inputs that</span>

<span class="s">    match the input shape provided here.</span>

<span class="s">    Arguments:</span>

<span class="s">        input_shape: Shape tuple (tuple of integers)</span>

<span class="s">            or list of shape tuples (one per output tensor of the layer).</span>

<span class="s">            Shape tuples can include None for free dimensions,</span>

<span class="s">            instead of an integer.</span>

<span class="s">    Returns:</span>

<span class="s">        An input shape tuple.</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">context</span>.<span class="n">executing_eagerly</span>():

      <span class="c1"># In this case we build the model first in order to do shape inference.</span>

      <span class="c1"># This is acceptable because the framework only calls</span>

      <span class="c1"># `compute_output_shape` on shape values that the layer would later be</span>

      <span class="c1"># built for. It would however cause issues in case a user attempts to</span>

      <span class="c1"># use `compute_output_shape` manually with shapes that are incompatible</span>

      <span class="c1"># with the shape the Layer will be called on (these users will have to</span>

      <span class="c1"># implement `compute_output_shape` themselves).</span>

      <span class="nb">self</span>.<span class="n">_maybe_build</span>(<span class="n">input_shape</span>)

      <span class="k">with</span> <span class="n">func_graph</span>.<span class="n">FuncGraph</span>(<span class="n">str</span>(<span class="nb">self</span>.<span class="nb">name</span>) + <span class="s">&#39;_scratch_graph&#39;</span>).<span class="n">as_default</span>():

        <span class="n">input_shape</span> = <span class="n">tf_utils</span>.<span class="n">convert_shapes</span>(<span class="n">input_shape</span>, <span class="n">to_tuples</span>=<span class="nb">False</span>)

        <span class="n">def</span> <span class="n">_make_placeholder_like</span>(<span class="nb">shape</span>):

          <span class="n">ph</span> = <span class="n">backend</span>.<span class="nb">placeholder</span>(<span class="nb">shape</span>=<span class="nb">shape</span>, <span class="n">dtype</span>=<span class="nb">self</span>.<span class="n">dtype</span>)

          <span class="n">ph</span>.<span class="n">_keras_mask</span> = <span class="n">None</span>

          <span class="k">return</span> <span class="n">ph</span>

        <span class="n">inputs</span> = <span class="n">nest</span>.<span class="n">map_structure</span>(<span class="n">_make_placeholder_like</span>, <span class="n">input_shape</span>)

        <span class="n">try:</span>

          <span class="n">outputs</span> = <span class="nb">self</span>(<span class="n">inputs</span>, <span class="n">training</span>=<span class="nb">False</span>)

        <span class="n">except</span> <span class="n">TypeError</span> <span class="n">as</span> <span class="n">e:</span>

          <span class="n">six</span>.<span class="n">raise_from</span>(

              <span class="n">NotImplementedError</span>(

                  <span class="s">&#39;We could not automatically infer the static shape of the &#39;</span>

                  <span class="s">&#39;layer\&#39;s output. Please implement the &#39;</span>

                  <span class="s">&#39;`compute_output_shape` method on your layer (%s).&#39;</span> %

                  <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>), <span class="nb">e</span>)

      <span class="k">return</span> <span class="n">nest</span>.<span class="n">map_structure</span>(<span class="n">lambda</span> <span class="n">t:</span> <span class="nb">t</span>.<span class="nb">shape</span>, <span class="n">outputs</span>)

    <span class="n">raise</span> <span class="n">NotImplementedError</span>(

        <span class="s">&#39;Please run in eager mode or implement the `compute_output_shape` &#39;</span>

        <span class="s">&#39;method on your layer (%s).&#39;</span> % <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>)
</code></pre></div>

</details>
<h4 id="compute_output_signature_1">compute_output_signature</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_signature</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_signature</span>
<span class="p">)</span>
</code></pre></div>

<p>Compute the output tensor signature of the layer based on the inputs.</p>
<p>Unlike a TensorShape object, a TensorSpec object contains both shape
and dtype information for a tensor. This method allows layers to provide
output dtype information if it is different from the input dtype.
For any layer that doesn't implement this function,
the framework will fall back to use <code>compute_output_shape</code>, and will
assume that the output dtype matches the input dtype.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_signature</td>
<td>Single TensorSpec or nested structure of TensorSpec<br>objects, describing a candidate input for the layer.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Single TensorSpec or nested structure of TensorSpec objects, describing<br>how the layer would transform the provided input.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>TypeError</td>
<td>If input_signature contains a non-TensorSpec object.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.for_subclass_implementers</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">compute_output_signature</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Compute the output tensor signature of the layer based on the inputs.</span>

<span class="s2">    Unlike a TensorShape object, a TensorSpec object contains both shape</span>

<span class="s2">    and dtype information for a tensor. This method allows layers to provide</span>

<span class="s2">    output dtype information if it is different from the input dtype.</span>

<span class="s2">    For any layer that doesn&#39;t implement this function,</span>

<span class="s2">    the framework will fall back to use `compute_output_shape`, and will</span>

<span class="s2">    assume that the output dtype matches the input dtype.</span>

<span class="s2">    Args:</span>

<span class="s2">      input_signature: Single TensorSpec or nested structure of TensorSpec</span>

<span class="s2">        objects, describing a candidate input for the layer.</span>

<span class="s2">    Returns:</span>

<span class="s2">      Single TensorSpec or nested structure of TensorSpec objects, describing</span>

<span class="s2">        how the layer would transform the provided input.</span>

<span class="s2">    Raises:</span>

<span class="s2">      TypeError: If input_signature contains a non-TensorSpec object.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">check_type_return_shape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_spec</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;Only TensorSpec signature types are supported, &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;but saw signature signature entry: {}.&#39;</span><span class="p">.</span><span class="k">format</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">shape</span><span class="w"></span>

<span class="w">    </span><span class="n">input_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">check_type_return_shape</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">output_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_compute_dtype</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">input_dtypes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[</span><span class="n">s</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_signature</span><span class="p">)</span><span class="err">]</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Default behavior when self.dtype is None, is to use the first input&#39;s</span><span class="w"></span>

<span class="w">      </span><span class="c1"># dtype.</span><span class="w"></span>

<span class="w">      </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_dtypes</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span><span class="w"></span>

<span class="w">        </span><span class="n">lambda</span><span class="w"> </span><span class="n">s</span><span class="o">:</span><span class="w"> </span><span class="n">tensor_spec</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="o">=</span><span class="n">s</span><span class="p">),</span><span class="w"></span>

<span class="w">        </span><span class="n">output_shape</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="count_params_1">count_params</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Count the total number of scalars composing the weights.</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>An integer count.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>if the layer isn't yet built<br>(in which case its weights aren't yet defined).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Count the total number of scalars composing the weights.</span>

<span class="s2">    Returns:</span>

<span class="s2">        An integer count.</span>

<span class="s2">    Raises:</span>

<span class="s2">        ValueError: if the layer isn&#39;t yet built</span>

<span class="s2">          (in which case its weights aren&#39;t yet defined).</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">built</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="n">tf_utils</span><span class="p">.</span><span class="n">maybe_init_scope</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inputs</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;You tried to call `count_params` on &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="o">+</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;, but the layer isn</span><span class="se">\&#39;</span><span class="s1">t built. &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;You can build it manually via: `&#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="o">+</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;.build(batch_input_shape)`.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">layer_utils</span><span class="p">.</span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="export_for_serving_1">export_for_serving</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">export_for_serving</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">filepath</span>
<span class="p">)</span>
</code></pre></div>

<p>Allow to bypass the save_model behavior the graph in serving mode.</p>
<p>Currently, the issue is that in training the ground_truths are passed to the call method but
not in inference. For the serving only the <code>images</code> and <code>images_information</code> are defined.
It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow
when the <code>training</code> arguments is defined int the method <code>call</code>, <code>tf.save_model.save</code> method
performs a check on the graph for training=False and training=True.
However, we don't want this check to be perform because our ground_truths inputs aren't defined.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">export_for_serving</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">filepath</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Allow to bypass the save_model behavior the graph in serving mode.</span>

<span class="s2">        Currently, the issue is that in training the ground_truths are passed to the call method but</span>

<span class="s2">        not in inference. For the serving only the `images` and `images_information` are defined.</span>

<span class="s2">        It means the inputs link to the ground_truths won&#39;t be defined in serving. However, in tensorflow</span>

<span class="s2">        when the `training` arguments is defined int the method `call`, `tf.save_model.save` method</span>

<span class="s2">        performs a check on the graph for training=False and training=True.</span>

<span class="s2">        However, we don&#39;t want this check to be perform because our ground_truths inputs aren&#39;t defined.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">True</span><span class="w"></span>

<span class="w">        </span><span class="n">call_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">serving_step</span><span class="p">.</span><span class="n">get_concrete_function</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">tf</span><span class="p">.</span><span class="n">saved_model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">filepath</span><span class="p">,</span><span class="w"> </span><span class="n">signatures</span><span class="o">=</span><span class="err">{</span><span class="s1">&#39;serving_default&#39;</span><span class="o">:</span><span class="w"> </span><span class="n">call_output</span><span class="err">}</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">False</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_input_at_1">get_input_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A tensor (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;input_tensors&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;input&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_input_mask_at_1">get_input_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A mask tensor</span>

<span class="ss">        (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_input_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &#39;_keras_mask&#39;, None) for x in inputs</span><span class="o">]</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_input_shape_at_1">get_input_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the input shape(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A shape tuple</span>

<span class="ss">        (or list of shape tuples if the layer has multiple inputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;input_shapes&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;input shape&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_losses_for_1">get_losses_for</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_losses_for</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use!</p>
<p>Retrieves losses relevant to a specific set of inputs.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Input tensor or list/tuple of input tensors.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>List of loss tensors of the layer that depend on <code>inputs</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_generate_docs</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_losses_for</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use!</span>

<span class="s2">    Retrieves losses relevant to a specific set of inputs.</span>

<span class="s2">    Arguments:</span>

<span class="s2">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="s2">    Returns:</span>

<span class="s2">      List of loss tensors of the layer that depend on `inputs`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.get_losses_for` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.losses` instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">losses</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_output_at_1">get_output_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A tensor (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;output_tensors&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;output&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_output_mask_at_1">get_output_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A mask tensor</span>

<span class="ss">        (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_output_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &#39;_keras_mask&#39;, None) for x in output</span><span class="o">]</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_output_shape_at_1">get_output_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the output shape(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A shape tuple</span>

<span class="ss">        (or list of shape tuples if the layer has multiple outputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;output_shapes&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;output shape&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_updates_for_1">get_updates_for</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_updates_for</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use!</p>
<p>Retrieves updates relevant to a specific set of inputs.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Input tensor or list/tuple of input tensors.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>List of update ops of the layer that depend on <code>inputs</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_generate_docs</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_updates_for</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use!</span>

<span class="s2">    Retrieves updates relevant to a specific set of inputs.</span>

<span class="s2">    Arguments:</span>

<span class="s2">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="s2">    Returns:</span>

<span class="s2">      List of update ops of the layer that depend on `inputs`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.get_updates_for` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.updates` method instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">updates</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="serving_step_1">serving_step</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">serving_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">images</span><span class="p">,</span>
    <span class="n">images_info</span>
<span class="p">)</span>
</code></pre></div>

<p>Allow to bypass the save_model behavior the graph in serving mode.</p>
<p>Currently, the issue is that in training the ground_truths are passed to the call method but
not in inference. For the serving only the <code>images</code> and <code>images_information</code> are defined.
It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow
absolutely want it and will return an exception if the ground_truth isn't provided.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@tf.function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="err">[</span><span class="w"></span>

<span class="w">        </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES</span><span class="p">),</span><span class="w"></span>

<span class="w">        </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES_INFO</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="err">]</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">serving_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">images</span><span class="p">,</span><span class="w"> </span><span class="n">images_info</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Allow to bypass the save_model behavior the graph in serving mode.</span>

<span class="s2">        Currently, the issue is that in training the ground_truths are passed to the call method but</span>

<span class="s2">        not in inference. For the serving only the `images` and `images_information` are defined.</span>

<span class="s2">        It means the inputs link to the ground_truths won&#39;t be defined in serving. However, tensorflow</span>

<span class="s2">        absolutely want it and will return an exception if the ground_truth isn&#39;t provided.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">predict_step</span><span class="p">(</span><span class="err">{</span><span class="w"></span>

<span class="w">            </span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES</span><span class="o">:</span><span class="w"> </span><span class="n">images</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES_INFO</span><span class="o">:</span><span class="w"> </span><span class="n">images_info</span><span class="w"></span>

<span class="w">        </span><span class="err">}</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="set_weights_1">set_weights</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">weights</span>
<span class="p">)</span>
</code></pre></div>

<p>Sets the weights of the layer, from Numpy arrays.</p>
<p>The weights of a layer represent the state of the layer. This function
sets the weight values from numpy arrays. The weight values should be
passed in the order they are created by the layer. Note that the layer's
weights must be instantiated before calling this function by calling
the layer.</p>
<p>For example, a Dense layer returns a list of two values-- per-output
weights and the bias value. These can be used to set the weights of another
Dense layer:</p>
<blockquote>
<blockquote>
<blockquote>
<p>a = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(1.))
a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))
a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
b = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(2.))
b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))
b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
b.set_weights(a.get_weights())
b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>weights</td>
<td>a list of Numpy arrays. The number<br>of arrays and their shape must match<br>number of the dimensions of the weights<br>of the layer (i.e. it should match the<br>output of <code>get_weights</code>).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>If the provided weights list does not match the<br>layer's specifications.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">):</span><span class="w"></span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets the weights of the layer, from Numpy arrays.</span>

<span class="sd">    The weights of a layer represent the state of the layer. This function</span>

<span class="sd">    sets the weight values from numpy arrays. The weight values should be</span>

<span class="sd">    passed in the order they are created by the layer. Note that the layer&#39;s</span>

<span class="sd">    weights must be instantiated before calling this function by calling</span>

<span class="sd">    the layer.</span>

<span class="sd">    For example, a Dense layer returns a list of two values-- per-output</span>

<span class="sd">    weights and the bias value. These can be used to set the weights of another</span>

<span class="sd">    Dense layer:</span>

<span class="sd">    &gt;&gt;&gt; a = tf.keras.layers.Dense(1,</span>

<span class="sd">    ...   kernel_initializer=tf.constant_initializer(1.))</span>

<span class="sd">    &gt;&gt;&gt; a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))</span>

<span class="sd">    &gt;&gt;&gt; a.get_weights()</span>

<span class="sd">    [array([[1.],</span>

<span class="sd">           [1.],</span>

<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    &gt;&gt;&gt; b = tf.keras.layers.Dense(1,</span>

<span class="sd">    ...   kernel_initializer=tf.constant_initializer(2.))</span>

<span class="sd">    &gt;&gt;&gt; b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))</span>

<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>

<span class="sd">    [array([[2.],</span>

<span class="sd">           [2.],</span>

<span class="sd">           [2.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    &gt;&gt;&gt; b.set_weights(a.get_weights())</span>

<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>

<span class="sd">    [array([[1.],</span>

<span class="sd">           [1.],</span>

<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    Arguments:</span>

<span class="sd">        weights: a list of Numpy arrays. The number</span>

<span class="sd">            of arrays and their shape must match</span>

<span class="sd">            number of the dimensions of the weights</span>

<span class="sd">            of the layer (i.e. it should match the</span>

<span class="sd">            output of `get_weights`).</span>

<span class="sd">    Raises:</span>

<span class="sd">        ValueError: If the provided weights list does not match the</span>

<span class="sd">            layer&#39;s specifications.</span>

<span class="sd">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="w"></span>

<span class="w">    </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"></span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">params</span><span class="p">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span><span class="w"></span>

<span class="w">        </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">param</span><span class="o">.</span><span class="n">num_tensors</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="p">:</span><span class="w"></span>

<span class="w">        </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;You called `set_weights(weights)` on layer &quot;</span><span class="si">%s</span><span class="s1">&quot; &#39;</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;with a weight list of length </span><span class="si">%s</span><span class="s1">, but the layer was &#39;</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;expecting </span><span class="si">%s</span><span class="s1"> weights. Provided weights: </span><span class="si">%s</span><span class="s1">...&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"></span>

<span class="w">          </span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span><span class="w"> </span><span class="n">expected_num_weights</span><span class="p">,</span><span class="w"> </span><span class="nb">str</span><span class="p">(</span><span class="n">weights</span><span class="p">)[:</span><span class="mi">50</span><span class="p">]))</span><span class="w"></span>

<span class="w">    </span><span class="n">weight_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"></span>

<span class="w">    </span><span class="n">weight_value_tuples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]</span><span class="w"></span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">params</span><span class="p">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span><span class="w"></span>

<span class="w">        </span><span class="n">num_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="o">.</span><span class="n">num_tensors</span><span class="w"></span>

<span class="w">        </span><span class="n">tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">:</span><span class="n">weight_index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">num_tensors</span><span class="p">]</span><span class="w"></span>

<span class="w">        </span><span class="n">param</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">num_tensors</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="p">:</span><span class="w"></span>

<span class="w">        </span><span class="n">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">]</span><span class="w"></span>

<span class="w">        </span><span class="n">ref_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">ref_shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span><span class="w"></span>

<span class="w">          </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">              </span><span class="s1">&#39;Layer weight shape </span><span class="si">%s</span><span class="s1"> not compatible with provided weight &#39;</span><span class="w"></span>

<span class="w">              </span><span class="s1">&#39;shape </span><span class="si">%s</span><span class="s1">&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="n">ref_shape</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">weight_value_tuples</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>

<span class="w">    </span><span class="n">backend</span><span class="o">.</span><span class="n">batch_set_value</span><span class="p">(</span><span class="n">weight_value_tuples</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h3 id="fasterrcnnfpnresnet50pytorch">FasterRcnnFPNResnet50Pytorch</h3>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">FasterRcnnFPNResnet50Pytorch</span><span class="p">(</span>
    <span class="n">num_classes</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>You can use it as follow:</p>
<div class="codehilite"><pre><span></span><code><span class="n">model_faster_rcnn</span> <span class="o">=</span> <span class="n">FasterRcnnFPNResnet50</span><span class="p">(</span><span class="mi">80</span><span class="p">)</span>
<span class="n">base_lr</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="n">base_lr</span><span class="p">)</span>
<span class="n">model_faster_rcnn</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
<span class="n">model_faster_rcnn</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">ds_train</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">ds_test</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">11</span><span class="p">,)</span>
</code></pre></div>

<h4 id="arguments_2">Arguments</h4>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>num_classes</td>
<td>The number of classes of your dataset<br>(<strong>do not include the background class</strong> it is handle for you)</td>
</tr>
<tr>
<td>backbone</td>
<td>A tensorflow Model.</td>
</tr>
</tbody>
</table>
<h4 id="ancestors-in-mro_2">Ancestors (in MRO)</h4>
<ul>
<li>kerod.model.faster_rcnn.FasterRcnnFPN</li>
<li>tensorflow.python.keras.engine.training.Model</li>
<li>tensorflow.python.keras.engine.base_layer.Layer</li>
<li>tensorflow.python.module.module.Module</li>
<li>tensorflow.python.training.tracking.tracking.AutoTrackable</li>
<li>tensorflow.python.training.tracking.base.Trackable</li>
<li>tensorflow.python.keras.utils.version_utils.LayerVersionSelector</li>
<li>tensorflow.python.keras.utils.version_utils.ModelVersionSelector</li>
</ul>
<h4 id="methods_2">Methods</h4>
<h4 id="add_loss_2">add_loss</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_loss</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">losses</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Add loss tensor(s), potentially dependent on layer inputs.</p>
<p>Some losses (for instance, activity regularization losses) may be dependent
on the inputs passed when calling a layer. Hence, when reusing the same
layer on different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.losses</code> may
be dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>This method can be used inside a subclassed layer or model's <code>call</code>
function, in which case <code>losses</code> should be a Tensor or list of Tensors.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MyLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">inputs</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div>

<p>This method can also be called directly on a Functional Model during
construction. In this case, any loss Tensors passed to this Model must
be symbolic and be able to be traced back to the model's <code>Input</code>s. These
losses become part of the model's topology and are tracked in <code>get_config</code>.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="c1"># Activity regularization.</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>
</code></pre></div>

<p>If this is not the case for your loss (if, for example, your loss references
a <code>Variable</code> of one of the model's layers), you can wrap your loss in a
zero-argument lambda. These losses are not tracked as part of the model's
topology since they can't be serialized.</p>
<p>Example:</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">d</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="c1"># Weight regularization.</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_loss</span><span class="p">(</span><span class="k">lambda</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">d</span><span class="o">.</span><span class="n">kernel</span><span class="p">))</span>
</code></pre></div>

<p>Arguments:
  losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses
    may also be zero-argument callables which create a loss tensor.
  **kwargs: Additional keyword arguments for backward compatibility.
    Accepted values:
      inputs - Deprecated, will be automatically inferred.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>  <span class="n">def</span> <span class="n">add_loss</span>(<span class="nb">self</span>, <span class="n">losses</span>, **<span class="n">kwargs</span>):

    <span class="s">&quot;&quot;&quot;Add loss tensor(s), potentially dependent on layer inputs.</span>

<span class="s">    Some losses (for instance, activity regularization losses) may be dependent</span>

<span class="s">    on the inputs passed when calling a layer. Hence, when reusing the same</span>

<span class="s">    layer on different inputs `a` and `b`, some entries in `layer.losses` may</span>

<span class="s">    be dependent on `a` and some on `b`. This method automatically keeps track</span>

<span class="s">    of dependencies.</span>

<span class="s">    This method can be used inside a subclassed layer or model&#39;s `call`</span>

<span class="s">    function, in which case `losses` should be a Tensor or list of Tensors.</span>

<span class="s">    Example:</span>

<span class="s">    ```python</span>

<span class="s">    class MyLayer(tf.keras.layers.Layer):</span>

<span class="s">      def call(self, inputs):</span>

<span class="s">        self.add_loss(tf.abs(tf.reduce_mean(inputs)))</span>

<span class="s">        return inputs</span>

<span class="s">    ```</span>

<span class="s">    This method can also be called directly on a Functional Model during</span>

<span class="s">    construction. In this case, any loss Tensors passed to this Model must</span>

<span class="s">    be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s">    losses become part of the model&#39;s topology and are tracked in `get_config`.</span>

<span class="s">    Example:</span>

<span class="s">    ```python</span>

<span class="s">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">    x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s">    # Activity regularization.</span>

<span class="s">    model.add_loss(tf.abs(tf.reduce_mean(x)))</span>

<span class="s">    ```</span>

<span class="s">    If this is not the case for your loss (if, for example, your loss references</span>

<span class="s">    a `Variable` of one of the model&#39;s layers), you can wrap your loss in a</span>

<span class="s">    zero-argument lambda. These losses are not tracked as part of the model&#39;s</span>

<span class="s">    topology since they can&#39;t be serialized.</span>

<span class="s">    Example:</span>

<span class="s">    ```python</span>

<span class="s">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s">    d = tf.keras.layers.Dense(10)</span>

<span class="s">    x = d(inputs)</span>

<span class="s">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s">    # Weight regularization.</span>

<span class="s">    model.add_loss(lambda: tf.reduce_mean(d.kernel))</span>

<span class="s">    ```</span>

<span class="s">    Arguments:</span>

<span class="s">      losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses</span>

<span class="s">        may also be zero-argument callables which create a loss tensor.</span>

<span class="s">      **kwargs: Additional keyword arguments for backward compatibility.</span>

<span class="s">        Accepted values:</span>

<span class="s">          inputs - Deprecated, will be automatically inferred.</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="n">kwargs</span>.<span class="nb">pop</span>(<span class="s">&#39;inputs&#39;</span>, <span class="n">None</span>)

    <span class="k">if</span> <span class="n">kwargs:</span>

      <span class="n">raise</span> <span class="n">TypeError</span>(<span class="s">&#39;Unknown keyword arguments: %s&#39;</span> % (<span class="n">kwargs</span>.<span class="nb">keys</span>(),))

    <span class="n">def</span> <span class="n">_tag_callable</span>(<span class="n">loss</span>):

      <span class="s">&quot;&quot;&quot;Tags callable loss tensor as `_unconditional_loss`.&quot;&quot;&quot;</span>

      <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

        <span class="c1"># We run the loss without autocasting, as regularizers are often</span>

        <span class="c1"># numerically unstable in float16.</span>

        <span class="k">with</span> <span class="n">autocast_variable</span>.<span class="n">enable_auto_cast_variables</span>(<span class="n">None</span>):

          <span class="n">loss</span> = <span class="n">loss</span>()

      <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

        <span class="k">return</span> <span class="n">None</span>  <span class="c1"># Will be filtered out when computing the .losses property</span>

      <span class="k">if</span> <span class="nb">not</span> <span class="n">tensor_util</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

        <span class="n">loss</span> = <span class="n">ops</span>.<span class="n">convert_to_tensor_v2_with_dispatch</span>(

            <span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

      <span class="n">loss</span>.<span class="n">_unconditional_loss</span> = <span class="nb">True</span>  <span class="c1"># pylint: disable=protected-access</span>

      <span class="k">return</span> <span class="n">loss</span>

    <span class="n">losses</span> = <span class="n">nest</span>.<span class="n">flatten</span>(<span class="n">losses</span>)

    <span class="n">callable_losses</span> = []

    <span class="n">eager_losses</span> = []

    <span class="n">symbolic_losses</span> = []

    <span class="k">for</span> <span class="n">loss</span> <span class="nb">in</span> <span class="n">losses:</span>

      <span class="k">if</span> <span class="n">callable</span>(<span class="n">loss</span>):

        <span class="n">callable_losses</span>.<span class="nb">append</span>(<span class="n">functools</span>.<span class="n">partial</span>(<span class="n">_tag_callable</span>, <span class="n">loss</span>))

        <span class="n">continue</span>

      <span class="k">if</span> <span class="n">loss</span> <span class="k">is</span> <span class="n">None:</span>

        <span class="n">continue</span>

      <span class="k">if</span> <span class="nb">not</span> <span class="n">tensor_util</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>) <span class="o">and</span> <span class="nb">not</span> <span class="n">isinstance</span>(

          <span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>):

        <span class="n">loss</span> = <span class="n">ops</span>.<span class="n">convert_to_tensor_v2_with_dispatch</span>(

            <span class="n">loss</span>, <span class="n">dtype</span>=<span class="n">backend</span>.<span class="n">floatx</span>())

      <span class="c1"># TF Functions should take the eager path.</span>

      <span class="k">if</span> ((<span class="n">tf_utils</span>.<span class="n">is_symbolic_tensor</span>(<span class="n">loss</span>) <span class="o">or</span>

           <span class="n">isinstance</span>(<span class="n">loss</span>, <span class="n">keras_tensor</span>.<span class="n">KerasTensor</span>)) <span class="o">and</span>

          <span class="nb">not</span> <span class="n">base_layer_utils</span>.<span class="n">is_in_tf_function</span>()):

        <span class="n">symbolic_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

      <span class="n">elif</span> <span class="n">tensor_util</span>.<span class="n">is_tensor</span>(<span class="n">loss</span>):

        <span class="n">eager_losses</span>.<span class="nb">append</span>(<span class="n">loss</span>)

    <span class="nb">self</span>.<span class="n">_callable_losses</span>.<span class="n">extend</span>(<span class="n">callable_losses</span>)

    <span class="n">in_call_context</span> = <span class="n">base_layer_utils</span>.<span class="n">call_context</span>().<span class="n">in_call</span>

    <span class="k">if</span> <span class="n">eager_losses</span> <span class="o">and</span> <span class="nb">not</span> <span class="n">in_call_context:</span>

      <span class="n">raise</span> <span class="n">ValueError</span>(

          <span class="s">&#39;Expected a symbolic Tensors or a callable for the loss value. &#39;</span>

          <span class="s">&#39;Please wrap your loss computation in a zero argument `lambda`.&#39;</span>)

    <span class="nb">self</span>.<span class="n">_eager_losses</span>.<span class="n">extend</span>(<span class="n">eager_losses</span>)

    <span class="k">if</span> <span class="n">in_call_context</span> <span class="o">and</span> <span class="nb">not</span> <span class="n">keras_tensor</span>.<span class="n">keras_tensors_enabled</span>():

      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="nb">in</span> <span class="n">symbolic_losses:</span>

        <span class="nb">self</span>.<span class="n">_losses</span>.<span class="nb">append</span>(<span class="n">symbolic_loss</span>)

    <span class="n">else:</span>

      <span class="k">for</span> <span class="n">symbolic_loss</span> <span class="nb">in</span> <span class="n">symbolic_losses:</span>

        <span class="k">if</span> <span class="n">getattr</span>(<span class="nb">self</span>, <span class="s">&#39;_is_graph_network&#39;</span>, <span class="nb">False</span>):

          <span class="nb">self</span>.<span class="n">_graph_network_add_loss</span>(<span class="n">symbolic_loss</span>)

        <span class="n">else:</span>

          <span class="c1"># Possible a loss was added in a Layer&#39;s `build`.</span>

          <span class="nb">self</span>.<span class="n">_losses</span>.<span class="nb">append</span>(<span class="n">symbolic_loss</span>)
</code></pre></div>

</details>
<h4 id="add_metric_2">add_metric</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_metric</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">value</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds metric tensor to the layer.</p>
<p>This method can be used inside the <code>call()</code> method of a subclassed layer
or model.</p>
<div class="codehilite"><pre><span></span><code><span class="k">class</span> <span class="nc">MyMetricLayer</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
  <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MyMetricLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;my_metric_layer&#39;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>

  <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_2&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">inputs</span>
</code></pre></div>

<p>This method can also be called directly on a Functional Model during
construction. In this case, any tensor passed to this Model must
be symbolic and be able to be traced back to the model's <code>Input</code>s. These
metrics become part of the model's topology and are tracked when you
save the model via <code>save()</code>.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">math_ops</span><span class="o">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p>Note: Calling <code>add_metric()</code> with the result of a metric object on a
Functional Model, as shown in the example below, is not supported. This is
because we cannot trace the metric result tensor back to the model's inputs.</p>
<div class="codehilite"><pre><span></span><code><span class="n">inputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">add_metric</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Mean</span><span class="p">()(</span><span class="n">x</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;metric_1&#39;</span><span class="p">)</span>
</code></pre></div>

<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>value</td>
<td>Metric tensor.</td>
</tr>
<tr>
<td>name</td>
<td>String metric name.</td>
</tr>
<tr>
<td>**kwargs</td>
<td>Additional keyword arguments for backward compatibility.<br>Accepted values:<br><code>aggregation</code> - When the <code>value</code> tensor provided is not the result of<br>calling a <code>keras.Metric</code> instance, it will be aggregated by default<br>using a <code>keras.Metric.Mean</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_metric</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Adds metric tensor to the layer.</span>

<span class="s2">    This method can be used inside the `call()` method of a subclassed layer</span>

<span class="s2">    or model.</span>

<span class="s2">    ```python</span>

<span class="s2">    class MyMetricLayer(tf.keras.layers.Layer):</span>

<span class="s2">      def __init__(self):</span>

<span class="s2">        super(MyMetricLayer, self).__init__(name=&#39;my_metric_layer&#39;)</span>

<span class="s2">        self.mean = tf.keras.metrics.Mean(name=&#39;metric_1&#39;)</span>

<span class="s2">      def call(self, inputs):</span>

<span class="s2">        self.add_metric(self.mean(x))</span>

<span class="s2">        self.add_metric(tf.reduce_sum(x), name=&#39;metric_2&#39;)</span>

<span class="s2">        return inputs</span>

<span class="s2">    ```</span>

<span class="s2">    This method can also be called directly on a Functional Model during</span>

<span class="s2">    construction. In this case, any tensor passed to this Model must</span>

<span class="s2">    be symbolic and be able to be traced back to the model&#39;s `Input`s. These</span>

<span class="s2">    metrics become part of the model&#39;s topology and are tracked when you</span>

<span class="s2">    save the model via `save()`.</span>

<span class="s2">    ```python</span>

<span class="s2">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">    x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">    model.add_metric(math_ops.reduce_sum(x), name=&#39;metric_1&#39;)</span>

<span class="s2">    ```</span>

<span class="s2">    Note: Calling `add_metric()` with the result of a metric object on a</span>

<span class="s2">    Functional Model, as shown in the example below, is not supported. This is</span>

<span class="s2">    because we cannot trace the metric result tensor back to the model&#39;s inputs.</span>

<span class="s2">    ```python</span>

<span class="s2">    inputs = tf.keras.Input(shape=(10,))</span>

<span class="s2">    x = tf.keras.layers.Dense(10)(inputs)</span>

<span class="s2">    outputs = tf.keras.layers.Dense(1)(x)</span>

<span class="s2">    model = tf.keras.Model(inputs, outputs)</span>

<span class="s2">    model.add_metric(tf.keras.metrics.Mean()(x), name=&#39;metric_1&#39;)</span>

<span class="s2">    ```</span>

<span class="s2">    Args:</span>

<span class="s2">      value: Metric tensor.</span>

<span class="s2">      name: String metric name.</span>

<span class="s2">      **kwargs: Additional keyword arguments for backward compatibility.</span>

<span class="s2">        Accepted values:</span>

<span class="s2">        `aggregation` - When the `value` tensor provided is not the result of</span>

<span class="s2">        calling a `keras.Metric` instance, it will be aggregated by default</span>

<span class="s2">        using a `keras.Metric.Mean`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">kwargs_keys</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">list</span><span class="p">(</span><span class="n">kwargs</span><span class="p">.</span><span class="k">keys</span><span class="p">())</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">or</span><span class="w"></span>

<span class="w">        </span><span class="p">(</span><span class="n">len</span><span class="p">(</span><span class="n">kwargs_keys</span><span class="p">)</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="mi">1</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="n">kwargs_keys</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="s1">&#39;aggregation&#39;</span><span class="p">))</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword arguments: &#39;</span><span class="p">,</span><span class="w"> </span><span class="n">str</span><span class="p">(</span><span class="n">kwargs</span><span class="p">.</span><span class="k">keys</span><span class="p">()))</span><span class="w"></span>

<span class="w">    </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">hasattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_metric_obj&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">keras_tensor</span><span class="p">.</span><span class="n">keras_tensors_enabled</span><span class="p">()</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">is_symbolic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">keras_tensor</span><span class="p">.</span><span class="n">KerasTensor</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">is_symbolic</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf_utils</span><span class="p">.</span><span class="n">is_symbolic_tensor</span><span class="p">(</span><span class="k">value</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">in_call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">().</span><span class="n">in_call</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">name</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Eg. `self.add_metric(math_ops.reduce_sum(x))`</span><span class="w"></span>

<span class="w">      </span><span class="c1"># In eager mode, we use metric name to lookup a metric. Without a name,</span><span class="w"></span>

<span class="w">      </span><span class="c1"># a new Mean metric wrapper will be created on every model/layer call.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># So, we raise an error when no name is provided.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># We will do the same for symbolic mode for consistency although a name</span><span class="w"></span>

<span class="w">      </span><span class="c1"># will be generated if no name is provided.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># We will not raise this error in the foll use case for the sake of</span><span class="w"></span>

<span class="w">      </span><span class="c1"># consistency as name in provided in the metric constructor.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># mean = metrics.Mean(name=&#39;my_metric&#39;)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># model.add_metric(mean(outputs))</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;Please provide a name for your metric like &#39;</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;`self.add_metric(tf.reduce_sum(inputs), &#39;</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;name=</span><span class="se">\&#39;</span><span class="s1">mean_activation</span><span class="se">\&#39;</span><span class="s1">)`&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">elif</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">value</span><span class="p">.</span><span class="n">_metric_obj</span><span class="p">.</span><span class="k">name</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">is_symbolic</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;Expected a symbolic Tensor for the metric value, &#39;</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;received: &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">str</span><span class="p">(</span><span class="k">value</span><span class="p">))</span><span class="w"></span>

<span class="w">    </span><span class="c1"># If a metric was added in a Layer&#39;s `call` or `build`.</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">in_call_context</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># TF Function path should take the eager path.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If the given metric is available in `metrics` list we just update state</span><span class="w"></span>

<span class="w">      </span><span class="c1"># on it, otherwise we create a new metric instance and</span><span class="w"></span>

<span class="w">      </span><span class="c1"># add it to the `metrics` list.</span><span class="w"></span>

<span class="w">      </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_metric_obj&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Tensors that come from a Metric object already updated the Metric state.</span><span class="w"></span>

<span class="w">      </span><span class="n">should_update_state</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">metric_obj</span><span class="w"></span>

<span class="w">      </span><span class="k">name</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metric_obj</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="k">name</span><span class="w"></span>

<span class="w">      </span><span class="k">with</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics_lock</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="k">match</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_existing_metric</span><span class="p">(</span><span class="k">name</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="k">match</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">match</span><span class="w"></span>

<span class="w">        </span><span class="n">elif</span><span class="w"> </span><span class="n">metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="c1"># Build the metric object with the value&#39;s dtype if it defines one</span><span class="w"></span>

<span class="w">          </span><span class="n">metric_obj</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">metrics_mod</span><span class="p">.</span><span class="n">Mean</span><span class="p">(</span><span class="w"></span>

<span class="w">              </span><span class="k">name</span><span class="o">=</span><span class="k">name</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">getattr</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;dtype&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">))</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_metrics</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">metric_obj</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">should_update_state</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">metric_obj</span><span class="p">(</span><span class="k">value</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;Using the result of calling a `Metric` object &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;when calling `add_metric` on a Functional &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;Model is not supported. Please pass the &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;Tensor to monitor directly.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Insert layers into the Keras Graph Network.</span><span class="w"></span>

<span class="w">      </span><span class="n">aggregation</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">from_metric_obj</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="s1">&#39;mean&#39;</span><span class="w"></span>

<span class="w">      </span><span class="n">self</span><span class="p">.</span><span class="n">_graph_network_add_metric</span><span class="p">(</span><span class="k">value</span><span class="p">,</span><span class="w"> </span><span class="n">aggregation</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="add_update_2">add_update</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_update</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">updates</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Add update op(s), potentially dependent on layer inputs.</p>
<p>Weight updates (for instance, the updates of the moving mean and variance
in a BatchNormalization layer) may be dependent on the inputs passed
when calling a layer. Hence, when reusing the same layer on
different inputs <code>a</code> and <code>b</code>, some entries in <code>layer.updates</code> may be
dependent on <code>a</code> and some on <code>b</code>. This method automatically keeps track
of dependencies.</p>
<p>This call is ignored when eager execution is enabled (in that case, variable
updates are run on the fly and thus do not need to be tracked for later
execution).</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>updates</td>
<td>Update op, or list/tuple of update ops, or zero-arg callable<br>that returns an update op. A zero-arg callable should be passed in<br>order to disable running the updates by setting <code>trainable=False</code><br>on this Layer, when executing in Eager mode.</td>
</tr>
<tr>
<td>inputs</td>
<td>Deprecated, will be automatically inferred.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_update</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">updates</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Add update op(s), potentially dependent on layer inputs.</span>

<span class="s2">    Weight updates (for instance, the updates of the moving mean and variance</span>

<span class="s2">    in a BatchNormalization layer) may be dependent on the inputs passed</span>

<span class="s2">    when calling a layer. Hence, when reusing the same layer on</span>

<span class="s2">    different inputs `a` and `b`, some entries in `layer.updates` may be</span>

<span class="s2">    dependent on `a` and some on `b`. This method automatically keeps track</span>

<span class="s2">    of dependencies.</span>

<span class="s2">    This call is ignored when eager execution is enabled (in that case, variable</span>

<span class="s2">    updates are run on the fly and thus do not need to be tracked for later</span>

<span class="s2">    execution).</span>

<span class="s2">    Arguments:</span>

<span class="s2">      updates: Update op, or list/tuple of update ops, or zero-arg callable</span>

<span class="s2">        that returns an update op. A zero-arg callable should be passed in</span>

<span class="s2">        order to disable running the updates by setting `trainable=False`</span>

<span class="s2">        on this Layer, when executing in Eager mode.</span>

<span class="s2">      inputs: Deprecated, will be automatically inferred.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">inputs</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">tf_logging</span><span class="p">.</span><span class="n">warning</span><span class="p">(</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;`add_update` `inputs` kwarg has been deprecated. You no longer need &#39;</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;to pass a value to `inputs` as it is being automatically inferred.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">call_context</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">call_context</span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="c1"># No need to run updates during Functional API construction.</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">in_keras_graph</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Callable updates are disabled by setting `trainable=False`.</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">call_context</span><span class="p">.</span><span class="n">frozen</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="k">update</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">updates</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">callable</span><span class="p">(</span><span class="k">update</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="k">update</span><span class="p">()</span><span class="w">  </span><span class="c1"># pylint: disable=not-callable</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="add_variable_2">add_variable</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_variable</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use! Alias for <code>add_weight</code>.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_variable</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use! Alias for `add_weight`.</span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.add_variable` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.add_weight` method instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">add_weight</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="add_weight_2">add_weight</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">add_weight</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">shape</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dtype</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">initializer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">regularizer</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">trainable</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">constraint</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">use_resource</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">synchronization</span><span class="o">=&lt;</span><span class="n">VariableSynchronization</span><span class="o">.</span><span class="n">AUTO</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="n">aggregation</span><span class="o">=&lt;</span><span class="n">VariableAggregation</span><span class="o">.</span><span class="n">NONE</span><span class="p">:</span> <span class="mi">0</span><span class="o">&gt;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Adds a new variable to the layer.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>name</td>
<td>Variable name.</td>
</tr>
<tr>
<td>shape</td>
<td>Variable shape. Defaults to scalar if unspecified.</td>
</tr>
<tr>
<td>dtype</td>
<td>The type of the variable. Defaults to <code>self.dtype</code>.</td>
</tr>
<tr>
<td>initializer</td>
<td>Initializer instance (callable).</td>
</tr>
<tr>
<td>regularizer</td>
<td>Regularizer instance (callable).</td>
</tr>
<tr>
<td>trainable</td>
<td>Boolean, whether the variable should be part of the layer's<br>"trainable_variables" (e.g. variables, biases)<br>or "non_trainable_variables" (e.g. BatchNorm mean and variance).<br>Note that <code>trainable</code> cannot be <code>True</code> if <code>synchronization</code><br>is set to <code>ON_READ</code>.</td>
</tr>
<tr>
<td>constraint</td>
<td>Constraint instance (callable).</td>
</tr>
<tr>
<td>use_resource</td>
<td>Whether to use <code>ResourceVariable</code>.</td>
</tr>
<tr>
<td>synchronization</td>
<td>Indicates when a distributed a variable will be<br>aggregated. Accepted values are constants defined in the class<br><code>tf.VariableSynchronization</code>. By default the synchronization is set to<br><code>AUTO</code> and the current <code>DistributionStrategy</code> chooses<br>when to synchronize. If <code>synchronization</code> is set to <code>ON_READ</code>,<br><code>trainable</code> must not be set to <code>True</code>.</td>
</tr>
<tr>
<td>aggregation</td>
<td>Indicates how a distributed variable will be aggregated.<br>Accepted values are constants defined in the class<br><code>tf.VariableAggregation</code>.</td>
</tr>
<tr>
<td>**kwargs</td>
<td>Additional keyword arguments. Accepted values are <code>getter</code>,<br><code>collections</code>, <code>experimental_autocast</code> and <code>caching_device</code>.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>The variable created.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>When giving unsupported dtype and no initializer or when<br>trainable has been set to True with synchronization set as <code>ON_READ</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.for_subclass_implementers</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">add_weight</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="k">name</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">shape</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">dtype</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">initializer</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">regularizer</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">trainable</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="k">constraint</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">use_resource</span><span class="o">=</span><span class="k">None</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">synchronization</span><span class="o">=</span><span class="n">tf_variables</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">AUTO</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="n">aggregation</span><span class="o">=</span><span class="n">tf_variables</span><span class="p">.</span><span class="n">VariableAggregation</span><span class="p">.</span><span class="k">NONE</span><span class="p">,</span><span class="w"></span>

<span class="w">                 </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Adds a new variable to the layer.</span>

<span class="s2">    Arguments:</span>

<span class="s2">      name: Variable name.</span>

<span class="s2">      shape: Variable shape. Defaults to scalar if unspecified.</span>

<span class="s2">      dtype: The type of the variable. Defaults to `self.dtype`.</span>

<span class="s2">      initializer: Initializer instance (callable).</span>

<span class="s2">      regularizer: Regularizer instance (callable).</span>

<span class="s2">      trainable: Boolean, whether the variable should be part of the layer&#39;s</span>

<span class="s2">        &quot;</span><span class="n">trainable_variables</span><span class="s2">&quot; (e.g. variables, biases)</span>

<span class="s2">        or &quot;</span><span class="n">non_trainable_variables</span><span class="s2">&quot; (e.g. BatchNorm mean and variance).</span>

<span class="s2">        Note that `trainable` cannot be `True` if `synchronization`</span>

<span class="s2">        is set to `ON_READ`.</span>

<span class="s2">      constraint: Constraint instance (callable).</span>

<span class="s2">      use_resource: Whether to use `ResourceVariable`.</span>

<span class="s2">      synchronization: Indicates when a distributed a variable will be</span>

<span class="s2">        aggregated. Accepted values are constants defined in the class</span>

<span class="s2">        `tf.VariableSynchronization`. By default the synchronization is set to</span>

<span class="s2">        `AUTO` and the current `DistributionStrategy` chooses</span>

<span class="s2">        when to synchronize. If `synchronization` is set to `ON_READ`,</span>

<span class="s2">        `trainable` must not be set to `True`.</span>

<span class="s2">      aggregation: Indicates how a distributed variable will be aggregated.</span>

<span class="s2">        Accepted values are constants defined in the class</span>

<span class="s2">        `tf.VariableAggregation`.</span>

<span class="s2">      **kwargs: Additional keyword arguments. Accepted values are `getter`,</span>

<span class="s2">        `collections`, `experimental_autocast` and `caching_device`.</span>

<span class="s2">    Returns:</span>

<span class="s2">      The variable created.</span>

<span class="s2">    Raises:</span>

<span class="s2">      ValueError: When giving unsupported dtype and no initializer or when</span>

<span class="s2">        trainable has been set to True with synchronization set as `ON_READ`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">shape</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;partitioner&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w">  </span><span class="c1"># Ignored.</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Validate optional keyword arguments.</span><span class="w"></span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">kwargs</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">kwarg</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="err">[</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;experimental_autocast&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                       </span><span class="s1">&#39;caching_device&#39;</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;getter&#39;</span><span class="err">]</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s1">&#39;Unknown keyword argument:&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">kwarg</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">collections_arg</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;collections&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="c1"># &#39;experimental_autocast&#39; can be set to False by the caller to indicate an</span><span class="w"></span>

<span class="w">    </span><span class="c1"># AutoCastVariable should never be created.</span><span class="w"></span>

<span class="w">    </span><span class="n">autocast</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;experimental_autocast&#39;</span><span class="p">,</span><span class="w"> </span><span class="no">True</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="c1"># See the docstring for tf.Variable about the details for caching_device.</span><span class="w"></span>

<span class="w">    </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;caching_device&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">backend</span><span class="p">.</span><span class="n">floatx</span><span class="p">()</span><span class="w"></span>

<span class="w">    </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">dtypes</span><span class="p">.</span><span class="n">as_dtype</span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># The policy is &quot;_infer&quot;, so we infer the policy from the variable dtype.</span><span class="w"></span>

<span class="w">      </span><span class="n">self</span><span class="p">.</span><span class="n">_set_dtype_policy</span><span class="p">(</span><span class="n">policy</span><span class="p">.</span><span class="n">Policy</span><span class="p">(</span><span class="n">dtype</span><span class="p">.</span><span class="n">base_dtype</span><span class="p">.</span><span class="k">name</span><span class="p">))</span><span class="w"></span>

<span class="w">    </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="n">initializer</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">regularizer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">regularizers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="n">regularizer</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">constraint</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">constraints</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="k">constraint</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">synchronization</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="n">tf_variables</span><span class="p">.</span><span class="n">VariableSynchronization</span><span class="p">.</span><span class="n">ON_READ</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;Synchronization value can be set to &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;VariableSynchronization.ON_READ only for non-trainable variables. &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;You have specified trainable=True and &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;synchronization=VariableSynchronization.ON_READ.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="c1"># Set trainable to be false when variable is to be synced on read.</span><span class="w"></span>

<span class="w">        </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">False</span><span class="w"></span>

<span class="w">    </span><span class="n">elif</span><span class="w"> </span><span class="n">trainable</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">trainable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">True</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Initialize variable when no initializer provided</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">initializer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If dtype is DT_FLOAT, provide a uniform unit scaling initializer</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="s1">&#39;glorot_uniform&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If dtype is DT_INT/DT_UINT, provide a default value `zero`</span><span class="w"></span>

<span class="w">      </span><span class="c1"># If dtype is DT_BOOL, provide a default value `FALSE`</span><span class="w"></span>

<span class="w">      </span><span class="n">elif</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_integer</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_unsigned</span><span class="w"> </span><span class="k">or</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_bool</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">initializer</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">initializers</span><span class="p">.</span><span class="k">get</span><span class="p">(</span><span class="s1">&#39;zeros&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># NOTES:Do we need to support for handling DT_STRING and DT_COMPLEX here?</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;An initializer for variable %s of type %s is required&#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39; for layer %s&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="k">name</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">base_dtype</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">name</span><span class="p">))</span><span class="w"></span>

<span class="w">    </span><span class="n">getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">kwargs</span><span class="p">.</span><span class="n">pop</span><span class="p">(</span><span class="s1">&#39;getter&#39;</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">make_variable</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">autocast</span><span class="w"> </span><span class="k">and</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">compute_dtype</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_dtype_policy</span><span class="p">.</span><span class="n">variable_dtype</span><span class="w"></span>

<span class="w">        </span><span class="k">and</span><span class="w"> </span><span class="n">dtype</span><span class="p">.</span><span class="n">is_floating</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">old_getter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">getter</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Wrap variable constructor to return an AutoCastVariable.</span><span class="w"></span>

<span class="w">      </span><span class="n">def</span><span class="w"> </span><span class="n">getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="o">:</span><span class="w">  </span><span class="c1"># pylint: disable=function-redefined</span><span class="w"></span>

<span class="w">        </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_getter</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">autocast_variable</span><span class="p">.</span><span class="n">create_autocast_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Also the caching_device does not work with the mixed precision API,</span><span class="w"></span>

<span class="w">      </span><span class="c1"># disable it if it is specified.</span><span class="w"></span>

<span class="w">      </span><span class="c1"># TODO(b/142020079): Reenable it once the bug is fixed.</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">caching_device</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">tf_logging</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`caching_device` does not work with mixed precision &#39;</span><span class="w"></span>

<span class="w">                        </span><span class="s1">&#39;API. Ignoring user specified `caching_device`.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">caching_device</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="k">None</span><span class="w"></span>

<span class="w">    </span><span class="n">variable</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_add_variable_with_custom_getter</span><span class="p">(</span><span class="w"></span>

<span class="w">        </span><span class="k">name</span><span class="o">=</span><span class="k">name</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="c1"># TODO(allenl): a `make_variable` equivalent should be added as a</span><span class="w"></span>

<span class="w">        </span><span class="c1"># `Trackable` method.</span><span class="w"></span>

<span class="w">        </span><span class="n">getter</span><span class="o">=</span><span class="n">getter</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="c1"># Manage errors in Layer rather than Trackable.</span><span class="w"></span>

<span class="w">        </span><span class="n">overwrite</span><span class="o">=</span><span class="no">True</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">initializer</span><span class="o">=</span><span class="n">initializer</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="k">constraint</span><span class="o">=</span><span class="k">constraint</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">trainable</span><span class="o">=</span><span class="n">trainable</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">use_resource</span><span class="o">=</span><span class="n">use_resource</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">collections</span><span class="o">=</span><span class="n">collections_arg</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">synchronization</span><span class="o">=</span><span class="n">synchronization</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">aggregation</span><span class="o">=</span><span class="n">aggregation</span><span class="p">,</span><span class="w"></span>

<span class="w">        </span><span class="n">caching_device</span><span class="o">=</span><span class="n">caching_device</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">regularizer</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="c1"># TODO(fchollet): in the future, this should be handled at the</span><span class="w"></span>

<span class="w">      </span><span class="c1"># level of variable creation, and weight regularization losses</span><span class="w"></span>

<span class="w">      </span><span class="c1"># should be variable attributes.</span><span class="w"></span>

<span class="w">      </span><span class="n">name_in_scope</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">variable</span><span class="p">.</span><span class="k">name</span><span class="err">[</span><span class="o">:</span><span class="n">variable</span><span class="p">.</span><span class="k">name</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="s1">&#39;:&#39;</span><span class="p">)</span><span class="err">]</span><span class="w"></span>

<span class="w">      </span><span class="n">self</span><span class="p">.</span><span class="n">_handle_weight_regularization</span><span class="p">(</span><span class="n">name_in_scope</span><span class="p">,</span><span class="w"></span>

<span class="w">                                         </span><span class="n">variable</span><span class="p">,</span><span class="w"></span>

<span class="w">                                         </span><span class="n">regularizer</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="p">.</span><span class="n">is_split_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">for</span><span class="w"> </span><span class="n">v</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">variable</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">v</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">backend</span><span class="p">.</span><span class="n">track_variable</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">trainable</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_non_trainable_weights</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">variable</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">variable</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="apply_2">apply</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="o">*</span><span class="n">args</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use!</p>
<p>This is an alias of <code>self.__call__</code>.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Input tensor(s).</td>
</tr>
<tr>
<td>*args</td>
<td>additional positional arguments to be passed to <code>self.call</code>.</td>
</tr>
<tr>
<td>**kwargs</td>
<td>additional keyword arguments to be passed to <code>self.call</code>.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Output tensor(s).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Deprecated, do NOT use!</span>

<span class="ss">    This is an alias of `self.__call__`.</span>

<span class="ss">    Arguments:</span>

<span class="ss">      inputs: Input tensor(s).</span>

<span class="ss">      *args: additional positional arguments to be passed to `self.call`.</span>

<span class="ss">      **kwargs: additional keyword arguments to be passed to `self.call`.</span>

<span class="ss">    Returns:</span>

<span class="ss">      Output tensor(s).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.apply` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.__call__` method instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">__call__</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="o">*</span><span class="n">args</span><span class="p">,</span><span class="w"> </span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="call_2">call</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">call</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">training</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Perform an inference in training.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>A dict with the following schema:<br><code>images</code>: A Tensor of shape [batch_size, height, width, 3]<br><code>image_informations</code>: A float32 Tensor of shape [batch_size, 2] where<br>    the last dimension represents the original height and<br>    width of the images (without the padding).<br><br><code>ground_truths</code>: A dict<br>    - <code>BoxField.LABELS</code>: A 3-D tensor of shape [batch_size, num_gt, num_classes],<br>    - <code>BoxField.BOXES</code>: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)]<br>    - <code>BoxField.LABELS</code>: A 3-D tensor of int32 and shape [batch_size, num_gt]<br>    - <code>BoxField.WEIGHTS</code>: A 3-D tensor of float and shape [batch_size, num_gt]<br>    - <code>BoxField.NUM_BOXES</code>: A 2-D tensor of int32 and shape [batch_size, 1]<br>        which allows to remove the padding created by tf.Data.<br>        Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32)<br>        then my second box has a padding of 1</td>
</tr>
<tr>
<td>training</td>
<td>Is automatically set to <code>True</code> in train and test mode<br>(normally test should be at false). Why? Through the call we the losses and the metrics<br>of the rpn and fast_rcnn. They are automatically added with <code>add_loss</code> and <code>add_metrics</code>.<br>In test we want to benefit from those and therefore we compute them. It is an inheritance<br>from tensorflow 2.0 and 2.1 and I'll think to move them in a more traditional way inside the<br>train_step and test_step. However for now this method benefit of the encapsulation of<br>the <code>self.compiled_loss</code> method.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tuple</td>
<td>- <code>classification_pred</code>: A Tensor of shape [batch_size, num_boxes, num_classes]<br>    representing the class probability.<br>- <code>localization_pred</code>: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)]<br>- <code>anchors</code>: A Tensor of shape [batch_size, num_boxes, 4]</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="k">call</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">training</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Perform an inference in training.</span>

<span class="s2">        Arguments:</span>

<span class="s2">            inputs: A dict with the following schema:</span>

<span class="s2">                `images`: A Tensor of shape [batch_size, height, width, 3]</span>

<span class="s2">                `image_informations`: A float32 Tensor of shape [batch_size, 2] where</span>

<span class="s2">                    the last dimension represents the original height and</span>

<span class="s2">                    width of the images (without the padding).</span>

<span class="s2">                `ground_truths`: A dict</span>

<span class="s2">                    - `BoxField.LABELS`: A 3-D tensor of shape [batch_size, num_gt, num_classes],</span>

<span class="s2">                    - `BoxField.BOXES`: A 3-D tensor of shape [batch_size, num_gt, (y1, x1, y2, x2)]</span>

<span class="s2">                    - `BoxField.LABELS`: A 3-D tensor of int32 and shape [batch_size, num_gt]</span>

<span class="s2">                    - `BoxField.WEIGHTS`: A 3-D tensor of float and shape [batch_size, num_gt]</span>

<span class="s2">                    - `BoxField.NUM_BOXES`: A 2-D tensor of int32 and shape [batch_size, 1]</span>

<span class="s2">                        which allows to remove the padding created by tf.Data.</span>

<span class="s2">                        Example: if batch_size=2 and this field equal tf.constant([[2], [1]], tf.int32)</span>

<span class="s2">                        then my second box has a padding of 1</span>

<span class="s2">            training: Is automatically set to `True` in train and test mode</span>

<span class="s2">                (normally test should be at false). Why? Through the call we the losses and the metrics</span>

<span class="s2">                of the rpn and fast_rcnn. They are automatically added with `add_loss` and `add_metrics`.</span>

<span class="s2">                In test we want to benefit from those and therefore we compute them. It is an inheritance</span>

<span class="s2">                from tensorflow 2.0 and 2.1 and I&#39;ll think to move them in a more traditional way inside the</span>

<span class="s2">                train_step and test_step. However for now this method benefit of the encapsulation of</span>

<span class="s2">                the `self.compiled_loss` method.</span>

<span class="s2">        Returns:</span>

<span class="s2">            Tuple:</span>

<span class="s2">                - `classification_pred`: A Tensor of shape [batch_size, num_boxes, num_classes]</span>

<span class="s2">                    representing the class probability.</span>

<span class="s2">                - `localization_pred`: A Tensor of shape [batch_size, num_boxes, 4 * (num_classes - 1)]</span>

<span class="s2">                - `anchors`: A Tensor of shape [batch_size, num_boxes, 4]</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">images</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="err">[</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES</span><span class="err">]</span><span class="w"></span>

<span class="w">        </span><span class="n">images_information</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="err">[</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES_INFO</span><span class="err">]</span><span class="w"></span>

<span class="w">        </span><span class="c1"># The preprocessing dedicated to the backbone is done inside the model.</span><span class="w"></span>

<span class="w">        </span><span class="n">x</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">(</span><span class="n">images</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">pyramid</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fpn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">anchors_per_lvl</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">rpn</span><span class="p">(</span><span class="n">pyramid</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="o">:</span><span class="w"></span>

<span class="w">            </span><span class="n">apply_kernel_regularization</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">l2</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">backbone</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="c1"># add_loss stores the rpn losses computation in self.losses</span><span class="w"></span>

<span class="w">            </span><span class="n">_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">rpn</span><span class="p">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span><span class="w"> </span><span class="n">anchors_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                      </span><span class="n">inputs</span><span class="err">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="err">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">num_boxes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">2000</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">else</span><span class="w"> </span><span class="mi">1000</span><span class="w"></span>

<span class="w">        </span><span class="n">rois</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">post_process_rpn</span><span class="p">(</span><span class="n">rpn_cls_pred_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">rpn_loc_pred_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">anchors_per_lvl</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">images_information</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">pre_nms_topk_per_lvl</span><span class="o">=</span><span class="n">num_boxes</span><span class="p">,</span><span class="w"></span>

<span class="w">                                </span><span class="n">post_nms_topk</span><span class="o">=</span><span class="n">num_boxes</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="o">:</span><span class="w"></span>

<span class="w">            </span><span class="n">ground_truths</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">inputs</span><span class="err">[</span><span class="s1">&#39;ground_truths&#39;</span><span class="err">]</span><span class="w"></span>

<span class="w">            </span><span class="c1"># Include the ground_truths as RoIs for the training</span><span class="w"></span>

<span class="w">            </span><span class="n">rois</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="nf">concat</span><span class="p">(</span><span class="err">[</span><span class="n">tf</span><span class="p">.</span><span class="nf">cast</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_compute_dtype</span><span class="p">),</span><span class="w"> </span><span class="n">ground_truths</span><span class="err">[</span><span class="n">BoxField</span><span class="p">.</span><span class="n">BOXES</span><span class="err">]]</span><span class="p">,</span><span class="w"></span>

<span class="w">                             </span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="w"></span>

<span class="w">            </span><span class="c1"># Sample the boxes needed for inference</span><span class="w"></span>

<span class="w">            </span><span class="n">y_true</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">rois</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fast_rcnn</span><span class="p">.</span><span class="n">sample_boxes</span><span class="p">(</span><span class="n">rois</span><span class="p">,</span><span class="w"> </span><span class="n">ground_truths</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">classification_pred</span><span class="p">,</span><span class="w"> </span><span class="n">localization_pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fast_rcnn</span><span class="p">(</span><span class="err">[</span><span class="n">pyramid</span><span class="p">,</span><span class="w"> </span><span class="n">rois</span><span class="err">]</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="n">training</span><span class="w"> </span><span class="k">and</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="o">:</span><span class="w"></span>

<span class="w">            </span><span class="c1"># add_loss stores the fast_rcnn losses computation in self.losses</span><span class="w"></span>

<span class="w">            </span><span class="n">_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">fast_rcnn</span><span class="p">.</span><span class="n">compute_loss</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">classification_pred</span><span class="p">,</span><span class="w"> </span><span class="n">localization_pred</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">classification_pred</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tf</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">classification_pred</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">classification_pred</span><span class="p">,</span><span class="w"> </span><span class="n">localization_pred</span><span class="p">,</span><span class="w"> </span><span class="n">rois</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="compute_mask_2">compute_mask</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span><span class="p">,</span>
    <span class="n">mask</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes an output mask tensor.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Tensor or list of tensors.</td>
</tr>
<tr>
<td>mask</td>
<td>Tensor or list of tensors.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>None or a tensor (or list of tensors,<br>one per output tensor of the layer).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@generic_utils</span><span class="p">.</span><span class="k">default</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">compute_mask</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">mask</span><span class="o">=</span><span class="k">None</span><span class="p">)</span><span class="err">:</span><span class="w">  </span><span class="err">#</span><span class="w"> </span><span class="nl">pylint</span><span class="p">:</span><span class="w"> </span><span class="n">disable</span><span class="o">=</span><span class="n">unused</span><span class="o">-</span><span class="n">argument</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Computes an output mask tensor.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        inputs: Tensor or list of tensors.</span>

<span class="ss">        mask: Tensor or list of tensors.</span>

<span class="ss">    Returns:</span>

<span class="ss">        None or a tensor (or list of tensors,</span>

<span class="ss">            one per output tensor of the layer).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="nl">_supports_masking</span><span class="p">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="ow">any</span><span class="p">(</span><span class="n">m</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">m</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="err">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="s1">&#39;Layer &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">name</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="s1">&#39; does not support masking, &#39;</span><span class="w"></span>

<span class="w">                        </span><span class="s1">&#39;but was passed an input_mask: &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">str</span><span class="p">(</span><span class="n">mask</span><span class="p">))</span><span class="w"></span>

<span class="w">      </span><span class="err">#</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="nl">supported</span><span class="p">:</span><span class="w"> </span><span class="k">return</span><span class="w"> </span><span class="k">None</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="n">mask</span><span class="p">.</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="k">None</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="k">if</span><span class="w"> </span><span class="n">masking</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="n">explicitly</span><span class="w"> </span><span class="n">supported</span><span class="p">,</span><span class="w"> </span><span class="k">by</span><span class="w"> </span><span class="k">default</span><span class="w"></span>

<span class="w">    </span><span class="err">#</span><span class="w"> </span><span class="n">carry</span><span class="w"> </span><span class="k">over</span><span class="w"> </span><span class="n">the</span><span class="w"> </span><span class="k">input</span><span class="w"> </span><span class="n">mask</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">mask</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="compute_output_shape_2">compute_output_shape</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_shape</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_shape</span>
<span class="p">)</span>
</code></pre></div>

<p>Computes the output shape of the layer.</p>
<p>If the layer has not been built, this method will call <code>build</code> on the
layer. This assumes that the layer will later be used with inputs that
match the input shape provided here.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_shape</td>
<td>Shape tuple (tuple of integers)<br>or list of shape tuples (one per output tensor of the layer).<br>Shape tuples can include None for free dimensions,<br>instead of an integer.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>An input shape tuple.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code>  <span class="n">def</span> <span class="n">compute_output_shape</span>(<span class="nb">self</span>, <span class="n">input_shape</span>):

    <span class="s">&quot;&quot;&quot;Computes the output shape of the layer.</span>

<span class="s">    If the layer has not been built, this method will call `build` on the</span>

<span class="s">    layer. This assumes that the layer will later be used with inputs that</span>

<span class="s">    match the input shape provided here.</span>

<span class="s">    Arguments:</span>

<span class="s">        input_shape: Shape tuple (tuple of integers)</span>

<span class="s">            or list of shape tuples (one per output tensor of the layer).</span>

<span class="s">            Shape tuples can include None for free dimensions,</span>

<span class="s">            instead of an integer.</span>

<span class="s">    Returns:</span>

<span class="s">        An input shape tuple.</span>

<span class="s">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">context</span>.<span class="n">executing_eagerly</span>():

      <span class="c1"># In this case we build the model first in order to do shape inference.</span>

      <span class="c1"># This is acceptable because the framework only calls</span>

      <span class="c1"># `compute_output_shape` on shape values that the layer would later be</span>

      <span class="c1"># built for. It would however cause issues in case a user attempts to</span>

      <span class="c1"># use `compute_output_shape` manually with shapes that are incompatible</span>

      <span class="c1"># with the shape the Layer will be called on (these users will have to</span>

      <span class="c1"># implement `compute_output_shape` themselves).</span>

      <span class="nb">self</span>.<span class="n">_maybe_build</span>(<span class="n">input_shape</span>)

      <span class="k">with</span> <span class="n">func_graph</span>.<span class="n">FuncGraph</span>(<span class="n">str</span>(<span class="nb">self</span>.<span class="nb">name</span>) + <span class="s">&#39;_scratch_graph&#39;</span>).<span class="n">as_default</span>():

        <span class="n">input_shape</span> = <span class="n">tf_utils</span>.<span class="n">convert_shapes</span>(<span class="n">input_shape</span>, <span class="n">to_tuples</span>=<span class="nb">False</span>)

        <span class="n">def</span> <span class="n">_make_placeholder_like</span>(<span class="nb">shape</span>):

          <span class="n">ph</span> = <span class="n">backend</span>.<span class="nb">placeholder</span>(<span class="nb">shape</span>=<span class="nb">shape</span>, <span class="n">dtype</span>=<span class="nb">self</span>.<span class="n">dtype</span>)

          <span class="n">ph</span>.<span class="n">_keras_mask</span> = <span class="n">None</span>

          <span class="k">return</span> <span class="n">ph</span>

        <span class="n">inputs</span> = <span class="n">nest</span>.<span class="n">map_structure</span>(<span class="n">_make_placeholder_like</span>, <span class="n">input_shape</span>)

        <span class="n">try:</span>

          <span class="n">outputs</span> = <span class="nb">self</span>(<span class="n">inputs</span>, <span class="n">training</span>=<span class="nb">False</span>)

        <span class="n">except</span> <span class="n">TypeError</span> <span class="n">as</span> <span class="n">e:</span>

          <span class="n">six</span>.<span class="n">raise_from</span>(

              <span class="n">NotImplementedError</span>(

                  <span class="s">&#39;We could not automatically infer the static shape of the &#39;</span>

                  <span class="s">&#39;layer\&#39;s output. Please implement the &#39;</span>

                  <span class="s">&#39;`compute_output_shape` method on your layer (%s).&#39;</span> %

                  <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>), <span class="nb">e</span>)

      <span class="k">return</span> <span class="n">nest</span>.<span class="n">map_structure</span>(<span class="n">lambda</span> <span class="n">t:</span> <span class="nb">t</span>.<span class="nb">shape</span>, <span class="n">outputs</span>)

    <span class="n">raise</span> <span class="n">NotImplementedError</span>(

        <span class="s">&#39;Please run in eager mode or implement the `compute_output_shape` &#39;</span>

        <span class="s">&#39;method on your layer (%s).&#39;</span> % <span class="nb">self</span>.<span class="n">__class__</span>.<span class="n">__name__</span>)
</code></pre></div>

</details>
<h4 id="compute_output_signature_2">compute_output_signature</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">compute_output_signature</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_signature</span>
<span class="p">)</span>
</code></pre></div>

<p>Compute the output tensor signature of the layer based on the inputs.</p>
<p>Unlike a TensorShape object, a TensorSpec object contains both shape
and dtype information for a tensor. This method allows layers to provide
output dtype information if it is different from the input dtype.
For any layer that doesn't implement this function,
the framework will fall back to use <code>compute_output_shape</code>, and will
assume that the output dtype matches the input dtype.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>input_signature</td>
<td>Single TensorSpec or nested structure of TensorSpec<br>objects, describing a candidate input for the layer.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>Single TensorSpec or nested structure of TensorSpec objects, describing<br>how the layer would transform the provided input.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>TypeError</td>
<td>If input_signature contains a non-TensorSpec object.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.for_subclass_implementers</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">compute_output_signature</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Compute the output tensor signature of the layer based on the inputs.</span>

<span class="s2">    Unlike a TensorShape object, a TensorSpec object contains both shape</span>

<span class="s2">    and dtype information for a tensor. This method allows layers to provide</span>

<span class="s2">    output dtype information if it is different from the input dtype.</span>

<span class="s2">    For any layer that doesn&#39;t implement this function,</span>

<span class="s2">    the framework will fall back to use `compute_output_shape`, and will</span>

<span class="s2">    assume that the output dtype matches the input dtype.</span>

<span class="s2">    Args:</span>

<span class="s2">      input_signature: Single TensorSpec or nested structure of TensorSpec</span>

<span class="s2">        objects, describing a candidate input for the layer.</span>

<span class="s2">    Returns:</span>

<span class="s2">      Single TensorSpec or nested structure of TensorSpec objects, describing</span>

<span class="s2">        how the layer would transform the provided input.</span>

<span class="s2">    Raises:</span>

<span class="s2">      TypeError: If input_signature contains a non-TensorSpec object.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">check_type_return_shape</span><span class="p">(</span><span class="n">s</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">s</span><span class="p">,</span><span class="w"> </span><span class="n">tensor_spec</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">TypeError</span><span class="p">(</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;Only TensorSpec signature types are supported, &#39;</span><span class="w"></span>

<span class="w">            </span><span class="s1">&#39;but saw signature signature entry: {}.&#39;</span><span class="p">.</span><span class="k">format</span><span class="p">(</span><span class="n">s</span><span class="p">))</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">s</span><span class="p">.</span><span class="n">shape</span><span class="w"></span>

<span class="w">    </span><span class="n">input_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">check_type_return_shape</span><span class="p">,</span><span class="w"> </span><span class="n">input_signature</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">output_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">compute_output_shape</span><span class="p">(</span><span class="n">input_shape</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_compute_dtype</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">dtype</span><span class="w"> </span><span class="k">is</span><span class="w"> </span><span class="k">None</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="n">input_dtypes</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="err">[</span><span class="n">s</span><span class="p">.</span><span class="n">dtype</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">input_signature</span><span class="p">)</span><span class="err">]</span><span class="w"></span>

<span class="w">      </span><span class="c1"># Default behavior when self.dtype is None, is to use the first input&#39;s</span><span class="w"></span>

<span class="w">      </span><span class="c1"># dtype.</span><span class="w"></span>

<span class="w">      </span><span class="n">dtype</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_dtypes</span><span class="err">[</span><span class="mi">0</span><span class="err">]</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">nest</span><span class="p">.</span><span class="n">map_structure</span><span class="p">(</span><span class="w"></span>

<span class="w">        </span><span class="n">lambda</span><span class="w"> </span><span class="n">s</span><span class="o">:</span><span class="w"> </span><span class="n">tensor_spec</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span><span class="w"> </span><span class="n">shape</span><span class="o">=</span><span class="n">s</span><span class="p">),</span><span class="w"></span>

<span class="w">        </span><span class="n">output_shape</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="count_params_2">count_params</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">count_params</span><span class="p">(</span>
    <span class="bp">self</span>
<span class="p">)</span>
</code></pre></div>

<p>Count the total number of scalars composing the weights.</p>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>An integer count.</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>if the layer isn't yet built<br>(in which case its weights aren't yet defined).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Count the total number of scalars composing the weights.</span>

<span class="s2">    Returns:</span>

<span class="s2">        An integer count.</span>

<span class="s2">    Raises:</span>

<span class="s2">        ValueError: if the layer isn&#39;t yet built</span>

<span class="s2">          (in which case its weights aren&#39;t yet defined).</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="k">not</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">built</span><span class="o">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_is_graph_network&#39;</span><span class="p">,</span><span class="w"> </span><span class="no">False</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="k">with</span><span class="w"> </span><span class="n">tf_utils</span><span class="p">.</span><span class="n">maybe_init_scope</span><span class="p">(</span><span class="n">self</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">          </span><span class="n">self</span><span class="p">.</span><span class="n">_maybe_build</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">inputs</span><span class="p">)</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="s1">&#39;You tried to call `count_params` on &#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="o">+</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;, but the layer isn</span><span class="se">\&#39;</span><span class="s1">t built. &#39;</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;You can build it manually via: `&#39;</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="k">name</span><span class="w"> </span><span class="o">+</span><span class="w"></span>

<span class="w">                         </span><span class="s1">&#39;.build(batch_input_shape)`.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">layer_utils</span><span class="p">.</span><span class="n">count_params</span><span class="p">(</span><span class="n">self</span><span class="p">.</span><span class="n">weights</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="export_for_serving_2">export_for_serving</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">export_for_serving</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">filepath</span>
<span class="p">)</span>
</code></pre></div>

<p>Allow to bypass the save_model behavior the graph in serving mode.</p>
<p>Currently, the issue is that in training the ground_truths are passed to the call method but
not in inference. For the serving only the <code>images</code> and <code>images_information</code> are defined.
It means the inputs link to the ground_truths won't be defined in serving. However, in tensorflow
when the <code>training</code> arguments is defined int the method <code>call</code>, <code>tf.save_model.save</code> method
performs a check on the graph for training=False and training=True.
However, we don't want this check to be perform because our ground_truths inputs aren't defined.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">export_for_serving</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">filepath</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Allow to bypass the save_model behavior the graph in serving mode.</span>

<span class="s2">        Currently, the issue is that in training the ground_truths are passed to the call method but</span>

<span class="s2">        not in inference. For the serving only the `images` and `images_information` are defined.</span>

<span class="s2">        It means the inputs link to the ground_truths won&#39;t be defined in serving. However, in tensorflow</span>

<span class="s2">        when the `training` arguments is defined int the method `call`, `tf.save_model.save` method</span>

<span class="s2">        performs a check on the graph for training=False and training=True.</span>

<span class="s2">        However, we don&#39;t want this check to be perform because our ground_truths inputs aren&#39;t defined.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">True</span><span class="w"></span>

<span class="w">        </span><span class="n">call_output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">serving_step</span><span class="p">.</span><span class="n">get_concrete_function</span><span class="p">()</span><span class="w"></span>

<span class="w">        </span><span class="n">tf</span><span class="p">.</span><span class="n">saved_model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">filepath</span><span class="p">,</span><span class="w"> </span><span class="n">signatures</span><span class="o">=</span><span class="err">{</span><span class="s1">&#39;serving_default&#39;</span><span class="o">:</span><span class="w"> </span><span class="n">call_output</span><span class="err">}</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">self</span><span class="p">.</span><span class="n">_serving</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="no">False</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_input_at_2">get_input_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the input tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A tensor (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;input_tensors&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;input&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_input_mask_at_2">get_input_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the input mask tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A mask tensor</span>

<span class="ss">        (or list of tensors if the layer has multiple inputs).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">inputs</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_input_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &#39;_keras_mask&#39;, None) for x in inputs</span><span class="o">]</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_input_shape_at_2">get_input_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_input_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the input shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple inputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_input_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the input shape(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A shape tuple</span>

<span class="ss">        (or list of shape tuples if the layer has multiple inputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;input_shapes&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;input shape&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_losses_for_2">get_losses_for</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_losses_for</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use!</p>
<p>Retrieves losses relevant to a specific set of inputs.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Input tensor or list/tuple of input tensors.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>List of loss tensors of the layer that depend on <code>inputs</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_generate_docs</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_losses_for</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use!</span>

<span class="s2">    Retrieves losses relevant to a specific set of inputs.</span>

<span class="s2">    Arguments:</span>

<span class="s2">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="s2">    Returns:</span>

<span class="s2">      List of loss tensors of the layer that depend on `inputs`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.get_losses_for` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.losses` instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">losses</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_output_at_2">get_output_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A tensor (or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the output tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A tensor (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;output_tensors&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;output&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_output_mask_at_2">get_output_mask_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_mask_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output mask tensor(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A mask tensor<br>(or list of tensors if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_mask_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the output mask tensor(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A mask tensor</span>

<span class="ss">        (or list of tensors if the layer has multiple outputs).</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">output</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">get_output_at</span><span class="p">(</span><span class="n">node_index</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="n">list</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="o">[</span><span class="n">getattr(x, &#39;_keras_mask&#39;, None) for x in output</span><span class="o">]</span><span class="w"></span>

<span class="w">    </span><span class="k">else</span><span class="err">:</span><span class="w"></span>

<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">getattr</span><span class="p">(</span><span class="k">output</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;_keras_mask&#39;</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_output_shape_at_2">get_output_shape_at</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_output_shape_at</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">node_index</span>
<span class="p">)</span>
</code></pre></div>

<p>Retrieves the output shape(s) of a layer at a given node.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>node_index</td>
<td>Integer, index of the node<br>from which to retrieve the attribute.<br>E.g. <code>node_index=0</code> will correspond to the<br>first time the layer was called.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>A shape tuple<br>(or list of shape tuples if the layer has multiple outputs).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>RuntimeError</td>
<td>If called in Eager mode.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls</span><span class="p">.</span><span class="n">do_not_doc_inheritable</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_output_shape_at</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">node_index</span><span class="p">)</span><span class="err">:</span><span class="w"></span>

<span class="w">    </span><span class="ss">&quot;&quot;&quot;Retrieves the output shape(s) of a layer at a given node.</span>

<span class="ss">    Arguments:</span>

<span class="ss">        node_index: Integer, index of the node</span>

<span class="ss">            from which to retrieve the attribute.</span>

<span class="ss">            E.g. `node_index=0` will correspond to the</span>

<span class="ss">            first time the layer was called.</span>

<span class="ss">    Returns:</span>

<span class="ss">        A shape tuple</span>

<span class="ss">        (or list of shape tuples if the layer has multiple outputs).</span>

<span class="ss">    Raises:</span>

<span class="ss">      RuntimeError: If called in Eager mode.</span>

<span class="ss">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">_get_node_attribute_at_index</span><span class="p">(</span><span class="n">node_index</span><span class="p">,</span><span class="w"> </span><span class="s1">&#39;output_shapes&#39;</span><span class="p">,</span><span class="w"></span>

<span class="w">                                             </span><span class="s1">&#39;output shape&#39;</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="get_updates_for_2">get_updates_for</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">get_updates_for</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">inputs</span>
<span class="p">)</span>
</code></pre></div>

<p>Deprecated, do NOT use!</p>
<p>Retrieves updates relevant to a specific set of inputs.</p>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>inputs</td>
<td>Input tensor or list/tuple of input tensors.</td>
</tr>
</tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>None</td>
<td>List of update ops of the layer that depend on <code>inputs</code>.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="nv">@doc_controls.do_not_generate_docs</span><span class="w"></span>

<span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">get_updates_for</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">inputs</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">    </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Deprecated, do NOT use!</span>

<span class="s2">    Retrieves updates relevant to a specific set of inputs.</span>

<span class="s2">    Arguments:</span>

<span class="s2">      inputs: Input tensor or list/tuple of input tensors.</span>

<span class="s2">    Returns:</span>

<span class="s2">      List of update ops of the layer that depend on `inputs`.</span>

<span class="s2">    </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">    </span><span class="k">warnings</span><span class="p">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;`layer.get_updates_for` is deprecated and &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;will be removed in a future version. &#39;</span><span class="w"></span>

<span class="w">                  </span><span class="s1">&#39;Please use `layer.updates` method instead.&#39;</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">updates</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="serving_step_2">serving_step</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">serving_step</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">images</span><span class="p">,</span>
    <span class="n">images_info</span>
<span class="p">)</span>
</code></pre></div>

<p>Allow to bypass the save_model behavior the graph in serving mode.</p>
<p>Currently, the issue is that in training the ground_truths are passed to the call method but
not in inference. For the serving only the <code>images</code> and <code>images_information</code> are defined.
It means the inputs link to the ground_truths won't be defined in serving. However, tensorflow
absolutely want it and will return an exception if the ground_truth isn't provided.</p>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">    </span><span class="nv">@tf.function</span><span class="p">(</span><span class="n">input_signature</span><span class="o">=</span><span class="err">[</span><span class="w"></span>

<span class="w">        </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES</span><span class="p">),</span><span class="w"></span>

<span class="w">        </span><span class="n">tf</span><span class="p">.</span><span class="n">TensorSpec</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="k">None</span><span class="p">,</span><span class="w"> </span><span class="mi">2</span><span class="p">),</span><span class="w"> </span><span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">float32</span><span class="p">,</span><span class="w"> </span><span class="k">name</span><span class="o">=</span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES_INFO</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="err">]</span><span class="p">)</span><span class="w"></span>

<span class="w">    </span><span class="n">def</span><span class="w"> </span><span class="n">serving_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span><span class="w"> </span><span class="n">images</span><span class="p">,</span><span class="w"> </span><span class="n">images_info</span><span class="p">)</span><span class="o">:</span><span class="w"></span>

<span class="w">        </span><span class="s2">&quot;</span><span class="se">&quot;&quot;</span><span class="s2">Allow to bypass the save_model behavior the graph in serving mode.</span>

<span class="s2">        Currently, the issue is that in training the ground_truths are passed to the call method but</span>

<span class="s2">        not in inference. For the serving only the `images` and `images_information` are defined.</span>

<span class="s2">        It means the inputs link to the ground_truths won&#39;t be defined in serving. However, tensorflow</span>

<span class="s2">        absolutely want it and will return an exception if the ground_truth isn&#39;t provided.</span>

<span class="s2">        </span><span class="se">&quot;&quot;</span><span class="s2">&quot;</span><span class="w"></span>

<span class="w">        </span><span class="k">return</span><span class="w"> </span><span class="n">self</span><span class="p">.</span><span class="n">predict_step</span><span class="p">(</span><span class="err">{</span><span class="w"></span>

<span class="w">            </span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES</span><span class="o">:</span><span class="w"> </span><span class="n">images</span><span class="p">,</span><span class="w"></span>

<span class="w">            </span><span class="n">DatasetField</span><span class="p">.</span><span class="n">IMAGES_INFO</span><span class="o">:</span><span class="w"> </span><span class="n">images_info</span><span class="w"></span>

<span class="w">        </span><span class="err">}</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
<h4 id="set_weights_2">set_weights</h4>
<div class="codehilite"><pre><span></span><code><span class="k">def</span> <span class="nf">set_weights</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">weights</span>
<span class="p">)</span>
</code></pre></div>

<p>Sets the weights of the layer, from Numpy arrays.</p>
<p>The weights of a layer represent the state of the layer. This function
sets the weight values from numpy arrays. The weight values should be
passed in the order they are created by the layer. Note that the layer's
weights must be instantiated before calling this function by calling
the layer.</p>
<p>For example, a Dense layer returns a list of two values-- per-output
weights and the bias value. These can be used to set the weights of another
Dense layer:</p>
<blockquote>
<blockquote>
<blockquote>
<p>a = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(1.))
a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))
a.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]
b = tf.keras.layers.Dense(1,
...   kernel_initializer=tf.constant_initializer(2.))
b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))
b.get_weights()
[array([[2.],
       [2.],
       [2.]], dtype=float32), array([0.], dtype=float32)]
b.set_weights(a.get_weights())
b.get_weights()
[array([[1.],
       [1.],
       [1.]], dtype=float32), array([0.], dtype=float32)]</p>
</blockquote>
</blockquote>
</blockquote>
<p><strong>Parameters:</strong></p>
<table>
<thead>
<tr>
<th>Name</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>weights</td>
<td>a list of Numpy arrays. The number<br>of arrays and their shape must match<br>number of the dimensions of the weights<br>of the layer (i.e. it should match the<br>output of <code>get_weights</code>).</td>
</tr>
</tbody>
</table>
<p><strong>Raises:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Description</th>
</tr>
</thead>
<tbody>
<tr>
<td>ValueError</td>
<td>If the provided weights list does not match the<br>layer's specifications.</td>
</tr>
</tbody>
</table>
<details class="example">
<summary>View Source</summary>
<div class="codehilite"><pre><span></span><code><span class="w">  </span><span class="n">def</span><span class="w"> </span><span class="n">set_weights</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">):</span><span class="w"></span>

<span class="w">    </span><span class="sd">&quot;&quot;&quot;Sets the weights of the layer, from Numpy arrays.</span>

<span class="sd">    The weights of a layer represent the state of the layer. This function</span>

<span class="sd">    sets the weight values from numpy arrays. The weight values should be</span>

<span class="sd">    passed in the order they are created by the layer. Note that the layer&#39;s</span>

<span class="sd">    weights must be instantiated before calling this function by calling</span>

<span class="sd">    the layer.</span>

<span class="sd">    For example, a Dense layer returns a list of two values-- per-output</span>

<span class="sd">    weights and the bias value. These can be used to set the weights of another</span>

<span class="sd">    Dense layer:</span>

<span class="sd">    &gt;&gt;&gt; a = tf.keras.layers.Dense(1,</span>

<span class="sd">    ...   kernel_initializer=tf.constant_initializer(1.))</span>

<span class="sd">    &gt;&gt;&gt; a_out = a(tf.convert_to_tensor([[1., 2., 3.]]))</span>

<span class="sd">    &gt;&gt;&gt; a.get_weights()</span>

<span class="sd">    [array([[1.],</span>

<span class="sd">           [1.],</span>

<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    &gt;&gt;&gt; b = tf.keras.layers.Dense(1,</span>

<span class="sd">    ...   kernel_initializer=tf.constant_initializer(2.))</span>

<span class="sd">    &gt;&gt;&gt; b_out = b(tf.convert_to_tensor([[10., 20., 30.]]))</span>

<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>

<span class="sd">    [array([[2.],</span>

<span class="sd">           [2.],</span>

<span class="sd">           [2.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    &gt;&gt;&gt; b.set_weights(a.get_weights())</span>

<span class="sd">    &gt;&gt;&gt; b.get_weights()</span>

<span class="sd">    [array([[1.],</span>

<span class="sd">           [1.],</span>

<span class="sd">           [1.]], dtype=float32), array([0.], dtype=float32)]</span>

<span class="sd">    Arguments:</span>

<span class="sd">        weights: a list of Numpy arrays. The number</span>

<span class="sd">            of arrays and their shape must match</span>

<span class="sd">            number of the dimensions of the weights</span>

<span class="sd">            of the layer (i.e. it should match the</span>

<span class="sd">            output of `get_weights`).</span>

<span class="sd">    Raises:</span>

<span class="sd">        ValueError: If the provided weights list does not match the</span>

<span class="sd">            layer&#39;s specifications.</span>

<span class="sd">    &quot;&quot;&quot;</span><span class="w"></span>

<span class="w">    </span><span class="n">params</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="w"></span>

<span class="w">    </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"></span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">params</span><span class="p">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span><span class="w"></span>

<span class="w">        </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">param</span><span class="o">.</span><span class="n">num_tensors</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="p">:</span><span class="w"></span>

<span class="w">        </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>

<span class="w">    </span><span class="k">if</span><span class="w"> </span><span class="n">expected_num_weights</span><span class="w"> </span><span class="o">!=</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">):</span><span class="w"></span>

<span class="w">      </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;You called `set_weights(weights)` on layer &quot;</span><span class="si">%s</span><span class="s1">&quot; &#39;</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;with a weight list of length </span><span class="si">%s</span><span class="s1">, but the layer was &#39;</span><span class="w"></span>

<span class="w">          </span><span class="s1">&#39;expecting </span><span class="si">%s</span><span class="s1"> weights. Provided weights: </span><span class="si">%s</span><span class="s1">...&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"></span>

<span class="w">          </span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">,</span><span class="w"> </span><span class="n">len</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span><span class="w"> </span><span class="n">expected_num_weights</span><span class="p">,</span><span class="w"> </span><span class="nb">str</span><span class="p">(</span><span class="n">weights</span><span class="p">)[:</span><span class="mi">50</span><span class="p">]))</span><span class="w"></span>

<span class="w">    </span><span class="n">weight_index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="w"></span>

<span class="w">    </span><span class="n">weight_value_tuples</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[]</span><span class="w"></span>

<span class="w">    </span><span class="k">for</span><span class="w"> </span><span class="n">param</span><span class="w"> </span><span class="ow">in</span><span class="w"> </span><span class="n">params</span><span class="p">:</span><span class="w"></span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="n">isinstance</span><span class="p">(</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">base_layer_utils</span><span class="o">.</span><span class="n">TrackableWeightHandler</span><span class="p">):</span><span class="w"></span>

<span class="w">        </span><span class="n">num_tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="o">.</span><span class="n">num_tensors</span><span class="w"></span>

<span class="w">        </span><span class="n">tensors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">:</span><span class="n">weight_index</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">num_tensors</span><span class="p">]</span><span class="w"></span>

<span class="w">        </span><span class="n">param</span><span class="o">.</span><span class="n">set_weights</span><span class="p">(</span><span class="n">tensors</span><span class="p">)</span><span class="w"></span>

<span class="w">        </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="n">num_tensors</span><span class="w"></span>

<span class="w">      </span><span class="k">else</span><span class="p">:</span><span class="w"></span>

<span class="w">        </span><span class="n">weight</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">weights</span><span class="p">[</span><span class="n">weight_index</span><span class="p">]</span><span class="w"></span>

<span class="w">        </span><span class="n">ref_shape</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">param</span><span class="o">.</span><span class="n">shape</span><span class="w"></span>

<span class="w">        </span><span class="k">if</span><span class="w"> </span><span class="ow">not</span><span class="w"> </span><span class="n">ref_shape</span><span class="o">.</span><span class="n">is_compatible_with</span><span class="p">(</span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">):</span><span class="w"></span>

<span class="w">          </span><span class="n">raise</span><span class="w"> </span><span class="n">ValueError</span><span class="p">(</span><span class="w"></span>

<span class="w">              </span><span class="s1">&#39;Layer weight shape </span><span class="si">%s</span><span class="s1"> not compatible with provided weight &#39;</span><span class="w"></span>

<span class="w">              </span><span class="s1">&#39;shape </span><span class="si">%s</span><span class="s1">&#39;</span><span class="w"> </span><span class="o">%</span><span class="w"> </span><span class="p">(</span><span class="n">ref_shape</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">weight_value_tuples</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">param</span><span class="p">,</span><span class="w"> </span><span class="n">weight</span><span class="p">))</span><span class="w"></span>

<span class="w">        </span><span class="n">weight_index</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">1</span><span class="w"></span>

<span class="w">    </span><span class="n">backend</span><span class="o">.</span><span class="n">batch_set_value</span><span class="p">(</span><span class="n">weight_value_tuples</span><span class="p">)</span><span class="w"></span>
</code></pre></div>

</details>
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid">
        
          <a href="../factory/" title="Factory" class="md-flex md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-back md-footer-nav__button"></i>
            </div>
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Factory
              </span>
            </div>
          </a>
        
        
          <a href="../" title="Index" class="md-flex md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-flex__cell md-flex__cell--stretch md-footer-nav__title">
              <span class="md-flex__ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Index
              </span>
            </div>
            <div class="md-flex__cell md-flex__cell--shrink">
              <i class="md-icon md-icon--arrow-forward md-footer-nav__button"></i>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Powered by
        <a href="http://timothycrosley.github.io/portray">portray.</a>
        You too can
        <a href="http://timothycrosley.github.io/portray">
          portray</a>
        your Python project well using automatic documentation.
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../../../..", "features": ["header.autohide"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../../../assets/javascripts/workers/search.f8263e09.min.js", "version": null}</script>
    
    
      <script src="../../../../assets/javascripts/bundle.4fc53ad4.min.js"></script>
      
    
  </body>
</html>